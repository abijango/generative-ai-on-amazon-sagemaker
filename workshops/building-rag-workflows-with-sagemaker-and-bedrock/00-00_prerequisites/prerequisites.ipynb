{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aa845",
   "metadata": {},
   "source": [
    "# Lab 0: Prerequisites\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c50906",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook demonstrates how to implement a Retrieval Augmented Generation (RAG) solution using:\n",
    "- Amazon SageMaker for hosting embedding and LLM models\n",
    "- Amazon OpenSearch for vector search\n",
    "- LangChain for orchestrating the RAG pipeline\n",
    "- we'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html) and leverage the features in [Langfuse](https://langfuse.com/) to manage and trace the RAG pipelines with traces and spans. We will create a OpenSearch Vector Database and the RAG results generation to show offline evaluation and scoring.\n",
    "\n",
    "In this notebook, Question Answering solution with Large Language Models (LLMs) and Amazon OpenSearch Service. An application using the RAG(Retrieval Augmented Generation) approach retrieves information most relevant to the user’s request from the enterprise knowledge base or content, bundles it as context along with the user’s request as a prompt, and then sends it to the LLM to get a GenAI response.\n",
    "\n",
    "LLMs have limitations around the maximum word count for the input prompt, therefore choosing the right passages among thousands or millions of documents in the enterprise, has a direct impact on the LLM’s accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d37f81",
   "metadata": {},
   "source": [
    "<H2>Part 1: Build conversational search with OpenSearch Service</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b731",
   "metadata": {},
   "source": [
    "The vector dataset used in this part of the lab is comprised of a predefined content resource from the [PubMedQA](https://pubmedqa.github.io/) dataset.\n",
    "\n",
    "You will use OpenSearch ingest pipeline with embedding processor to generate text embeddings for the dataset. Using the neural plugin in OpenSearch will allow you to generate the embeddings of the search query as well.\n",
    "You will then use the large language model (LLM) hosted on Amazon SageMaker endpoints with the RAG processor in the search pipeline to generate text. The RAG processor will combine the retrieved search results from OpenSearch with the generated answer from the LLM to send back to the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ae065",
   "metadata": {},
   "source": [
    "### The key steps in this lab are as follows:\n",
    "\n",
    "1. Get prerequisites installed and libraries imported.\n",
    "1. Create an OpenSearch Service Domain\n",
    "1. Create a KNN-enabled index and ingest the catalog items into the index\n",
    "1. Deploy the embedding model to a SageMaker endpoint.\n",
    "1. Deploy the generation model to a SageMaker endpoint.\n",
    "1. Build the end-to-end pipeline with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df92a75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Import libraries & initialize resources\n",
    "The code blocks below will install and import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35fc57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip uninstall -q -y autogluon-multimodal autogluon-timeseries autogluon-features autogluon-common autogluon-core\n",
    "\n",
    "%pip install -Uq boto3==1.37.38\n",
    "%pip install -Uq sagemaker==2.243.2\n",
    "    \n",
    "%pip install -Uq opensearch-py==2.8.0\n",
    "%pip install -Uq opensearch_py_ml==1.1.0\n",
    "    \n",
    "print(\"Installs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "from typing import Any, Dict, List, Optional\n",
    "import boto3\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# Sagemaker\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a9721-d612-4701-a4ba-f8b4447a6f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "region = sess.boto_region_name\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "opensearch_client = boto3.client('opensearch')\n",
    "\n",
    "\n",
    "print(f\"account id: {account_id}\")\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed452d3d-d89e-48ad-bdcf-a2c9acacc820",
   "metadata": {},
   "source": [
    "# 2. Deploy the embedding model to a SageMaker endpoint & build retrieval integration with OpenSearch\n",
    "\n",
    "We have taken the PubMedQA dataset and prepared it to include the contexts in the `extracted_context.json` file.\n",
    "\n",
    "The following cells will perform the steps to generate embeddings with the dataset and ingest into the OpenSearch vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c01a6b-7ba2-4041-a381-63c588190300",
   "metadata": {},
   "source": [
    "## 2.1 Create an OpenSearch Service Domain\n",
    "\n",
    "In the following steps, you will create a new OpenSearch Service domain. The configuration used here creates a publicly accessible domain, in 1 AZ, with your SageMaker execution role as the master user. If you are deploying an OpenSearch Domain for a real use case, you will want to deploy inside of a VPC and use multiple nodes in multiple AZs for high availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedee7d-302d-4dba-ab63-2a865684bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OS_DOMAIN_NAME = 'opensearch-rag-domain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361efe5-b9a5-4bcf-8922-804cbf14c4da",
   "metadata": {},
   "source": [
    "Store this variable for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba8844-0906-4460-9288-5661badf64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store OS_DOMAIN_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912b9ba-8783-4257-9808-cfb653fba192",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_policy = \\\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": [f\"{sagemaker.get_execution_role()}\"]\n",
    "             },\n",
    "             \"Action\": [\"es:*\"],\n",
    "             \"Resource\": f\"arn:aws:es:{sagemaker.Session().boto_region_name}:{sagemaker.Session().account_id()}:domain/{OS_DOMAIN_NAME}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "domain_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3da51-5f7e-40cd-b33d-5bddd55951f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_domain_payload = {\n",
    "    \"DomainName\": OS_DOMAIN_NAME,\n",
    "    \"EngineVersion\": \"OpenSearch_2.17\",\n",
    "    \"ClusterConfig\": {\n",
    "        \"InstanceType\": \"t3.small.search\",\n",
    "        \"InstanceCount\": 1,\n",
    "        \"DedicatedMasterEnabled\": False,\n",
    "        \"ZoneAwarenessEnabled\": False,\n",
    "        \"WarmEnabled\": False,\n",
    "        \"ColdStorageOptions\": {\n",
    "            \"Enabled\": False\n",
    "        },\n",
    "        \"MultiAZWithStandbyEnabled\": False\n",
    "    },\n",
    "    \"EBSOptions\": {\n",
    "        \"EBSEnabled\": True,\n",
    "        \"VolumeType\": \"gp3\",\n",
    "        \"VolumeSize\": 100,\n",
    "        \"Iops\": 3000,\n",
    "        \"Throughput\": 125\n",
    "    },\n",
    "    \"AccessPolicies\": json.dumps(domain_policy),\n",
    "    \"IPAddressType\": \"dualstack\",\n",
    "    \"SnapshotOptions\": {},\n",
    "    \"EncryptionAtRestOptions\": {\n",
    "        \"Enabled\": True\n",
    "    },\n",
    "    \"NodeToNodeEncryptionOptions\": {\n",
    "        \"Enabled\": True\n",
    "    },\n",
    "    \"AdvancedOptions\": {\n",
    "        \"indices.fielddata.cache.size\": \"20\",\n",
    "        \"override_main_response_version\": \"false\",\n",
    "        \"indices.query.bool.max_clause_count\": \"1024\",\n",
    "        \"rest.action.multi.allow_explicit_index\": \"true\"\n",
    "    },\n",
    "    \"DomainEndpointOptions\": {\n",
    "        \"EnforceHTTPS\": True,\n",
    "        \"CustomEndpointEnabled\": False\n",
    "    },\n",
    "    \"AdvancedSecurityOptions\": {\n",
    "        \"Enabled\": True,\n",
    "        \"InternalUserDatabaseEnabled\": False,\n",
    "        \"MasterUserOptions\": {\n",
    "            \"MasterUserARN\": sagemaker.get_execution_role()\n",
    "        }\n",
    "    },\n",
    "    \"TagList\": [],\n",
    "    \"OffPeakWindowOptions\": {\n",
    "        \"Enabled\": True,\n",
    "        \"OffPeakWindow\": {\n",
    "            \"WindowStartTime\": {\n",
    "                \"Hours\": 0,\n",
    "                \"Minutes\": 0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"SoftwareUpdateOptions\": {\n",
    "        \"AutoSoftwareUpdateEnabled\": False\n",
    "    },\n",
    "    \"AIMLOptions\": {\n",
    "        \"NaturalLanguageQueryGenerationOptions\": {\n",
    "            \"DesiredState\": \"ENABLED\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76348ec-e128-4175-8cad-708077144048",
   "metadata": {},
   "source": [
    "Creating an OpenSearch Domain will take about X minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a0b8f3-873d-41de-8b80-4804fdd5386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_domain_response = opensearch_client.create_domain(**create_domain_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934dea3-078c-4284-b9ee-8927e99d39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    status = opensearch_client.describe_domain(DomainName=OS_DOMAIN_NAME)[\"DomainStatus\"][\"Processing\"]\n",
    "    print(f\"DomainProcessingStatus: {status}\")\n",
    "    if status:\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# wait for endpoint uri to be available\n",
    "time.sleep(10)\n",
    "\n",
    "AOS_HOST = opensearch_client.describe_domain(DomainName=OS_DOMAIN_NAME)[\"DomainStatus\"][\"Endpoint\"]\n",
    "print(f\"Your OpenSearch Endpoint is available: https://{AOS_HOST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e091b-c4c5-4155-8449-521781054e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store AOS_HOST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7145ba-f124-4294-a518-b2dfa03d2fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Create SageMaker Embedding Endpoint\n",
    "A **Hugging Face text embedding model (gte-base-en-v1.5)** is deployed via SageMaker JumpStart to a SageMaker real-time endpoint. This model converts text into 384-dimensional vectors for semantic search.\n",
    "### Embedding Model Deployment\n",
    "- Deploy Hugging Face embedding model (gte-base-en-v1.5) on SageMaker\n",
    "- Create embedding endpoint for text vectorization\n",
    "- Configure content handlers for model input/output processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234baca0-4e34-482b-b858-616cc657d0b6",
   "metadata": {},
   "source": [
    "Now deploy `Alibaba-NLP/gte-base-en-v1.5` as an embedding model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6efe3-3249-402d-b45f-238071c075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "def get_embedding_image_uri(instance_type, version=\"1.4\"):\n",
    "  key = \"huggingface-tei\" if instance_type.startswith(\"ml.g\") or instance_type.startswith(\"ml.p\") else \"huggingface-tei-cpu\"\n",
    "  return get_huggingface_llm_image_uri(key, version=version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3951436-36c1-4532-9bdb-e1fce4dd2e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
    "embedding_instance_type = \"ml.c5.4xlarge\"\n",
    "\n",
    "# currently the latest HF TEI containers arent supported by the python SDK, use the direct URI\n",
    "#embedding_image = get_embedding_image_uri(embedding_instance_type)\n",
    "embedding_image = f\"683313688378.dkr.ecr.{region}.amazonaws.com/tei-cpu:2.0.1-tei1.6.0-cpu-py310-ubuntu22.04\"\n",
    "embedding_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adbf9a5-8b90-45c1-ae0b-f5621fa62a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID': EMBEDDING_MODEL_NAME\n",
    "}\n",
    "\n",
    "embedding_model_for_deployment = HuggingFaceModel(\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    image_uri=embedding_image,\n",
    ")\n",
    "\n",
    "EMBED_ENDPOINT_NAME = sagemaker.utils.name_from_base(\"gte-base-en-v1-5\")\n",
    "\n",
    "health_check_timeout = 300\n",
    "\n",
    "embedding_model_for_deployment.deploy(\n",
    "    endpoint_name=EMBED_ENDPOINT_NAME,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=embedding_instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    routing_config = {\n",
    "        \"RoutingStrategy\":  sagemaker.enums.RoutingStrategy.LEAST_OUTSTANDING_REQUESTS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4299f-c79a-41bc-bf1f-911783db1ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Successfully deployed embedding model to the SageMaker endpoint: {EMBED_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b66a0c-2e34-4e7e-98f4-7adaa9961edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store EMBEDDING_MODEL_NAME\n",
    "%store EMBED_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04f987-5478-40b4-be60-a978b7e9164f",
   "metadata": {},
   "source": [
    "Now deploy `Llama 3.1 8B` onto a SageMaker real-time endpoint for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c61ebb-c73b-42e6-b6b5-3ebff6e25bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id_llm, model_version = \"meta-textgeneration-llama-3-1-8b-instruct\", \"*\"\n",
    "accept_eula = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56e839-dc01-4cfe-9212-3e46044b03b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_instance_type = \"ml.g5.4xlarge\"\n",
    "generation_model = JumpStartModel(model_id=model_id_llm, model_version=model_version,instance_type=generation_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd3b77-7000-41c4-af67-18f7dcbf83fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_predictor = generation_model.deploy(accept_eula=accept_eula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1b459-3d6b-4372-b29c-5172e2a3a936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENERATION_ENDPOINT_NAME = generation_predictor.endpoint_name\n",
    "print(f\"Successfully deployed generation model to the SageMaker endpoint: {GENERATION_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f5d3ac-0715-4b53-a722-43407bd89a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store GENERATION_ENDPOINT_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
