{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker RAG retrieval and generation with SageMaker Inference and Bedrock Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab demonstrates how to enhance a Retrieval-Augmented Generation (RAG) pipeline by integrating Amazon SageMaker Inference with Amazon Bedrock Guardrails. We will walk through the process of querying a OpenSearch vector knowledge base, using SageMaker for model inference, applying Guardrails to control the generation of responses, and filtering results with metadata to ensure compliance and quality. We will use the same PubMed medical theme generated in the opensearch RAG lab where we will refer to the previously created opensearch vector database with PubMed dataset and show how guardrails can be used to filter the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "- Before proceeding with this notebook, you should complete all of the previous labs.\n",
    "- Required Python libraries: opensearch-py, langchain, boto3, requests_aws4auth\n",
    "- Access to Amazon SageMaker and OpenSearch\n",
    "- Appropriate IAM roles and permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain boto3 -q\n",
    "%pip install opensearch-py\n",
    "%pip install requests-aws4auth\n",
    "%pip install certifi\n",
    "print(\"Installs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError, BotoCoreError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session's IAM Role ARN: arn:aws:sts::329599663853:assumed-role/AmazonSageMaker-ExecutionRole-20250203T171097/SageMaker\n"
     ]
    }
   ],
   "source": [
    "# Opensearch Configuration\n",
    "aos_host = \"<ENTER_VALUE>\" # replace with the opensearch Domain endpoint (IPv4) from the previous lab (Without https://)\n",
    "OPENSEARCH_URL = f\"https://{aos_host}\"\n",
    "service = \"es\"  \n",
    "port = 443 # Enter the opensearch \n",
    "index_name = \"<ENTER_VALUE>\"  # Enter the opensearch index used in the ingestion from the previous lab\n",
    "\n",
    "# Sagemaker configuration\n",
    "SAGEMAKER_LLM_ENDPOINT_NAME = \"<ENTER_VALUE>\"  # Enter the sagemaker LLM endpoint created in the previous lab\n",
    "session = boto3.Session()\n",
    "sts_client = boto3.client('sts')\n",
    "# Get caller identity\n",
    "caller_identity = sts_client.get_caller_identity()\n",
    "\n",
    "# Extract and print the IAM role ARN\n",
    "iam_role_arn = caller_identity[\"Arn\"]\n",
    "account_id = sts_client.get_caller_identity().get('Account')\n",
    "region = session.region_name\n",
    "\n",
    "print(\"Session's IAM Role ARN:\", iam_role_arn)\n",
    "\n",
    "# Initialize the Amazon Bedrock client in the region\n",
    "bedrock = boto3.client('bedrock', region_name=region)\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Amazon Bedrock Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a Guardrail\n",
    "\n",
    "Bedrock Guardrails enable us to define policies that restrict or modify model responses based on compliance, safety, and contextual relevance. \n",
    "\n",
    "See AWS Bedrock Guardrails documentation for more details: https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html \n",
    "\n",
    "Note: Update the IAM role by adding the managed policy `arn:aws:iam::aws:policy/AmazonBedrockFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardrail ID: cb7wc871en5w\n",
      "Guardrail ARN: arn:aws:bedrock:us-west-2:329599663853:guardrail/cb7wc871en5w\n",
      "Version: DRAFT\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique client request token\n",
    "client_request_token = str(uuid.uuid4())\n",
    "\n",
    "# Create a Guardrail with specific filtering and compliance policies for medical use-case\n",
    "response = bedrock.create_guardrail(\n",
    "    name=\"MedicalContextGuardrails\",\n",
    "    description=\"Restrict responses to PubMed-based medical content only\",\n",
    "    blockedInputMessaging=\"This request cannot be processed due to safety protocols.\",\n",
    "    blockedOutputsMessaging=\"Response modified per compliance guidelines.\",\n",
    "\n",
    "    # Topic-based restrictions (e.g., denying non-medical advice)\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {'name': 'non-medical-advice', 'definition': 'Any recommendations outside medical expertise or context', 'type': 'DENY'},\n",
    "            {'name': 'misinformation', 'definition': 'Dissemination of inaccurate or unverified medical information', 'type': 'DENY'},\n",
    "            {'name': 'medical-cure-claims', 'definition': 'Claims of guaranteed or definitive cures for medical conditions without sufficient evidence', 'type': 'DENY'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Content filtering policies (e.g., blocking harmful or unethical content)\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {'type': 'HATE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'INSULTS', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'SEXUAL', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'VIOLENCE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'MISCONDUCT', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'PROMPT_ATTACK', 'inputStrength': 'HIGH', 'outputStrength': 'NONE'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Contextual grounding policies ensuring relevance to PubMed-based embeddings\n",
    "    contextualGroundingPolicyConfig={\n",
    "        # Ensure responses are grounded in the embeddings loaded from PubMed articles\n",
    "        'filtersConfig': [\n",
    "            {'type': 'GROUNDING', 'threshold': 0.1},\n",
    "            {'type': 'RELEVANCE', 'threshold': 0.1}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # List of restricted words related to sensitive medical topics\n",
    "    wordPolicyConfig={\n",
    "        # Example: blocking inappropriate usage of critical medical terms\n",
    "        'wordsConfig': [\n",
    "            {'text': \"malpractice\"}, {'text': \"misdiagnosis\"}, {'text': \"unauthorized treatment\"},\n",
    "            {'text': \"experimental drug\"}, {'text': \"unapproved therapy\"}, {'text': \"medical fraud\"},\n",
    "            {'text': \"cure\"}, {'text': \"guaranteed cure\"}, {'text': \"permanent remission\"}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Sensitive data anonymization (e.g., patient information)\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        # Anonymize identifiable patient information\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': \"NAME\", \"action\": \"ANONYMIZE\"}, {'type': \"EMAIL\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PHONE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_SOCIAL_SECURITY_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"ADDRESS\", \"action\": \"ANONYMIZE\"}, {'type': \"CA_HEALTH_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PASSWORD\", \"action\": \"ANONYMIZE\"}, {'type': \"IP_ADDRESS\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"CA_SOCIAL_INSURANCE_NUMBER\", \"action\": \"ANONYMIZE\"}, {'type': \"CREDIT_DEBIT_CARD_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"AGE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_BANK_ACCOUNT_NUMBER\", \"action\": \"ANONYMIZE\"}\n",
    "        ],\n",
    "        # Example regex patterns for anonymizing sensitive medical data\n",
    "        'regexesConfig': [\n",
    "            {\n",
    "                \"name\": \"medical_procedure_code\",\n",
    "                \"description\": \"Pattern for medical procedure codes\",\n",
    "                \"pattern\": \"\\\\b[A-Z]{1,5}\\\\d{1,5}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"clinical_trial_id\",\n",
    "                \"description\": \"Pattern for clinical trial identifiers\",\n",
    "                \"pattern\": \"\\\\bNCT\\\\d{8}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Tags for environment tracking\n",
    "    tags=[\n",
    "        {\"key\": \"Environment\", \"value\": \"Production\"},\n",
    "        {\"key\": \"Department\", \"value\": \"Medical\"}\n",
    "    ],\n",
    "    clientRequestToken=client_request_token\n",
    ")\n",
    "\n",
    "# Retrieve and print the Guardrail ID, ARN, and version\n",
    "guardrail_id = response['guardrailId']\n",
    "print(f\"Guardrail ID: {guardrail_id}\")\n",
    "print(f\"Guardrail ARN: {response['guardrailArn']}\")\n",
    "print(f\"Version: {response['version']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a Published Version of the Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create a published version\n",
    "version_response = bedrock.create_guardrail_version(\n",
    "    guardrailIdentifier=response['guardrailId'],\n",
    "    description=\"Production version 1.0\"\n",
    ")\n",
    "guardrail_version=version_response['version']\n",
    "guardrail_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define function to apply bedrock guardrail at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_output_guardrail(output_text):\n",
    "    \"\"\"Apply guardrails to the output after generation\"\"\"\n",
    "    print(\"Apply bedrock guardrails to the input using\", guardrail_id ,guardrail_version)\n",
    "    try:\n",
    "        # Use only the parameters supported by your boto3 version\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion=guardrail_version,\n",
    "            source='OUTPUT',\n",
    "            content=[\n",
    "                {\n",
    "                    'text': {\n",
    "                        'text': output_text\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Process response based on what fields are available\n",
    "        if 'outputs' in response and response['outputs']:\n",
    "            return response['outputs'][0]['text']\n",
    "        else:\n",
    "            return output_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Output guardrail application failed: {str(e)}\")\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Define SageMaker functions\n",
    "We use the similar approach for OpenSearch retrieval and SageMaker inference as defined in the previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Answer the question using the provided context.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        print(\"Input prompt:\", input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        # Parse and extract generated text from response\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        print(\"Raw response:\", response_json)  # Debugging\n",
    "        # Handle different response formats\n",
    "        return response_json[\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=SAGEMAKER_LLM_ENDPOINT_NAME,\n",
    "        region_name=region,\n",
    "        model_kwargs={\"temperature\": 1e-10},\n",
    "        content_handler=content_handler,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: {\"inputs\": \"Answer the question using the provided context.\\nQuestion: What is the role of mitochondrial dynamics in programmed cell death in lace plants?\\nContext: \\nBased on research into Aponogeton madagascariensis (lace plant), programmed cell death (PCD) occurs in the cells at the center of areoles in leaves.\\nThe role of mitochondrial dynamics during this process is being investigated.\\n\\nAnswer:\", \"parameters\": {\"temperature\": 1e-10}}\n",
      "Raw response: {'generated_text': ' The role of mitochondrial dynamics in programmed cell death in lace plants is currently under investigation. Research is focused on understanding how changes in mitochondrial shape and function contribute'}\n",
      " The role of mitochondrial dynamics in programmed cell death in lace plants is currently under investigation. Research is focused on understanding how changes in mitochondrial shape and function contribute\n"
     ]
    }
   ],
   "source": [
    "# Updated prompt and context for medical question\n",
    "\n",
    "prompt = \"What is the role of mitochondrial dynamics in programmed cell death in lace plants?\"\n",
    "context_prompt = \"\"\"\n",
    "Based on research into Aponogeton madagascariensis (lace plant), programmed cell death (PCD) occurs in the cells at the center of areoles in leaves.\n",
    "The role of mitochondrial dynamics during this process is being investigated.\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = PROMPT.format(question=prompt, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "response = sagemaker_llm(input_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. OpenSearch retrieval \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define the OpenSearch vector database retrieval\n",
    "Note: The IAM role used will need the managed permissions `arn:aws:iam::aws:policy/AmazonOpenSearchServiceFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 17:45:08.962730: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenSearch Authentication setup\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    region,\n",
    "    service,\n",
    "    session_token=credentials.token\n",
    ")\n",
    "\n",
    "# Initialize OpenSearch client\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts=[{'host': aos_host, 'port': port}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: {'.plugins-ml-model-group': {'aliases': {}}, 'gtm-rag-index': {'aliases': {}}, '.plugins-flow-framework-state': {'aliases': {}}, '.ql-datasources': {'aliases': {}}, '.plugins-ml-agent': {'aliases': {}}, '.plugins-flow-framework-templates': {'aliases': {}}, '.plugins-ml-task': {'aliases': {}}, '.kibana_1': {'aliases': {'.kibana': {}}}, '.opendistro_security': {'aliases': {}}, '.plugins-ml-config': {'aliases': {}}, '.opensearch-observability': {'aliases': {}}, '.plugins-ml-model': {'aliases': {}}, '.opensearch-sap-log-types-config': {'aliases': {}}, '.plugins-flow-framework-config': {'aliases': {}}}\n"
     ]
    }
   ],
   "source": [
    "# Test opensearch connection by listing indices\n",
    "try:\n",
    "    response = opensearch_client.indices.get_alias(\"*\")\n",
    "    print(\"Indices:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to OpenSearch:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding model\n",
    "embedder = pipeline(\"feature-extraction\", model=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# RAG retrieval function\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = embedder(query)[0][0]\n",
    "    \n",
    "    search_body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"vector\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = opensearch_client.search(\n",
    "        index=index_name,\n",
    "        body=search_body\n",
    "    )\n",
    "    \n",
    "    return [hit[\"_source\"][\"text\"] for hit in response[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: ['ID: 25819796 - Cerebral hemispherectomy, a surgical procedure undergone to control intractable seizures, is becoming a standard procedure with more cases identified and treated early in life [33]. While the effect of the dominant hemisphere resection on spoken language has been extensively researched, little is known about reading abilities in individuals after left-sided resection. Left-lateralized phonological abilities are the key components of reading, i.e., grapheme-phoneme conversion skills [1]. These skills are critical for the acquisition of word-specific orthographic knowledge and have been shown to predict reading levels in average readers as well as in readers with mild cognitive disability [26]. Furthermore, impaired phonological processing has been implicated as the cognitive basis in struggling readers. Here, we explored the reading skills in participants who have undergone left cerebral hemispherectomy.', 'ID: 25819796 - Seven individuals who have undergone left cerebral hemispherectomy to control intractable seizures associated with perinatal infarct have been recruited for this study. We examined if components of phonological processing that are shown to reliably separate average readers from struggling readers, i.e., phonological awareness, verbal memory, speed of retrieval, and size of vocabulary, show the same relationship to reading levels when they are mediated by the right hemisphere [2].', 'ID: 25819796 - We found that about 60% of our group developed both word reading and paragraph reading in the average range. Phonological processing measured by both phonological awareness and nonword reading was unexpectedly spared in the majority of participants. Phonological awareness levels strongly correlated with word reading. Verbal memory, a component of phonological processing skills, together with receptive vocabulary size, positively correlated with reading levels similar to those reported in average readers. Receptive vocabulary, a bilateral function, was preserved to a certain degree similar to that of strongly left-lateralized phonological skills [3]. Later seizure onset was associated with better reading levels.']\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant context\n",
    "query = \" what are the key components of phonological processing that are believed to influence \\\n",
    "reading levels in individuals who have undergone cerebral hemispherectomy procedure?\"\n",
    "contexts = retrieve_context(query)\n",
    "print(\"Retrieved context:\", contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Retrieve context for input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opensearch_vector_context(input_query):\n",
    "    # Retrieve relevant context\n",
    "    query = \"What is Amazon?\"\n",
    "    contexts = retrieve_context(input_query)\n",
    "    print(\"Retrieved context: \", contexts)\n",
    "\n",
    "    # Handle empty or single returned list\n",
    "    if contexts:\n",
    "        context_prompt = contexts[0] if len(contexts) > 0 else None\n",
    "    else:\n",
    "        context_prompt = None\n",
    "    \n",
    "    return context_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validate inference results with Bedrock guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Restrict reponse based on blocked content/topic\n",
    "Lets ask the Foundational model for an investment advice. When we created the guardrails, we restricted bedrock to provide any investment advice. Bedrock should be return a preconfigured response \"This request cannot be processed due to safety protocols\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context:  [\"ID: 9444542 - To investigate whether the presence of hippocampal atrophy (HCA) on MRI in Alzheimer's disease (AD) leads to a more rapid decline in cognitive function. To investigate whether cognitively unimpaired controls and depressed subjects with HCA are at higher risk than those without HCA of developing dementia.\", 'ID: 18378554 - Ambulatory residents meeting DSM-IV criteria for dementia (N = 181) were studied.', 'ID: 9444542 - No significant differences in rate of cognitive decline, mortality or progression to dementia were found between subjects with or without HCA.']\n",
      "Input prompt: {\"inputs\": \"Answer the question using the provided context.\\nQuestion: Is there a guaranteed cure for Alzheimer's disease based on the latest research?\\nContext: ID: 9444542 - To investigate whether the presence of hippocampal atrophy (HCA) on MRI in Alzheimer's disease (AD) leads to a more rapid decline in cognitive function. To investigate whether cognitively unimpaired controls and depressed subjects with HCA are at higher risk than those without HCA of developing dementia.\\nAnswer:\", \"parameters\": {\"temperature\": 1e-10}}\n",
      "Raw response: {'generated_text': \" Based on the provided context, there is no information about a guaranteed cure for Alzheimer's disease. The context focuses on the relationship between hippocampal at\"}\n",
      "\n",
      " Initial response without guardrails:   Based on the provided context, there is no information about a guaranteed cure for Alzheimer's disease. The context focuses on the relationship between hippocampal at\n",
      "Apply bedrock guardrails to the input using cb7wc871en5w 1\n",
      "\n",
      " User's query:  Is there a guaranteed cure for Alzheimer's disease based on the latest research?\n",
      "\n",
      " Answer with guardrails:  Response modified per compliance guidelines.\n"
     ]
    }
   ],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"Is there a guaranteed cure for Alzheimer's disease based on the latest research?\"\n",
    "\n",
    "# Retrieve relevant context from the OpenSearch based on the query\n",
    "# TODO Update the context retrieval function\n",
    "context_prompt = get_opensearch_vector_context(query)  # Replace with your OpenSearch retrieval\n",
    "\n",
    "input_prompt = PROMPT.format(question=query, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "raw_response = sagemaker_llm(input_prompt)\n",
    "print(\"\\n Initial response without guardrails: \", raw_response)\n",
    "\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"\\n User's query: \", query)  # Use the variable directly, not a dictionary\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"\\n Answer with guardrails: \", guardrail_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Restrict reponse based on PII Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context:  ['ID: 23949294 - Of 619 patients on follow-up, 82 (13.2%) were diagnosed in the pre-HAART era. At the time of our study, 79 (96.3%) patients were on HAART, with a median duration of 14 years (IQR 12-15) of therapy, and exposure to mono or dual nucleoside reverse transcriptase inhibitors regimens in 47.8% of cases.\\xa0Sixty-nine patients (87.3%) had undetectable VL, 37 (46.8%) never presented virologic failure, and 19 (24.1%) experienced only one failure. Thirteen patients (16.5%) were receiving third-line ART regimens, with an average of 2.7-fold more virologic failures than those on first- or second-line regimens (p = 0.007).', \"ID: 21252642 - It was hypothesized that patients' satisfaction with information regarding clinical trials would improve after targeted educational interventions, and accruals to clinical trials would increase in the year following those interventions.\", 'ID: 21252642 - Patient satisfaction with information significantly increased after the interventions. There was no increase in subsequent enrollment in clinical trials. Patients who indicated an inclination to participate in clinical trials tended to have greater satisfaction with the information they received.']\n",
      "Input prompt: {\"inputs\": \"Answer the question using the provided context.\\nQuestion: Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, who led the clinical trial NCT12345678?\\nContext: ID: 23949294 - Of 619 patients on follow-up, 82 (13.2%) were diagnosed in the pre-HAART era. At the time of our study, 79 (96.3%) patients were on HAART, with a median duration of 14 years (IQR 12-15) of therapy, and exposure to mono or dual nucleoside reverse transcriptase inhibitors regimens in 47.8% of cases.\\u00a0Sixty-nine patients (87.3%) had undetectable VL, 37 (46.8%) never presented virologic failure, and 19 (24.1%) experienced only one failure. Thirteen patients (16.5%) were receiving third-line ART regimens, with an average of 2.7-fold more virologic failures than those on first- or second-line regimens (p = 0.007).\\nAnswer:\", \"parameters\": {\"temperature\": 1e-10}}\n",
      "Raw response: {'generated_text': \" I'm sorry, but I cannot provide the contact information for Dr. Vivek Murthy or any other individual. If you need to contact him,\"}\n",
      "\n",
      " Initial response without guardrails:   I'm sorry, but I cannot provide the contact information for Dr. Vivek Murthy or any other individual. If you need to contact him,\n",
      "Apply bedrock guardrails to the input using cb7wc871en5w 1\n",
      "\n",
      " User's query:  Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, who led the clinical trial NCT12345678?\n",
      "\n",
      " Answer with guardrails:   I'm sorry, but I cannot provide the contact information for Dr. {NAME} or any other individual. If you need to contact him,\n"
     ]
    }
   ],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, \\\n",
    "who led the clinical trial NCT12345678?\"\n",
    "\n",
    "# Retrieve relevant context from the OpenSearch based on the query\n",
    "# TODO Update the context retrieval function\n",
    "context_prompt = get_opensearch_vector_context(query) # Replace with your OpenSearch retrieval\n",
    "\n",
    "input_prompt = PROMPT.format(question=query, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "raw_response = sagemaker_llm(input_prompt)\n",
    "print(\"\\n Initial response without guardrails: \", raw_response)\n",
    "\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"\\n User's query: \", query) # Use the variable directly, not a dictionary\n",
    "\n",
    "# Print the generated answer from the model based on the query and context, with applying the guardrails\n",
    "print(\"\\n Answer with guardrails: \", guardrail_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
