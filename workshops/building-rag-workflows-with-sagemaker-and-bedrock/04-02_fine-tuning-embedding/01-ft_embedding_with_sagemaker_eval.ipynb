{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b20107",
   "metadata": {},
   "source": [
    "# Fine-Tuning Sentence Transformers on SageMaker\n",
    "This notebook shows how to launch a SageMaker training job using a `trainer.py` script for fine-tuning a Sentence Transformer model on custom data. You will also do some preliminary evaluation in this notebook, and additional evaluation in the `02-embeddings-eval.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ef5ed",
   "metadata": {},
   "source": [
    "## Setup Dependencies\n",
    "Initialize the SageMaker session and retrieve the execution role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bc8ba-a5e4-45c8-bf92-7780bcc07dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sentence-transformers==3.1.1 datasets==2.19.2 transformers==4.40.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Initialize SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751b801-c95b-4d72-aee8-e16f29b50df0",
   "metadata": {},
   "source": [
    "In this evaluation, you'll pull samples from the [PubMedQA dataset](https://huggingface.co/datasets/qiaojin/PubMedQA). It has sets of prebuilt Question/Context/Answers on complex medical topics which will be used to tune the embeddings to the medical domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6175c53-0c49-4c86-af22-98f54f5fcb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
    "source_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee67c21-5dc0-4e16-8acd-e3a686f89617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_dataset, output_filename, max_items=-1):\n",
    "    output_data = []\n",
    "\n",
    "    if max_items > -1:\n",
    "        print(f\"max_items set, reducing input to {max_items} items.\")\n",
    "    else:\n",
    "        max_items = len(input_dataset)\n",
    "    \n",
    "    for idx, item in enumerate(input_dataset.select(range(max_items))):\n",
    "        data_item = {\n",
    "            \"id\": item[\"pubid\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"context\": item[\"context\"][\"contexts\"][0]\n",
    "        }\n",
    "        output_data.append(data_item)\n",
    "\n",
    "        print(f\"item: {idx+1}\", end=\"\\r\")\n",
    "        \n",
    "    #write training data to an output file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46085ad-2582-45de-bbbf-1594d08fbf34",
   "metadata": {},
   "source": [
    "This will take 9000 items from the source dataset to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f883d4c5-a0bd-45d1-93ee-d003304923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(source_dataset[\"train\"],\"./data/base_data/base_data.json\", max_items=9000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b47c9-dc75-4e26-9ab9-244f6e6151c2",
   "metadata": {},
   "source": [
    "Split the test and train datasets, shuffling the source and doing a 90/10 split, then upload to S3 to be used in a SageMaker managed training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a9dff-b487-430e-9585-d350385125ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = load_dataset(\"json\", data_files=\"./data/base_data/base_data.json\", split=\"train\")\n",
    "\n",
    "training_dataset = training_dataset.train_test_split(test_size=0.10, shuffle=True)\n",
    "\n",
    "prefix = \"embedding-finetuning\" \n",
    "s3_output_path = f\"s3://{bucket}/{prefix}/output\"\n",
    "\n",
    "local_data_path = f\"./data/{prefix}\"\n",
    "s3_data_path = f\"s3://{bucket}/{prefix}\"\n",
    "\n",
    "training_dataset[\"train\"].to_json(f\"{local_data_path}/train/train.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{s3_data_path}/train/train.json\"\n",
    "training_dataset[\"train\"].to_json(train_dataset_s3_path, orient=\"records\")\n",
    "\n",
    "training_dataset[\"test\"].to_json(f\"{local_data_path}/test/test.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{s3_data_path}/test/test.json\"\n",
    "training_dataset[\"test\"].to_json(test_dataset_s3_path, orient=\"records\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"\\nYou can view the uploaded dataset in the console here: \\nhttps://s3.console.aws.amazon.com/s3/buckets/{sagemaker_session.default_bucket()}/?region={sagemaker_session.boto_region_name}&prefix={s3_data_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f4bba7",
   "metadata": {},
   "source": [
    "## Configure PyTorch Estimator\n",
    "We configure the training job to run `trainer.py` with the desired hyperparameters. \n",
    "\n",
    "Here you configure:\n",
    "- the link to your training script\n",
    "- any library upgrades necessary\n",
    "- the IAM role for the training job to assume, providing it access to the training data and other resources\n",
    "- instance type and count to be used in the training job\n",
    "- pytorch versions (since you are using the pytorch estimator here)\n",
    "- training hyperparameters (# of training epochs, training batch size, base model to use for training)\n",
    "- `keep_alive_period_in_seconds` if you want to use SageMaker warm pools between iterative runs.\n",
    "\n",
    ">Note: when configuring the training job, adding additional data seemed to perform better than adding more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf948cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"scripts/trainer.py\",\n",
    "    source_dir=\".\",\n",
    "    requirements_file=\"requirements.txt\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    framework_version=\"2.2.0\",     # ‚úÖ Supports SDPA and FlashAttention-2\n",
    "    py_version=\"py310\",            # ‚úÖ Python 3.10 for modern libraries\n",
    "    hyperparameters={\n",
    "        \"epochs\": 4,\n",
    "        \"batch_size\": 16,\n",
    "        \"model_name\": \"Alibaba-NLP/gte-base-en-v1.5\"\n",
    "    },\n",
    "    output_path=s3_output_path,\n",
    "    base_job_name=\"embedding-finetune\",\n",
    "    keep_alive_period_in_seconds=1800\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205515d9",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "This command will start the SageMaker training job using the uploaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae21fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({\n",
    "    \"train\": TrainingInput(train_dataset_s3_path, content_type=\"application/json\"),\n",
    "    \"validation\": TrainingInput(test_dataset_s3_path, content_type=\"application/json\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96591e12",
   "metadata": {},
   "source": [
    "## üîç Evaluate Tuned Model from the SageMaker Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc4349-4669-4852-837d-750fcaf5d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9206b3-d291-4fa2-99d8-e1b64952b8a9",
   "metadata": {},
   "source": [
    "Set the `max_items` parameter to choose how large of a test to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99933dec-2a72-499d-b3f6-fb16b1df69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
    "\n",
    "process_dataset(source_dataset[\"train\"], \"./data/test_full.json\", max_items=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501952f1-0313-49fc-affc-2ba06daa1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/test_full.json\", split=\"train\")\n",
    "# Add an id column to the dataset\n",
    "#dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "# split dataset into a 10% test set\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    " \n",
    "# save datasets to disk\n",
    "dataset[\"train\"].to_json(\"./data/test_train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"./data/test_test_dataset.json\", orient=\"records\")\n",
    "\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6886c-9dbf-4699-aa38-98928a43c60b",
   "metadata": {},
   "source": [
    "Here you are taking the full test and training dataset and assembling it into a document corpus that you can use for evaluation. This is a subset of the overall corpus (which you can choose to run against later in the notebook.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f271727-fe14-4713-9866-3e865513833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset again\n",
    "train_dataset = load_dataset(\"json\", data_files=\"./data/test_train_dataset.json\", split=\"train\")\n",
    "test_dataset = load_dataset(\"json\", data_files=\"./data/test_test_dataset.json\", split=\"train\")\n",
    "corpus_dataset = concatenate_datasets([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37a80a-f3f3-478c-a8af-f3326a3a73f4",
   "metadata": {},
   "source": [
    "### Evaluate Base Model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6117029-4c17-4f12-a827-57d4faf7d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "base_model_id = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
    "base_model_id_safe = base_model_id.replace(\"/\",\"_\")\n",
    "\n",
    "# Evaluate the BASE model\n",
    "model = SentenceTransformer(\n",
    "    base_model_id, \n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a38db-aeb3-402e-836c-4a258c92964a",
   "metadata": {},
   "source": [
    "Here you are setting up the dimensions of the vectors to evaluate. The `matryoshka_dimensions` need to be in descending order, with the maximum dimension not exceeding the maximum of source model. This is supplied to create the loss function for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64789a-9857-4a09-9c4c-ea7afb05a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "# Important: large to small, the max dimension cannot be greater than the embedding model's max\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b5e4d2-42ec-4842-90fc-e6f1c09e7c0f",
   "metadata": {},
   "source": [
    "In this section you are taking the full document corpus to search against, along with their ids for validation of accuracy, and a set of queries to be the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9c248-a1e7-401c-96c2-4eac322dd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datasets to dictionaries\n",
    "corpus = dict(\n",
    "    zip(corpus_dataset[\"id\"], corpus_dataset[\"context\"])\n",
    ")  # Our corpus (cid => document)\n",
    "queries = dict(\n",
    "    zip(test_dataset[\"id\"], test_dataset[\"question\"])\n",
    ")  # Our queries (qid => question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967c0b4-4bb9-4b75-8f64-7400ffa095b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = {}  # Query ID to relevant documents (qid => set([relevant_cids])\n",
    "for q_id in queries:\n",
    "    relevant_docs[q_id] = [q_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e47a2-a6c8-4684-a6af-021e99f89827",
   "metadata": {},
   "outputs": [],
   "source": [
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": cos_sim},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    " \n",
    "# Create a sequential evaluator\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30f862-4b25-4b2f-9e06-a5955081468a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the BASE model\n",
    "model = SentenceTransformer(\n",
    "    base_model_id, \n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "base_results = evaluator(model)\n",
    "\n",
    "print(\"===============\\nBASE MODEL\\n===============\")\n",
    "\n",
    "# # COMMENT IN for full results\n",
    "# print(base_results)\n",
    " \n",
    "# Print the main score\n",
    "import pandas as pd\n",
    "data = {'dimension':[], 'base': []}\n",
    "\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    data['dimension'].append(key)\n",
    "    data['base'].append(base_results[key])\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b2067-b0e1-46c0-87e7-61cd77ad3a84",
   "metadata": {},
   "source": [
    "### Evaluate Tuned Model\n",
    "\n",
    "This will grab the output model artifact from the training job, download it, then unpack it locally so it can be used for quick evaluation. You can skip this section if you already have model artifacts downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac57866-50bd-465a-abee-b6cb57d7ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Step 1: Attach to the completed training job\n",
    "job_name = \"<<YOUR_TRAINING_JOB_NAME>>\"  # <-- replace with your actual job name\n",
    "estimator = Estimator.attach(job_name)\n",
    "\n",
    "# Step 2: Download the model tar.gz file\n",
    "model_tar_path = estimator.model_data\n",
    "local_model_path = \"./downloaded_model\"\n",
    "os.makedirs(local_model_path, exist_ok=True)\n",
    "\n",
    "# Step 3: Download and extract\n",
    "s3 = sagemaker.Session().boto_session.resource(\"s3\")\n",
    "bucket, key = model_tar_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "s3.Bucket(bucket).download_file(key, f\"{local_model_path}/model.tar.gz\")\n",
    "\n",
    "with tarfile.open(f\"{local_model_path}/model.tar.gz\") as tar:\n",
    "    tar.extractall(path=local_model_path)\n",
    "\n",
    "print(\"‚úÖ Model downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4512561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator, SequentialEvaluator\n",
    "import torch\n",
    "\n",
    "# Adjust path if you're pointing to model artifacts from SageMaker output\n",
    "tuned_model_path = \"downloaded_model\"\n",
    "model = SentenceTransformer(\n",
    "    tuned_model_path,\n",
    "    model_kwargs={\"attn_implementation\": \"sdpa\"},\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83bb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the TUNED model\n",
    "tuned_results = evaluator(model)\n",
    "\n",
    "print(\"===============\\nTUNED MODEL\\n===============\")\n",
    " \n",
    "# # COMMENT IN for full results\n",
    "# print(tuned_results)\n",
    " \n",
    "# Print the main score\n",
    "import pandas as pd\n",
    "data = {'dimension':[], 'tuned': []}\n",
    "\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    data['dimension'].append(key)\n",
    "    data['tuned'].append(tuned_results[key])\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bcb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare base vs tuned\n",
    "data = {'dimension':[], 'base': [], 'tuned': [], 'delta': [], 'delta_percent': []}\n",
    "for dim in matryoshka_dimensions:\n",
    "    key = f\"dim_{dim}_cosine_ndcg@10\"\n",
    "    delta = tuned_results[key] - base_results[key]\n",
    "    delta_percent = (delta / base_results[key]) * 100\n",
    "    data['dimension'].append(key)\n",
    "    data['base'].append(base_results[key])\n",
    "    data['tuned'].append(tuned_results[key])\n",
    "    data['delta'].append(delta)\n",
    "    data['delta_percent'].append(delta_percent)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42d86b-b32f-40ff-8f26-c7deeb250b36",
   "metadata": {},
   "source": [
    "In this comparison of the base versus tuned model, you can see improvements in retrieval in every metric and every dimension. Higher dimensionality across the board had less improvements, while lower dimensionality showed significant gains. Note that the 64 dimension results for the tuned model are actually as good or better than the 768 dimenstion results. If implemented, this could lead to significant improvements in search performance, or a 2-tier system where the first pass over the dataset is done at low dimensionality and then the resultset is evaluated at full dimensionality.\n",
    "\n",
    "- NDCG (Normalized Discounted Cumulative Gain) is a metric used to evaluate the ranking of embeddings, which are numerical representations of objects like words or documents. It measures how well the ranking of embeddings corresponds to the expected or desired ranking, taking into account both the relevance of the embeddings and their position in the ranking.\n",
    "\n",
    "- Accuracy measures the proportion of correctly classified or identified embeddings out of the total number of embeddings.\n",
    "\n",
    "- Precision measures the proportion of relevant embeddings among the top-ranked or recommended embeddings.\n",
    "\n",
    "- Recall measures the proportion of relevant embeddings that are successfully retrieved or recommended out of the total number of relevant embeddings.\n",
    "\n",
    "- Mean Reciprocal Rank (MRR) evaluates how well an embedding model ranks the most relevant items. It focuses on the position of the first relevant item in the ranked list of embeddings.\n",
    "\n",
    "    The reciprocal rank is calculated as 1 divided by the rank of the first relevant item. For example, if the first relevant item is ranked 3rd, the reciprocal rank would be 1/3. MRR is then calculated as the average of these reciprocal ranks across multiple queries or test cases. \n",
    "\n",
    "Improvements\n",
    "---\n",
    "- Normalized Dicounted Cumulative Gain (NDCG): 9.45% to 29.3%\n",
    "- Accuracy: 5.5% to 21.6%\n",
    "- Precision: 5.5% to 21.6%\n",
    "- Recall: 5.5% to 21.6%\n",
    "- MRR: 10.8% to 32.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deffcb-de8a-4bde-b0fc-7ea4812e0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Compare base vs tuned\n",
    "data = {'dimension':[], 'base': [], 'tuned': [], 'delta': [], 'delta_percent': []}\n",
    "metrics = [\"ndcg\", \"accuracy\", \"precision\", \"recall\", \"mrr\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    for dim in matryoshka_dimensions:\n",
    "        key = f\"dim_{dim}_cosine_{metric}@10\"\n",
    "        delta = tuned_results[key] - base_results[key]\n",
    "        delta_percent = (delta / base_results[key]) * 100\n",
    "        data['dimension'].append(key)\n",
    "        data['base'].append(base_results[key])\n",
    "        data['tuned'].append(tuned_results[key])\n",
    "        data['delta'].append(delta)\n",
    "        data['delta_percent'].append(delta_percent)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021705b-2d1d-4c15-b210-b5b9c52a5404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
