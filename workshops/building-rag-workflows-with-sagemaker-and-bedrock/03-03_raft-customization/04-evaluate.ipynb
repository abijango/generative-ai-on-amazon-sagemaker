{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae8f86-38ea-4d88-81de-f8d20a6ad9ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -Uq evaluate\n",
    "%pip install -Uq rouge-score\n",
    "%pip install -Uq bleu\n",
    "%pip install -Uq ragas\n",
    "%pip install -Uq langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c252e-eacd-4b09-b2e3-f735c7867cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import pandas as pd  # For working with tabular data\n",
    "import boto3, uuid\n",
    "from botocore.response import StreamingBody\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from random import sample\n",
    "from asyncio import run\n",
    "\n",
    "import evaluate as hf_evaluate\n",
    "\n",
    "# Langchain\n",
    "from langchain_aws.chat_models.sagemaker_endpoint import ChatSagemakerEndpoint, ChatModelContentHandler\n",
    "from langchain_core.messages import HumanMessage, AIMessageChunk, SystemMessage\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "\n",
    "\n",
    "# Sagemaker\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# RAGAS\n",
    "import ragas\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac7fc6-b62c-4bee-b018-a76eb5104d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the BLEU evaluation metric\n",
    "bleu = hf_evaluate.load(\"bleu\")\n",
    "rouge = hf_evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd385d8b-b794-45ae-a6be-9ece2ed1c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = load_dataset(\"json\", data_files=\"eval.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa29fe45-91b8-4618-89d0-9c7b62c54cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "base_predictions = []\n",
    "sft_predictions = []\n",
    "\n",
    "for eval_item in evaluation_dataset:\n",
    "\n",
    "    ground_truth.append(eval_item[\"ground_truth\"])\n",
    "    base_predictions.append(eval_item['base'])\n",
    "    sft_predictions.append(eval_item['tuned'])\n",
    "\n",
    "\n",
    "base_bleu_results = bleu.compute(predictions=base_predictions, references=ground_truth)\n",
    "base_rouge_results = rouge.compute(predictions=base_predictions, references=ground_truth)\n",
    "\n",
    "# Compute the BLEU score\n",
    "sft_bleu_results = bleu.compute(predictions=sft_predictions, references=ground_truth)\n",
    "sft_rouge_results = rouge.compute(predictions=sft_predictions, references=ground_truth)\n",
    "\n",
    "base_scores = (base_bleu_results | base_rouge_results)\n",
    "sft_scores = (sft_bleu_results | sft_rouge_results)\n",
    "\n",
    "print(\"=======BASE MODEL=======\")\n",
    "print(base_scores)\n",
    "print(\"=======TUNED MODEL=======\")\n",
    "print(sft_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f795b8b-0252-415c-a338-3d3e1f0d9df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'dimension':[], 'base': [], 'tuned': [], 'delta': [], 'delta_percent': []}\n",
    "\n",
    "for key in base_scores.keys():\n",
    "    if key in [\"length_ratio\",\"precisions\",\"brevity_penalty\",\"translation_length\",\"reference_length\"]:\n",
    "        continue\n",
    "        \n",
    "    delta = sft_scores[key]-base_scores[key]\n",
    "    delta_percent = (delta/base_scores[key])*100\n",
    "    \n",
    "    data['dimension'].append(key)\n",
    "    data['base'].append(base_scores[key])\n",
    "    data['tuned'].append(sft_scores[key])\n",
    "    data['delta'].append(delta)\n",
    "    data['delta_percent'].append(delta_percent)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531148c-a12b-44b2-b94a-2154255bc71a",
   "metadata": {},
   "source": [
    "## LLM-as-a-judge Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225530c-9fb1-4fdf-b242-f2a4c92e6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  version=\"3.0.1\"\n",
    ")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "hub = {\n",
    "    'HF_TASK': 'text-generation', \n",
    "    'HF_MODEL_ID': 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "}\n",
    "\n",
    "model_for_deployment = HuggingFaceModel(\n",
    "    #model_data=s3_location,\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    image_uri=llm_image,\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(\"qwen25\")\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "model_for_deployment.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    routing_config = {\n",
    "        \"RoutingStrategy\":  sagemaker.enums.RoutingStrategy.LEAST_OUTSTANDING_REQUESTS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb850cf9-76a0-42fa-b769-1ff75ce4ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface-tei\",\n",
    "  version=\"1.4\"\n",
    ")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "hub = {\n",
    "    #'HF_TASK': 'text-generation', \n",
    "    'HF_MODEL_ID': 'Alibaba-NLP/gte-large-en-v1.5'\n",
    "}\n",
    "\n",
    "embedding_model_for_deployment = HuggingFaceModel(\n",
    "    #model_data=s3_location,\n",
    "    role=role,\n",
    "    env=hub,\n",
    "    image_uri=llm_image,\n",
    ")\n",
    "\n",
    "endpoint_name = name_from_base(\"gte-large-en-v1-5\")\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 300\n",
    "\n",
    "embedding_model_for_deployment.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=health_check_timeout,\n",
    "    routing_config = {\n",
    "        \"RoutingStrategy\":  sagemaker.enums.RoutingStrategy.LEAST_OUTSTANDING_REQUESTS\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fa676f-3baf-4d9b-9a01-1551bd2940e4",
   "metadata": {},
   "source": [
    "### Deploy Qwen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb0a5f-a5c8-4284-be7d-01b295bc6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "metrics=[\n",
    "        ragas.metrics.answer_relevancy,\n",
    "        ragas.metrics.faithfulness,\n",
    "        ragas.metrics.context_precision,\n",
    "        ragas.metrics.context_recall,\n",
    "        ragas.metrics.answer_similarity,\n",
    "        ragas.metrics.answer_correctness,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3619d2fe-56ab-4222-9a97-073a0bc8041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f8b76-9f15-49a4-8c68-3bc6dc715150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "\n",
    "sm = boto3.Session().client('sagemaker-runtime', config=Config(max_pool_connections=20))\n",
    "endpoint_name = \"qwen25-judge-model\"\n",
    "\n",
    "class ChatContentHandler(ChatModelContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt, model_kwargs: Dict) -> bytes:\n",
    "        body = {\n",
    "            \"messages\": prompt,\n",
    "            \"stream\": True,\n",
    "            **model_kwargs  # Ensure all model parameters are passed\n",
    "        }\n",
    "        return json.dumps(body).encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: StreamingBody) -> AIMessageChunk:\n",
    "        stop_token = \"[DONE]\"\n",
    "        try:\n",
    "            all_content = []\n",
    "\n",
    "            # Process streaming response line by line\n",
    "            for line in output.iter_lines():\n",
    "                if line:\n",
    "                    line = line.decode(\"utf-8\").strip()\n",
    "\n",
    "                    # Skip empty lines or lines without \"data:\"\n",
    "                    if not line.startswith(\"data:\"):\n",
    "                        continue\n",
    "\n",
    "                    # Validate and parse JSON\n",
    "                    try:\n",
    "                        json_data = json.loads(line[6:])\n",
    "                        \n",
    "                    except json.JSONDecodeError as e:\n",
    "                        #print(f\"Skipping invalid JSON chunk: {line}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for stop token\n",
    "                    if json_data.get(\"choices\", [{}])[0].get(\"delta\", {}).get(\"content\") == stop_token:\n",
    "                        break\n",
    "                    \n",
    "                    # Extract content and append to the list\n",
    "                    content = json_data[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                    all_content.append(content)\n",
    "\n",
    "            # Join all chunks into a single string\n",
    "            full_response = \"\".join(all_content)\n",
    "            return AIMessageChunk(content=full_response)\n",
    "        except Exception as e:\n",
    "            return AIMessageChunk(content=f\"Error processing response: {str(e)}\")\n",
    "\n",
    "\n",
    "chat_content_handler = ChatContentHandler()\n",
    "\n",
    "llm = ChatSagemakerEndpoint(\n",
    "    name=\"Testmodel\",\n",
    "    endpoint_name=endpoint_name,\n",
    "    client=sm,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,  # Adjust temperature for balanced randomness\n",
    "        \"max_new_tokens\": 1200,  # Ensure sufficient token generation\n",
    "        \"top_p\": 0.95,  # Use nucleus sampling for diversity\n",
    "        \"do_sample\": True  # Enable sampling for generative tasks\n",
    "    },\n",
    "    content_handler=chat_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86ea2af-29a6-4ecb-acf6-50041aa5f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "class EmbedContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            inputs: List of input strings.\n",
    "            model_kwargs: Additional keyword arguments to be passed to the endpoint.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        # Example: inference.py expects a JSON string with a \"inputs\" key:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        # Example: inference.py returns a JSON string with the list of\n",
    "        # embeddings in a \"vectors\" key:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json #response_json[\"vectors\"]\n",
    "\n",
    "\n",
    "embeddings_content_handler = EmbedContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137da7b-91b5-4f44-bc24-6cc00d321dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_endpoint_name = \"gte-large-en-v1-5-embedding\"\n",
    "\n",
    "embed = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=embed_endpoint_name,\n",
    "    client=sm,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,  # Adjust temperature for balanced randomness\n",
    "        \"max_new_tokens\": 1200,  # Ensure sufficient token generation\n",
    "        \"top_p\": 0.95,  # Use nucleus sampling for diversity\n",
    "        \"do_sample\": True  # Enable sampling for generative tasks\n",
    "    },\n",
    "    content_handler=embeddings_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c6a366-8ad3-43dd-be4f-58c9551fa02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embedding=LangchainEmbeddingsWrapper(embed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b524342-7e44-4d64-ab06-e688f79e38c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the JSONL file line by line\n",
    "data = []\n",
    "with open('full_eval.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "\n",
    "# Define metrics\n",
    "metrics = [\n",
    "    ragas.metrics.answer_relevancy,\n",
    "    ragas.metrics.faithfulness,\n",
    "    #ragas.metrics.context_precision,\n",
    "    #ragas.metrics.context_recall,\n",
    "    #ragas.metrics.answer_similarity,\n",
    "    ragas.metrics.answer_correctness,\n",
    "]\n",
    "\n",
    "sample_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6643d4de-245f-444b-b3a7-4d50489f5945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the evaluation_batch structure\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"reference\": []\n",
    "}\n",
    "\n",
    "# BASE\n",
    "for item in data:\n",
    "    evaluation_batch[\"question\"].append(item.get(\"question\", \"\"))\n",
    "    evaluation_batch[\"contexts\"].append([item.get(\"context\", \"\")])  # Wrap in list\n",
    "    evaluation_batch[\"answer\"].append(item.get(\"base\", \"\")) \n",
    "    evaluation_batch[\"reference\"].append(item.get(\"original_answer\", \"\"))\n",
    "\n",
    "\n",
    "\n",
    "# Create the full dataset first\n",
    "ds_full = Dataset.from_dict(evaluation_batch)\n",
    "\n",
    "# Shuffle and select 100 random samples\n",
    "ds = ds_full.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "# Run evaluation\n",
    "base_evals_results = ragas_evaluate(\n",
    "    ds,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embeddings=LangchainEmbeddingsWrapper(embed),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(base_evals_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35404901-a72d-494a-953e-3eda74a3c8b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the evaluation_batch structure\n",
    "evaluation_batch = {\n",
    "    \"question\": [],\n",
    "    \"contexts\": [],\n",
    "    \"answer\": [],\n",
    "    \"reference\": []\n",
    "}\n",
    "\n",
    "# TUNED\n",
    "for item in data:\n",
    "    evaluation_batch[\"question\"].append(item.get(\"question\", \"\"))\n",
    "    evaluation_batch[\"contexts\"].append([item.get(\"context\", \"\")])  # Wrap in list\n",
    "    evaluation_batch[\"answer\"].append(item.get(\"tuned\", \"\")) \n",
    "    evaluation_batch[\"reference\"].append(item.get(\"original_answer\", \"\"))\n",
    "\n",
    "# Create the full dataset first\n",
    "ds_full = Dataset.from_dict(evaluation_batch)\n",
    "\n",
    "# Shuffle and select 100 random samples\n",
    "ds = ds_full.shuffle(seed=42).select(range(sample_size))\n",
    "\n",
    "#llm = ThrottledLLM(llm, delay=1.0, max_retries=6)\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "tuned_evals_results = ragas_evaluate(\n",
    "    ds,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embeddings=LangchainEmbeddingsWrapper(embed),\n",
    "    metrics=metrics,\n",
    ")\n",
    "\n",
    "print(tuned_evals_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60bf1-2814-4174-a643-4dd3c13643b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_evals_results_dict = json.loads(str(base_evals_results).replace(\"'\",\"\\\"\"))\n",
    "base_evals_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a1f09-58db-4b73-918c-b60b9a336b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_evals_results_dict = json.loads(str(tuned_evals_results).replace(\"'\",\"\\\"\"))\n",
    "tuned_evals_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb7302-c12b-4edb-bc46-99231808d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Metric names and corresponding values for both models\n",
    "metrics = [\"Answer Relevancy\", \"Faithfulness\", \"Answer Correctness\"]\n",
    "base_scores = [base_evals_results_dict[\"answer_relevancy\"], base_evals_results_dict[\"faithfulness\"], base_evals_results_dict[\"answer_correctness\"]]\n",
    "tuned_scores = [tuned_evals_results_dict[\"answer_relevancy\"], tuned_evals_results_dict[\"faithfulness\"], tuned_evals_results_dict[\"answer_correctness\"]]\n",
    "\n",
    "x = np.arange(len(metrics))  # label locations\n",
    "width = 0.35  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars1 = ax.bar(x - width/2, base_scores, width, label='Base Model')\n",
    "bars2 = ax.bar(x + width/2, tuned_scores, width, label='Tuned Model')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('RAGAS Evaluation: Base vs Tuned Model')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on top\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
