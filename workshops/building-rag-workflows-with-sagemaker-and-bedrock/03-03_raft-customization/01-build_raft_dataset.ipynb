{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf70c43-b793-45a3-b3fe-9852b035a55a",
   "metadata": {},
   "source": [
    "# Building a Retrieval Augmented Fine-Tuning (RAFT) dataset\n",
    "\n",
    "This notebook uses the PubMedQA dataset:\n",
    "- Jin, Q., Dhingra, B., Liu, Z., Cohen, W., & Lu, X. (2019). PubMedQA: A Dataset for Biomedical Research Question Answering. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2567â€“2577.\n",
    "\n",
    "\n",
    "The RAFT approach is a product of this research paper:\n",
    "- Zhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., & Gonzalez, J. E. (2024). RAFT: Adapting Language Model to Domain Specific RAG. https://arxiv.org/abs/2403.10131\n",
    "\n",
    "> Note that this notebook set doesn't yet incorporate CoT Reasoning, but it will further enhance the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b44a8-5fb5-44af-adc4-13ed94fd9f6a",
   "metadata": {},
   "source": [
    "## Introduction to RAFT\n",
    "\n",
    "Retrieval Augmented Fine Tuning (RAFT) aims to improve the performance of large language models on domain-specific open-book question answering tasks. \n",
    "\n",
    "![](images/x1.png)\n",
    "\n",
    "RAFT trains the language model to ignore \"distractor\" documents that do not contain relevant information to answer the given question, and instead focus on extracting the answer from the \"golden\" relevant documents. It also encourages the model to generate chain-of-thought style responses that cite relevant quotes from the documents, which improves the model's reasoning abilities. Experiments show that RAFT consistently outperforms standard fine-tuning and retrieval-augmented generation approaches across several specialized domains like medical literature, coding APIs, and multi-hop reasoning.\n",
    "\n",
    "As outlined in the following diagrams, training a model on distractor data has a material impact on the final accuracy of the response.\n",
    "\n",
    "Natural Questions|HotspotQA\n",
    "--- | --\n",
    "![](images/x5.png) | ![](images/x6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4578e9-373f-4cc3-ac47-81a0f3805d54",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "In this series of notebooks you will learn how to build a RAFT dataset based on the PubMedQA dataset, then train a new generation model in a SageMaker Managed Training Job, then host the fine-tuned model on SageMaker hosting or Bedrock via Custom Model Import. After hosting the model, you will run some quick evaluations to quantify the improvement on hold out data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b818842-e799-40dd-b44a-eb93a907330d",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4933ed-ebca-4825-a9f6-e0a9d50b2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -q -y autogluon-multimodal autogluon-timeseries autogluon-features autogluon-common autogluon-core\n",
    "%pip install -Uq pathos==0.3.2\n",
    "%pip install -Uq datasets==2.19.2\n",
    "%pip install -Uq transformers==4.40.2\n",
    "%pip install -Uq transformers[torch]==4.40.2\n",
    "%pip install -Uq sentence_transformers==3.1.1\n",
    "%pip install -Uq accelerate==1.0.0\n",
    "%pip install -Uq sagemaker==2.224.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec9584-0bbe-4b81-acc5-9d98e5d2e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6d77a-81ae-4d72-b3b4-7092786d941e",
   "metadata": {},
   "source": [
    "Process the raw PubMedQA dataset for RAFT. This involves building a set of Question/Answer/Context elements, with oracle context (all the correct context to answer the question) and distractor context (irrelevant data). The dataset will have a set of \"distracted\" documents where the oracle context isn't present at all along with standard documents where the oracle contexts are present but shuffled to prevent the model from learning to bias early contexts.\n",
    "\n",
    "The combination of these elements allows the model to better discern the correct way to answer a given user query when presented with a mixed corpus of content to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef01bd-9685-45ef-8d07-8bb1bbb2b4f7",
   "metadata": {},
   "source": [
    "First, you'll pull down the PubMedQA from HuggingFace Datasets, then build a base dataset that you will use for a variety of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae0b72-26aa-49ce-9e98-ec0e08c6b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_artificial\")\n",
    "source_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9925f-d63b-46da-87bf-05c8658f0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def process_dataset(input_dataset, output_filename, p=0.7, distract=3, max_items=-1):\n",
    "\n",
    "    output_data = []\n",
    "\n",
    "    if max_items > -1:\n",
    "        print(f\"max_items set, reducing input to {max_items} items.\")\n",
    "    else:\n",
    "        max_items = len(input_dataset)\n",
    "    \n",
    "    for idx, item in enumerate(input_dataset.select(range(max_items))):\n",
    "        \n",
    "        distractor_docs = []\n",
    "        \n",
    "        for i in range(distract):\n",
    "            distractor_element = input_dataset[random.randint(0,len(input_dataset)-1)]\n",
    "            distractor_contexts = distractor_element[\"context\"][\"contexts\"]\n",
    "            distractor_docs.append(random.sample(sorted(distractor_contexts),1)[0])\n",
    "\n",
    "        contexts = []\n",
    "        \n",
    "        #randomly select distractors\n",
    "        full_distractor = random.uniform(0, 1) > p\n",
    "        \n",
    "        if full_distractor:\n",
    "            contexts = distractor_docs\n",
    "        else:\n",
    "            contexts = item[\"context\"][\"contexts\"] + distractor_docs\n",
    "            \n",
    "        random.shuffle(contexts)\n",
    "        \n",
    "        data_item = {\n",
    "            \"question\": item[\"question\"],\n",
    "            \"context\": \"\\n\\n\".join(contexts),\n",
    "            \"oracle\": \"\\n\\n\".join(item[\"context\"][\"contexts\"]),\n",
    "            \"distracted\": full_distractor,\n",
    "            \"original_answer\": item[\"long_answer\"]\n",
    "        }\n",
    "        output_data.append(data_item)\n",
    "        \n",
    "        print(f\"item: {idx+1}\", end=\"\\r\")\n",
    "        \n",
    "    #write training data to an output file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322633a-a009-4c2b-9a3f-12f6a6360ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(source_dataset[\"train\"].shuffle(),\"./data/base_data/base_data.json\", max_items=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddd164-5d85-4b6c-a20b-3c825b3d5ea9",
   "metadata": {},
   "source": [
    "Then load the processed file into a dataset object and inspect one of the elements. \n",
    "\n",
    "You can see 5 properties in the dataset:\n",
    "- `question` - The user query related to this entry.\n",
    "- `oracle` - The oracle context for the given question, this is all of the correct context to generate the answer. This will be used to generate synthetic data in a following step as well as can be used to further measure the factual accuracy of generated responses.\n",
    "- `context` - The combined context elements. Either consistes entirely of distractor documents, or a mix of oracle/distractor documents.\n",
    "- `distracted` - A boolean flag identifying whether the context completely consists of distractor documents.\n",
    "- `original_answer` - The source PubMedQA answer. Here, you will generate longer versions based on oracle context since these are typically short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0030e9-5a4a-421f-af27-74581df84b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"./data/base_data/base_data.json\", split=\"train\")\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f6375-2b72-4777-bd74-5cfd6ca758b5",
   "metadata": {},
   "source": [
    "The `create_oracle_rag_prompts` function takes in an element from the base dataset and generates a prompt consisting of only oracle context so you can generate training data with longer answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcee73d-c1e8-4f29-9a30-e756a7d64bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to summarization messages    \n",
    "def create_oracle_rag_prompts(data_point):\n",
    "    full_prompt = f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are an assistant for question-answering tasks. Answer the following question in 5 sentences using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Context: {data_point[\"oracle\"]}\n",
    "        \n",
    "        Question: {data_point[\"question\"]}\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        Answer:\"\"\"\n",
    "    return {\"prompt\": full_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90881e6e-0a34-4a1e-adff-c60c2d1667bd",
   "metadata": {},
   "source": [
    "Use `dataset.map` to run the `create_oracle_rag_prompts` function on all the rows of the dataset, creating a `prompt` feature for each element that you'll use for generation. After the mapping, dump to a `generation_data.json` file in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015459f4-de88-4176-bef5-2962a203c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    create_oracle_rag_prompts,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset.to_json(\"./data/generation_data/generation_data.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcdcf7-3ab3-437f-a2a6-c86897113c21",
   "metadata": {},
   "source": [
    "Load the data directly from the filesystem in case you've already generated the prompt data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616bdce-1c7e-40d0-8a8e-bb8b004b8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_dataset = load_dataset(\"json\", data_files=\"./data/generation_data/generation_data.json\", split=\"train\")\n",
    "generation_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de608428-00a7-4a76-958c-c3274ff0d3c5",
   "metadata": {},
   "source": [
    "## Generate Oracle Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a9df92-049e-4740-9452-398eb218416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "generation_base_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=\"<<YOUR ENDPOINT HERE>>\",\n",
    "    sagemaker_session=sess,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b613e9df-b096-44d5-b241-8c8848355293",
   "metadata": {},
   "source": [
    "Preview the prompt being used for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424c853-d6a7-4e64-a6bc-42a936132a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = generation_dataset[2]\n",
    "test_item[\"prompt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da8e012-e5e2-48b7-b7ca-c10a54665571",
   "metadata": {},
   "source": [
    "Here, you will set the parameters being used for generation and insert your prompt. Then use the Bedrock `invoke.model` API to test what a generated response looks like before running against the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec5283-5d3f-404f-82a9-f3fd7c4dcd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_prompt(predictor, prompt, parameters):\n",
    "    # convert u/a format \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response = predictor.predict(payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c40aa-c31c-45e1-944e-bdc5b8911537",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = test_item[\"prompt\"]\n",
    "\n",
    "base_response = send_prompt(\n",
    "    p4d_base_predictor,\n",
    "    prompt,\n",
    "    parameters={\n",
    "        \"temperature\": 0.9, \n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")\n",
    "\n",
    "base_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b33a04-59c6-4459-916f-fb6584e6ef9c",
   "metadata": {},
   "source": [
    "## Generate summaries based on oracle contexts for supervised fine tuning\n",
    "\n",
    "This step will go through the generation dataset and build longer summaries than the standard PubMedQA summaries, only using oracle contexts. This is then joined with the existing data fields to build the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb85bf5-9f4b-420f-a0e2-a9c9dddaf37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "with open(f\"../data/synthetic_data/synthetic_training_data.json\", \"w\") as output_file:\n",
    "    output_json = []\n",
    "    for idx, data in enumerate(generation_dataset):\n",
    "    \n",
    "        model_response = send_prompt(\n",
    "            generation_base_predictor,\n",
    "            data[\"prompt\"],\n",
    "            parameters={\n",
    "                \"temperature\": 0.9, \n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Extract and print the response text.\n",
    "        response_text = model_response[\"generated_text\"]\n",
    "        #print(response_text)\n",
    "    \n",
    "        output_item = data\n",
    "\n",
    "        #print(output_item)\n",
    "        \n",
    "        del(output_item[\"prompt\"])\n",
    "        output_item[\"synthetic_answer\"] = response_text\n",
    "\n",
    "        output_json.append(output_item)\n",
    "\n",
    "        clear_output()\n",
    "        print(f\"{idx+1} of {len(generation_dataset)}\\n\\n{output_item}\")\n",
    "        \n",
    "    output_file.write(json.dumps(output_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819c182-8c70-497a-bc89-25d3c24c5056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
