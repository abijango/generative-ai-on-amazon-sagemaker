{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDDDD; border-left: 5px solid red; padding: 10px; color: black;\">\n",
    "    <strong>Kernel: Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune your RAFT model with PyTorch FSDP and Q-Lora on Amazon SageMaker\n",
    "\n",
    "This blog post explains how you can fine-tune a Llama 3.1 8b model using PyTorch FSDP and Q-Lora with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index) on Amazon SageMaker. \n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "Hugging Face share the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allows you now to fine-tune Llama-like architectures or Mixtral 8x7B. Hugging Face PEFT is were the core logic resides, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n",
    "\n",
    "This blog post walks you thorugh how to fine-tune open LLMs from Hugging Face using Amazon SageMaker.\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. Ignore this line if you've already run task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq py7zr==0.22.0\n",
    "%pip install -Uq datasets==2.21.0\n",
    "%pip install -Uq transformers==4.45.0\n",
    "%pip install -Uq peft==0.12.0\n",
    "%pip install -Uq s3fs==2024.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in your own environment, you need access to an IAM Role with the required permissions for Sagemaker. You can learn more about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from datasets import load_dataset\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import os\n",
    "from sagemaker.model import Model\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This parameter will toggle between local mode (downloading from S3) and loading models from the HF Model Hub.\n",
    "#In a workshop environment you will have a local model pre-downloaded. \n",
    "#Otherwise you will either download the model to S3 and leave this True, or set this to false and fill in the HuggingFace Model ID and Token if necessary.\n",
    "USE_LOCAL_MODEL_FROM_S3 = True\n",
    "\n",
    "\n",
    "if USE_LOCAL_MODEL_FROM_S3 == True:\n",
    "    os.environ['use_local']=\"true\"\n",
    "    #the default path set here is for workshop environments. \n",
    "    #If using this outside of a hosted workshop, you will need to set this to wherever you downloaded your model.\n",
    "    #Ignore the model_id and hf_token fields, they are simply being cleared here to avoid conflicts with subsequent runs.\n",
    "    os.environ['model_id']=\"\"\n",
    "    os.environ['hf_token']=\"\"\n",
    "    os.environ['base_model_s3_path']=f\"\"\n",
    "\n",
    "else:\n",
    "    os.environ['use_local']=\"false\"\n",
    "    # Model_ID - set this to the HuggingFace Model ID you want to load.\n",
    "    os.environ['model_id']=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"    \n",
    "    # HF_Token - use your HuggingFace Token here to access gated models. Llama-3-8B-Instruct is a gated model.\n",
    "    os.environ['hf_token']=\"<<YOUR_HF_TOKEN>>\"\n",
    "    #ignore this env variable for remote mode\n",
    "    os.environ['base_model_s3_path']=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this variable and set the value to your MLFlow Tracking Server ARN to activate MLFLow experiment tracking\n",
    "os.environ['mlflow_tracking_server_arn']=\"<<YOUR_MLFLOW_TRACKING_SERVER_ARN>>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "For training your model, you will use the `CONTEXT` property rather that the `ORACLE` property. This will introduce distractor documents into the training process to allow the model to better discern the signal from noise.\n",
    "\n",
    "Here you'll generate the training prompts with the `CONTEXT` as well as the synthetically generated `ANSWER` from the dataset generation. We will hold out a subset for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert dataset to summarization messages  \n",
    "def create_rag_sft_prompts(data_point):\n",
    "    full_prompt = f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are an assistant for question-answering tasks. Answer the following question in 5 sentences using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Context: {data_point[\"context\"]}\n",
    "        \n",
    "        Question: {data_point[\"question\"]}\n",
    "        <|start_header_id|>assistant<|end_header_id|> \n",
    "        Answer:{data_point[\"synthetic_answer\"]}\"\"\"\n",
    "    return {\"prompt\": full_prompt}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will load your synthetic dataset, then generate your train/test/validation splits. \n",
    "\n",
    "In this example, you'll take 80% of the data for training, then 20% for training/validation, then split the 20% in half to give 10% validation and 10% test splits and reassemble into a single dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "full_dataset = load_dataset(\"json\", data_files=\"./data/synthetic_data/synthetic_training_data.json\", split=\"train\")\n",
    "\n",
    "columns_to_remove = list(full_dataset.features)\n",
    "\n",
    "train_test_eval_dataset = full_dataset.train_test_split(test_size=0.2, shuffle=True)\n",
    "test_eval_dataset = train_test_eval_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "full_dataset = DatasetDict({\n",
    "    'train': train_test_eval_dataset['train'].map(\n",
    "        create_rag_sft_prompts,\n",
    "        remove_columns=columns_to_remove,\n",
    "        batched=False\n",
    "    ),\n",
    "    'eval': test_eval_dataset['train'].map(\n",
    "        create_rag_sft_prompts,\n",
    "        remove_columns=columns_to_remove,\n",
    "        batched=False\n",
    "    ),\n",
    "    'test': test_eval_dataset['test'].map(\n",
    "        create_rag_sft_prompts,\n",
    "        batched=False\n",
    "    )\n",
    "})\n",
    "\n",
    "# full_dataset = full_dataset.map(\n",
    "#     create_rag_sft_prompts,\n",
    "#     remove_columns=columns_to_remove,\n",
    "#     batched=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "full_dataset, full_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "local_data_path = \"datasets/raft\"\n",
    "s3_data_path = f's3://{sess.default_bucket()}/datasets/raft'\n",
    "\n",
    "# Save datasets to s3 and locally\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "full_dataset[\"train\"].to_json(f\"{local_data_path}/train/train.json\", orient=\"records\")\n",
    "full_dataset[\"train\"].to_json(f\"{s3_data_path}/train/train.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{s3_data_path}/train/train.json\"\n",
    "\n",
    "full_dataset[\"eval\"].to_json(f\"{local_data_path}/eval/eval.json\", orient=\"records\")\n",
    "full_dataset[\"eval\"].to_json(f\"{s3_data_path}/eval/eval.json\", orient=\"records\")\n",
    "eval_dataset_s3_path = f\"{s3_data_path}/eval/eval.json\"\n",
    "\n",
    "full_dataset[\"test\"].to_json(f\"{local_data_path}/test/test.json\", orient=\"records\")\n",
    "full_dataset[\"test\"].to_json(f\"{s3_data_path}/test/test.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{s3_data_path}/test/test.json\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(eval_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"\\nYou can view the uploaded dataset in the console here: \\nhttps://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={s3_data_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure input length\n",
    "\n",
    "While passing in a dataset to the LLM for fine-tuning, it's important to ensure that the inputs are all of a uniform length. To achieve this, we first visualize the distribution of the input token lengths (or alternatively, firectly find the max length). Based on these results, we identify the maximum input token length, and utilize \"padding\" to ensure all the inputs are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_lengths(tokenized_train_dataset, tokenized_validation_dataset):\n",
    "    lengths1 = [len(x[\"prompt\"].split()) for x in tokenized_train_dataset]\n",
    "    lengths2 = [len(x[\"prompt\"].split()) for x in tokenized_validation_dataset]\n",
    "    lengths = lengths1 + lengths2\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"prompt lengths\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of lengths of input_ids\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(full_dataset[\"train\"], full_dataset[\"eval\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 3.1 on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers`. We prepared a script [launch_fsdp_qlora.py](scripts/launch_fsdp_qlora.py) which will loads the dataset from disk, prepares the model/tokenizer, and starts the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. \n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a yaml file. This `yaml` will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning your model. You will save the config file as `args.yaml` and upload it to S3 to be used in your training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "hf_token: \"${hf_token}\"                         # Use HF token to login into Hugging Face to access the Llama 3.1 8b model\n",
    "model_id: \"${model_id}\"                         # Hugging Face model id\n",
    "use_local: \"${use_local}\"\n",
    "\n",
    "max_seq_length: 2048  #512 # 2048               # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "eval_dataset_path: \"/opt/ml/input/data/eval/\"   # path to where SageMaker saves eval dataset\n",
    "base_model_s3_path: \"/opt/ml/input/data/basemodel/\"\n",
    "#tokenizer_s3_path: \"/opt/ml/input/data/tokenizer/\"\n",
    "ml_tracking_server_arn: \"${mlflow_tracking_server_arn}\"\n",
    "\n",
    "output_dir: \"/opt/ml/model/llama3.1/adapters/sum\"         # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"mlflow\"                    # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 5                    # number of training epochs\n",
    "per_device_train_batch_size: 8         # batch size per device during training\n",
    "per_device_eval_batch_size: 4          # batch size for evaluation\n",
    "gradient_accumulation_steps: 1         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "eval_strategy: epoch                   # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: false                            # use bfloat16 precision\n",
    "tf32: false                            # use tf32 precision\n",
    "fp16: true\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\"   # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"false\"\n",
    "    activation_checkpointing: true\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload the config file to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{s3_data_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LoRA adapter\n",
    "\n",
    "Below estimtor will train the model with QLoRA and will save the LoRA adapter in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-1-8b-finetune'\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point= 'launch_fsdp_qlora.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=50000,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=50,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\" # path to TRL config which was uploaded to s3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: When using QLoRA, we only train adapters and not the full model. The [launch_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) saves the `adapter` at the end of the training to Amazon SageMaker S3 bucket (sagemaker-<region name>-<account_id>).\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'eval': eval_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    "\n",
    "if(os.environ[\"use_local\"].lower()==\"true\"):\n",
    "    data.update({'basemodel':os.environ['base_model_s3_path']})\n",
    " \n",
    "# Check input channels configured \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine the job name of the last run or you can browse the console\n",
    "latest_run_job_name=pytorch_estimator.latest_training_job.job_name\n",
    "latest_run_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find S3 path for the last job that ran successfully. You can find this from the SageMaker console \n",
    "# *** Get a job name from the AWS console for the last training run or from the above cell\n",
    "\n",
    "job_name = latest_run_job_name\n",
    "\n",
    "def get_s3_path_from_job_name(job_name):\n",
    "    # Create a Boto3 SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # Describe the training job\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "    \n",
    "    # Extract the model artifacts S3 path\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Extract the output path (this is the general output location)\n",
    "    output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "    \n",
    "    return model_artifacts_s3_path, output_path\n",
    "\n",
    "\n",
    "model_artifacts, output_path = get_s3_path_from_job_name(job_name)\n",
    "\n",
    "print(f\"Model artifacts S3 path: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Point to the directory where we have the adapter saved \n",
    "\n",
    "adapter_dir_path=f\"{model_artifacts}/llama3.1/adapters/sum/\"\n",
    "\n",
    "adapter_serving_dir_path=f\"{model_artifacts}/llama3.1/\"\n",
    "\n",
    "print(f'\\nAdapter S3 Dir path:{adapter_dir_path} \\n')\n",
    "\n",
    "print(f'\\nServing S3 Dir path:{adapter_serving_dir_path} \\n')\n",
    "\n",
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you already have this environment variable set\n",
    "base_model_s3_path = os.environ['base_model_s3_path'] if os.environ['use_local'].lower() == 'true' else os.environ['model_id']\n",
    "\n",
    "# Store the variables required for the next notebook \n",
    "%store base_model_s3_path\n",
    "%store adapter_serving_dir_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step - Use register_model_adapter.ipynb notebook to register the adapter to the SageMaker model registry "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge base model with fine-tuned adapter in fp16 and Test Inference \n",
    "\n",
    "Following Steps are taken by the next estimator:\n",
    "1. Load base model in fp16 precision\n",
    "2. Convert adapter saved in previous step from fp32 to fp16\n",
    "3. Merge the model\n",
    "4. Run inference both on base model and merged model for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# Define Training Job Name \n",
    "job_name = f'llama3-1-8b-merge-adapter'\n",
    "\n",
    "hyperparameters = {\n",
    "    \"dataset_name\": \"none\",\n",
    "    \"use_local\": os.environ['use_local']\n",
    "}\n",
    "\n",
    "# Add hf_token only if it's available in the environment\n",
    "if 'hf_token' in os.environ and os.environ['hf_token']:\n",
    "    hyperparameters[\"hf_token\"] = os.environ['hf_token']\n",
    "\n",
    "\n",
    "# Hugging Face model id\n",
    "if 'model_id' in os.environ and os.environ['model_id']:\n",
    "    hyperparameters[\"model_id\"] = os.environ['model_id']\n",
    "\n",
    "pytorch_estimator_adapter = PyTorch(\n",
    "    entry_point= 'merge_model_adapter.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters=hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'adapter': adapter_dir_path,\n",
    "  'testdata': test_dataset_s3_path \n",
    "  }\n",
    "\n",
    "if(os.environ[\"use_local\"].lower()==\"true\"):\n",
    "    data.update({'basemodel':os.environ['base_model_s3_path']})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator_adapter.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_run_job_name=pytorch_estimator_adapter.latest_training_job.job_name\n",
    "\n",
    "model_artifacts, output_path = get_s3_path_from_job_name(latest_run_job_name)\n",
    "\n",
    "print(f\"Model artifacts S3 path: {model_artifacts}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
