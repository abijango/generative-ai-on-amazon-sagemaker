{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8fd722b-7c0b-4813-ae7f-963215645fb9",
   "metadata": {},
   "source": [
    "# ðŸš€ Deploy Your Fine-tuned RAFT Model with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a414500-d1e8-40d8-ac2c-861f385014fc",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8cc62-53a9-4542-a566-56f47ada5e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -Uq sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f9750-ff90-4985-a978-52b22dbab4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import (\n",
    "    HuggingFaceModel, \n",
    "    get_huggingface_llm_image_uri\n",
    ")\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe509d4a-91e5-4826-8cd5-8be23ffe875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_region = boto3.Session().region_name\n",
    "session = sagemaker.session.Session(boto_session=boto3.Session(region_name=boto_region))\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03803d-3b72-4a27-861c-2af12106fa18",
   "metadata": {},
   "source": [
    "## Deploy using DJL-Inference Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92ece7-9d1f-4ffd-a056-583e4a0222cc",
   "metadata": {},
   "source": [
    "The [Deep Java Library (DJL) Large Model Inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-container-docs.html) containers are specialized Docker containers designed to facilitate the deployment of large language models (LLMs) on Amazon SageMaker. These containers integrate a model server with optimized inference libraries, providing a comprehensive solution for serving LLMs. \n",
    "\n",
    "**Key Features of DJL LMI Containers:**\n",
    "\n",
    "* __Optimized Inference Performance__: Support for popular model architectures like DeepSeek, Mistral, Llama, Falcon and many more..\n",
    "* __Integration with Inference Libraries__: Seamless integration with libraries such as vLLM, TensorRT-LLM, and Transformers NeuronX.\n",
    "* __Advanced Capabilities__: Features like continuous batching, token streaming, quantization (e.g., AWQ, GPTQ, FP8), multi-GPU inference using tensor parallelism, and support for LoRA fine-tuned models.\n",
    "\n",
    "**Benefits for Deploying LLMs with DJL-LMI on Amazon SageMaker:**\n",
    "\n",
    "* __Simplified Deployment__: DJL LMI containers offer a low-code interface, allowing users to specify configurations like model parallelization and optimization settings through a configuration file. \n",
    "* __Performance Optimization__: By leveraging optimized inference libraries and techniques, these containers enhance inference performance, reducing latency and improving throughput.\n",
    "* __Scalability__: Designed to handle large models that may not fit on a single accelerator, enabling efficient scaling across multiple GPUs or specialized hardware like AWS Inferentia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b78106-b50f-4a26-afda-d0a865d3a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmi_container_uri = f\"763104351884.dkr.ecr.{boto_region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1121de3-6465-4bcb-a026-9e7b896cc261",
   "metadata": {},
   "source": [
    "Choose an appropriate model name and endpoint name when hosting your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dabec-ad96-4476-8da8-4028983b8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_timestamp = datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "base_model_name = f\"llama-3-1-8B-base-lmi-{model_timestamp}\"\n",
    "tuned_model_name = f\"llama-3-1-8B-tuned-lmi-{model_timestamp}\"\n",
    "\n",
    "base_endpoint_name = f\"{base_model_name}-ep\"\n",
    "tuned_endpoint_name = f\"{tuned_model_name}-ep\"\n",
    "\n",
    "print(f\"base: \\n{base_endpoint_name}\")\n",
    "print(f\"tuned: \\n{tuned_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976098f1-4cdc-4405-8022-07432ced6d1b",
   "metadata": {},
   "source": [
    "Create a new [SageMaker Model](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65d9f4",
   "metadata": {},
   "source": [
    "> âš  Swap `HF_MODEL_ID` with another model tag if you want to compare against a different base model.\n",
    ">\n",
    "> Gated models will require you to supply a HuggingFace API Token via the `HF_TOKEN: \"hf_...\"` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508a0a3-dab1-4880-8eb0-76b82ebbb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set these to either S3 paths or HuggingFace model tags\n",
    "\n",
    "BASE_MODEL_ARTIFACTS =  \"<<PATH_TO_YOUR_BASE_MODEL>>\"\n",
    "TUNED_MODEL_ARTIFACTS = \"<<PATH_TO_YOUR_TUNED_MERGED_MODEL>>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622d23d-b59c-4aa8-8cec-3d5e3be19ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = sagemaker.Model(\n",
    "    image_uri=lmi_container_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": BASE_MODEL_ARTIFACTS,\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"5000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "    },\n",
    "    role=role,\n",
    "    name=base_model_name,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e15a5-5897-4187-b22d-20ff0d94ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = sagemaker.Model(\n",
    "    image_uri=lmi_container_uri,\n",
    "    env={\n",
    "        \"HF_MODEL_ID\": TUNED_MODEL_ARTIFACTS,\n",
    "        \"OPTION_MAX_MODEL_LEN\": \"5000\",\n",
    "        \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.95\",\n",
    "        \"OPTION_ENABLE_STREAMING\": \"false\",\n",
    "        \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "        \"OPTION_MODEL_LOADING_TIMEOUT\": \"3600\",\n",
    "        \"OPTION_PAGED_ATTENTION\": \"false\",\n",
    "        \"OPTION_DTYPE\": \"fp16\",\n",
    "    },\n",
    "    role=role,\n",
    "    name=tuned_model_name,\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ac41f-8586-491b-8bcc-621b527d1f0c",
   "metadata": {},
   "source": [
    "ðŸš€ Deploy. Please wait for the endpoint to be `InService` before running inference against it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550b451-6ad6-46c5-b104-6739683b365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_predictor = base_model.deploy(\n",
    "    endpoint_name=base_endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    "    wait=True\n",
    ")\n",
    "print(f\"\\nYour BASE Endpoint: {base_endpoint_name} is now deployed! ðŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bb6a4-76c5-4ab7-93a6-5d080a391a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model_predictor = tuned_model.deploy(\n",
    "    endpoint_name=tuned_endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    container_startup_health_check_timeout=600,\n",
    "    wait=True\n",
    ")\n",
    "print(f\"\\nYour TUNED Endpoint: {tuned_endpoint_name} is now deployed! ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481d9178-dade-44c4-b633-99f500b20de6",
   "metadata": {},
   "source": [
    "### Inference with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc27b2d-78a0-42a0-8605-8c94d4e5fd89",
   "metadata": {},
   "source": [
    "SageMaker python sdk simplifies the inference construct using `sagemaker.Predictor` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d357e-d4e3-4297-a4cd-6dc45d12630e",
   "metadata": {},
   "source": [
    "Llama 3 utilizes the following prompt format:\n",
    "\n",
    "\n",
    "```json\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are an assistant for question-answering tasks. Answer the following question in 5 sentences using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Context: {CONTEXT}\n",
    "\n",
    "Question: {QUESTION} \n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0096cd-5ccb-4885-85b4-d986ce7711ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(messages: list[dict[str, str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "    \n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding an inferred prefix\n",
    "    system_prefix = f\"\\n\\nCutting Knowledge Date: December 2024\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if entry['role'] == 'system':\n",
    "            output += f\"{system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif entry['role'] != 'system' and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def send_prompt(predictor, messages, parameters):\n",
    "    # convert u/a format \n",
    "    frmt_input = format_messages(messages)\n",
    "    payload = {\n",
    "        \"inputs\": frmt_input,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "    response = predictor.predict(payload)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a840f9-34ce-400c-a597-08ce2242e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(data):\n",
    "    system_content = f\"\"\"You are an assistant for question-answering tasks. Answer the following question in 5 sentences using the provided context. If you don't know the answer, just say \"I don't know.\".\"\"\"\n",
    "    user_content = f\"\"\"\n",
    "        Context: {data[\"context\"]} \n",
    "        \n",
    "        Question: {data[\"question\"]}\n",
    "        \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcf7f7-8f5e-4468-ae53-343ccba7c586",
   "metadata": {},
   "source": [
    "We can continue to use a simple `List[Dict[str, str]]` format to chat and simplify `system`, `user` and `assistant` chat transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d8e4c-8ab0-4036-9cd0-b1a88803ec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=\"datasets/raft/test/test.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40538b5-5e23-4179-be92-0a277de181f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = test_dataset[0]\n",
    "test_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5974fe3f-c1f0-4f76-9060-4e9878d6aa31",
   "metadata": {},
   "source": [
    "reloading the predictors from endpoint names in case we are working with existing endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7bd10-062b-4397-8ae9-2eb366aabda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=\"<<YOUR_BASE_ENDPOINT_NAME>\", #base_endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")\n",
    "\n",
    "tuned_predictor = sagemaker.Predictor(\n",
    "    endpoint_name=\"<<YOUR_TUNED_ENDPOINT_NAME>>\", #tuned_endpoint_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b7ec8-72b2-4ab7-a93a-0d0e4c2dfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "messages = build_messages(test_item)\n",
    "\n",
    "base_response = send_prompt(\n",
    "    base_predictor,\n",
    "    messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.9, \n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")\n",
    "\n",
    "tuned_response = send_prompt(\n",
    "    tuned_predictor,\n",
    "    messages,\n",
    "    parameters={\n",
    "        \"temperature\": 0.9, \n",
    "        \"max_new_tokens\": 512,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "    ============== Question ============\n",
    "    {test_item[\"question\"]}\n",
    "\n",
    "    ========= Baseline Answer ==========\n",
    "    {base_response['generated_text']}\n",
    "    \n",
    "    ========= Generated Answer =========\n",
    "    {tuned_response['generated_text']}\n",
    "\n",
    "    ======== Ground Truth Answer =======\n",
    "    {test_item[\"synthetic_answer\"]}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef576e-e03f-47f3-914b-f302642891fc",
   "metadata": {},
   "source": [
    "## Evaluate your results\n",
    "\n",
    "In this section, you will build a dataset of evaluation data. The `MAX_EVALUATIONS` value will limit the scope of the evaluation and the time it takes to complete.\n",
    "\n",
    "Since there are pure distractor documents from splitting our training dataset, we will remove them during the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e750-d509-4486-84c2-dc3fcbc193db",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "oracle_context = []\n",
    "test_context = []\n",
    "ground_truth = []\n",
    "base_predictions = []\n",
    "sft_predictions = []\n",
    "\n",
    "#evaluation_data = {}\n",
    "\n",
    "MAX_EVALUATIONS = -1 #set to -1 to run the entire dataset. WARNING: THIS WILL TAKE A LONG TIME\n",
    "\n",
    "if MAX_EVALUATIONS > -1:\n",
    "    print(f\"MAX_EVALUATIONS set, reducing input to {MAX_EVALUATIONS} items.\")\n",
    "else:\n",
    "    MAX_EVALUATIONS = len(test_dataset)\n",
    "\n",
    "for idx, test_item in enumerate(test_dataset.select(range(MAX_EVALUATIONS))):\n",
    "    \n",
    "    if test_item[\"distracted\"] == True:\n",
    "        continue #skip distractor docs\n",
    "\n",
    "    \n",
    "    messages = build_messages(test_item)\n",
    "    \n",
    "    base_response = send_prompt(\n",
    "        base_predictor,\n",
    "        messages,\n",
    "        parameters={\n",
    "            \"temperature\": 0.9, \n",
    "            \"max_new_tokens\": 512,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    sft_response = send_prompt(\n",
    "        tuned_predictor,\n",
    "        messages,\n",
    "        parameters={\n",
    "            \"temperature\": 0.9, \n",
    "            \"max_new_tokens\": 512,\n",
    "            \"top_p\": 0.9\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Define the candidate predictions and reference sentences\n",
    "\n",
    "    # evaluation_data.append({\n",
    "    #     \"ground_truth\": test_item[\"ANSWER\"],\n",
    "    #     \"base\": base_response['generated_text'],\n",
    "    #     \"tuned\": sft_response['generated_text'],\n",
    "    #     \"test_context\": test_item[\"CONTEXT\"],\n",
    "    #     \"oracle_context\": test_item[\"ORACLE\"]\n",
    "    # })\n",
    "    \n",
    "    ground_truth.append(test_item[\"synthetic_answer\"])\n",
    "    base_predictions.append(base_response['generated_text'])\n",
    "    sft_predictions.append(sft_response['generated_text'])\n",
    "\n",
    "    print(f\"{idx} of {MAX_EVALUATIONS}\", end=\"\\r\")\n",
    "    \n",
    "\n",
    "evaluation_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"ground_truth\": ground_truth, \n",
    "        \"base\": base_predictions, \n",
    "        \"tuned\": sft_predictions}\n",
    ")\n",
    "evaluation_dataset.to_json(f\"./eval.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9ba43-35d0-42d6-af9d-41a0c216a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "base_predictions = []\n",
    "sft_predictions = []\n",
    "\n",
    "for eval_item in evaluation_dataset:\n",
    "\n",
    "    ground_truth.append(eval_item[\"ground_truth\"])\n",
    "    base_predictions.append(eval_item['base'])\n",
    "    sft_predictions.append(eval_item['tuned'])\n",
    "\n",
    "\n",
    "base_bleu_results = bleu.compute(predictions=base_predictions, references=ground_truth)\n",
    "base_rouge_results = rouge.compute(predictions=base_predictions, references=ground_truth)\n",
    "\n",
    "# Compute the BLEU score\n",
    "sft_bleu_results = bleu.compute(predictions=sft_predictions, references=ground_truth)\n",
    "sft_rouge_results = rouge.compute(predictions=sft_predictions, references=ground_truth)\n",
    "\n",
    "base_scores = (base_bleu_results | base_rouge_results)\n",
    "sft_scores = (sft_bleu_results | sft_rouge_results)\n",
    "\n",
    "\n",
    "# base_scores.append(base_bleu_results | base_rouge_results)\n",
    "# sft_scores.append(sft_bleu_results | sft_rouge_results)\n",
    "print(\"=======BASE MODEL=======\")\n",
    "print(base_scores)\n",
    "print(\"=======TUNED MODEL=======\")\n",
    "print(sft_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebc86e-9ca8-4f3a-bf93-61e807e61929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'dimension':[], 'base': [], 'tuned': [], 'delta': [], 'delta_percent': []}\n",
    "\n",
    "for key in base_scores.keys():\n",
    "    if key in [\"length_ratio\",\"precisions\",\"brevity_penalty\",\"translation_length\",\"reference_length\"]:\n",
    "        continue\n",
    "        \n",
    "    delta = sft_scores[key]-base_scores[key]\n",
    "    delta_percent = (delta/base_scores[key])*100\n",
    "    \n",
    "    data['dimension'].append(key)\n",
    "    data['base'].append(base_scores[key])\n",
    "    data['tuned'].append(sft_scores[key])\n",
    "    data['delta'].append(delta)\n",
    "    data['delta_percent'].append(delta_percent)\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d12f13-c803-4ebe-8679-79256aa70f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
