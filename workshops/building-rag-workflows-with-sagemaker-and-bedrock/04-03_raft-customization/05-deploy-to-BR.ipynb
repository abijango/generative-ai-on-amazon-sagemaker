{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Hosting SageMaker AI models in Bedrock with Bedrock Custom Model Import\n",
    "\n",
    "In this notebook, you'll take a model artifact that you trained with Amazon SageMaker AI and host it in Amazon Bedrock using Bedrock Custom Model Import.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19163cfd-f05e-47ca-b086-a17163a2269a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb17ac-0844-4f99-bfd6-7f5ea7952b38",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "\n",
    "This section contains python variables used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab776e2-a4a8-41bc-899f-4e46e6b2b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "print(bucket_name)\n",
    "print(account_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ed653",
   "metadata": {},
   "source": [
    "# Import SageMaker AI fine-tuned Model to Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to import models to Amazon Bedrock using Custom Model Import (CMI) feature.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM roles and permissions for Bedrock and Amazon S3, following [the instruction here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)\n",
    "- A S3 bucket prepared to store the custom model\n",
    "- Sufficient local storage space (At least 17GB for 8B and 135GB for 70B models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4dc696",
   "metadata": {},
   "source": [
    "### Step 1: Verify S3 Path\n",
    "\n",
    "Update these parameters according to your AWS environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cef38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = f'<<YOUR_FINE_TUNED_MERGED_MODEL>>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_s3_path = f\"s3://{bucket_name}/{s3_prefix}/\"\n",
    "full_s3_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {full_s3_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30269e4b-5993-48d7-a99e-c98861bc7dbf",
   "metadata": {},
   "source": [
    "The Bedrock Custom Model Import job requires a service role to run. The appropriate policies can be found in the [CMI documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62501f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%m-%d-%Y-%H%M%S\")\n",
    "role_name = \"bedrock-cmi-role\" #replace with your own bedrock service role name if different\n",
    "\n",
    "\n",
    "# Define your parameters (please update this part based on your setup)\n",
    "imported_model_name = 'Fine-Tuned-RAFT' # E.x. Deepseek-8B-model\n",
    "job_name = imported_model_name + '-' + timestamp # E.x. Deepseek-8B-job\n",
    "role_arn = f'arn:aws:iam::{account_id}:role/{role_name}' # Please make sure it has sufficient permission as listed in the pre-requisite\n",
    "\n",
    "# Region (currently only 'us-west-2' and 'us-east-1' support CMI with Deepseek-Distilled-Llama models)\n",
    "region_info = sagemaker_session.boto_region_name#'us-west-2' # You can modify to 'us-east-1' based on your need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552c110",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9ae8e",
   "metadata": {},
   "source": [
    "### Step 2: Create Custom Model Import Job\n",
    "\n",
    "Initialize the import job in Amazon Bedrock\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Creating CMI job for 8B model could take 5-20 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1715643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=region_info)\n",
    "\n",
    "s3_uri = f's3://{bucket_name}/{s3_prefix}/'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ff649",
   "metadata": {},
   "source": [
    "### Step 3: Monitor Import Job Status\n",
    "\n",
    "Check the status of your import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66d12c",
   "metadata": {},
   "source": [
    "### Step 4: Wait for Model Initialization\n",
    "\n",
    "Allow time for the model to initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f501fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5mins for cold start \n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a438cae",
   "metadata": {},
   "source": [
    "### Step 5: Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b9b83-8b2d-4395-8b53-9726ddc44014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "import json\n",
    "\n",
    "def format_messages(messages: list[dict[str, str]]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Format messages for Llama 3+ chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    # auto assistant suffix\n",
    "    # messages.append({\"role\": \"assistant\"})\n",
    "    \n",
    "    output = \"<|begin_of_text|>\"\n",
    "    # Adding an inferred prefix\n",
    "    system_prefix = f\"\\n\\nCutting Knowledge Date: December 2024\\nToday Date: {datetime.now().strftime('%d %b %Y')}\\n\\n\"\n",
    "    for i, entry in enumerate(messages):\n",
    "        output += f\"<|start_header_id|>{entry['role']}<|end_header_id|>\"\n",
    "        if entry['role'] == 'system':\n",
    "            output += f\"{system_prefix}{entry['content']}<|eot_id|>\"\n",
    "        elif entry['role'] != 'system' and 'content' in entry:\n",
    "            output += f\"\\n\\n{entry['content']}<|eot_id|>\"\n",
    "    output += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def send_prompt(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    # convert u/a format \n",
    "    frmt_input = format_messages(messages)\n",
    "\n",
    "    client = boto3.Session().client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region_info,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,     # 5 minutes\n",
    "        retries={'max_attempts': 3}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps({\n",
    "                    'prompt': frmt_input,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fe7dc-f652-4a99-b7aa-873abe0f0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_messages(data):\n",
    "    system_content = f\"\"\"You are an assistant for question-answering tasks. Answer the following question in 5 sentences using the provided context. If you don't know the answer, just say \"I don't know.\".\"\"\"\n",
    "    user_content = f\"\"\"\n",
    "        Context: {data[\"CONTEXT\"]} \n",
    "        \n",
    "        Question: {data[\"QUESTION\"]}\n",
    "        \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfa900-b822-44bd-87a5-482b784e6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=\"../data/sft_test_data.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df94d85-3233-458a-bdf3-84508ffcac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = test_dataset[2]\n",
    "test_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afba42-4ae9-4a56-9b35-5a397bc7a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = build_messages(test_item)\n",
    "\n",
    "model_response = send_prompt(messages)\n",
    "\n",
    "print(f\"\"\"\n",
    "    ============== Question ============\n",
    "    {test_item[\"QUESTION\"]}\n",
    "    \n",
    "    ========= Generated Answer =========\n",
    "    {model_response}\n",
    "\n",
    "    ======== Ground Truth Answer =======\n",
    "    {test_item[\"ANSWER\"]}\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
