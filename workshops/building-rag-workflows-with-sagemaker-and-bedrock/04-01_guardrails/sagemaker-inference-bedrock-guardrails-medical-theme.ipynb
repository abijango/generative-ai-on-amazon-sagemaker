{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker RAG retrieval and generation with SageMaker Inference and Bedrock Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab demonstrates how to enhance a Retrieval-Augmented Generation (RAG) pipeline by integrating Amazon SageMaker Inference with Amazon Bedrock Guardrails. We will walk through the process of querying a OpenSearch vector knowledge base, using SageMaker for model inference, applying Guardrails to control the generation of responses, and filtering results with metadata to ensure compliance and quality. We will use the same PubMed medical theme generated in the opensearch RAG lab where we will refer to the previously created opensearch vector database with PubMed dataset and show how guardrails can be used to filter the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Before proceeding with this notebook, you should complete all of the previous labs.\n",
    "- Required Python libraries: opensearch-py, langchain, boto3, requests_aws4auth\n",
    "- Access to Amazon SageMaker and OpenSearch\n",
    "- Appropriate IAM roles and permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain boto3 -q\n",
    "%pip install opensearch-py\n",
    "%pip install requests-aws4auth\n",
    "%pip install certifi\n",
    "print(\"Installs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError, BotoCoreError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r OS_DOMAIN_NAME\n",
    "%store -r AOS_HOST\n",
    "%store -r OPENSEARCH_INDEX_NAME\n",
    "%store -r EMBEDDING_MODEL_NAME\n",
    "%store -r EMBED_ENDPOINT_NAME\n",
    "%store -r GENERATION_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OS_DOMAIN_NAME:{OS_DOMAIN_NAME}\")\n",
    "print(f\"AOS_HOST:{AOS_HOST}\")\n",
    "print(f\"OPENSEARCH_INDEX_NAME:{OPENSEARCH_INDEX_NAME}\")\n",
    "print(f\"EMBEDDING_MODEL_NAME:{EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"EMBED_ENDPOINT_NAME:{EMBED_ENDPOINT_NAME}\")\n",
    "print(f\"GENERATION_ENDPOINT_NAME:{GENERATION_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opensearch Configuration\n",
    "OPENSEARCH_URL = f\"https://{AOS_HOST}\"\n",
    "service = \"es\"  \n",
    "port = 443 \n",
    "\n",
    "# Sagemaker configuration\n",
    "session = boto3.Session()\n",
    "sts_client = boto3.client('sts')\n",
    "# Get caller identity\n",
    "caller_identity = sts_client.get_caller_identity()\n",
    "\n",
    "# Extract and print the IAM role ARN\n",
    "iam_role_arn = caller_identity[\"Arn\"]\n",
    "account_id = sts_client.get_caller_identity().get('Account')\n",
    "region = session.region_name\n",
    "\n",
    "print(\"Session's IAM Role ARN:\", iam_role_arn)\n",
    "\n",
    "# Initialize the Amazon Bedrock client in the region\n",
    "bedrock = boto3.client('bedrock', region_name=region)\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Amazon Bedrock Guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a Guardrail\n",
    "\n",
    "Bedrock Guardrails enable us to define policies that restrict or modify model responses based on compliance, safety, and contextual relevance. \n",
    "\n",
    "See AWS Bedrock Guardrails documentation for more details: https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-components.html \n",
    "\n",
    "Note: Update the IAM role by adding the managed policy `arn:aws:iam::aws:policy/AmazonBedrockFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique client request token\n",
    "client_request_token = str(uuid.uuid4())\n",
    "\n",
    "# Create a Guardrail with specific filtering and compliance policies for medical use-case\n",
    "response = bedrock.create_guardrail(\n",
    "    name=\"MedicalContextGuardrails\",\n",
    "    description=\"Restrict responses to PubMed-based medical content only\",\n",
    "    blockedInputMessaging=\"This request cannot be processed due to safety protocols.\",\n",
    "    blockedOutputsMessaging=\"Response blocked per compliance guidelines.\",\n",
    "\n",
    "    # Topic-based restrictions (e.g., denying non-medical advice)\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {'name': 'non-medical-advice', 'definition': 'Any recommendations outside medical expertise or context', 'type': 'DENY'},\n",
    "            {'name': 'misinformation', 'definition': 'Dissemination of inaccurate or unverified medical information', 'type': 'DENY'},\n",
    "            {'name': 'medical-cure-claims', 'definition': 'Claims of guaranteed or definitive cures for medical conditions without sufficient evidence', 'type': 'DENY'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Content filtering policies (e.g., blocking harmful or unethical content)\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {'type': 'HATE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'INSULTS', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'SEXUAL', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'VIOLENCE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'MISCONDUCT', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'PROMPT_ATTACK', 'inputStrength': 'HIGH', 'outputStrength': 'NONE'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Contextual grounding policies ensuring relevance to PubMed-based embeddings\n",
    "    contextualGroundingPolicyConfig={\n",
    "        # Ensure responses are grounded in the embeddings loaded from PubMed articles\n",
    "        'filtersConfig': [\n",
    "            {'type': 'GROUNDING', 'threshold': 0.1},\n",
    "            {'type': 'RELEVANCE', 'threshold': 0.1}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # List of restricted words related to sensitive medical topics\n",
    "    wordPolicyConfig={\n",
    "        # Example: blocking inappropriate usage of critical medical terms\n",
    "        'wordsConfig': [\n",
    "            {'text': \"malpractice\"}, {'text': \"misdiagnosis\"}, {'text': \"unauthorized treatment\"},\n",
    "            {'text': \"experimental drug\"}, {'text': \"unapproved therapy\"}, {'text': \"medical fraud\"},\n",
    "            {'text': \"cure\"}, {'text': \"guaranteed cure\"}, {'text': \"permanent remission\"}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Sensitive data anonymization (e.g., patient information)\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        # Anonymize identifiable patient information\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': \"NAME\", \"action\": \"ANONYMIZE\"}, {'type': \"EMAIL\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PHONE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_SOCIAL_SECURITY_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"ADDRESS\", \"action\": \"ANONYMIZE\"}, {'type': \"CA_HEALTH_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PASSWORD\", \"action\": \"ANONYMIZE\"}, {'type': \"IP_ADDRESS\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"CA_SOCIAL_INSURANCE_NUMBER\", \"action\": \"ANONYMIZE\"}, {'type': \"CREDIT_DEBIT_CARD_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"AGE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_BANK_ACCOUNT_NUMBER\", \"action\": \"ANONYMIZE\"}\n",
    "        ],\n",
    "        # Example regex patterns for anonymizing sensitive medical data\n",
    "        'regexesConfig': [\n",
    "            {\n",
    "                \"name\": \"medical_procedure_code\",\n",
    "                \"description\": \"Pattern for medical procedure codes\",\n",
    "                \"pattern\": \"\\\\b[A-Z]{1,5}\\\\d{1,5}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"clinical_trial_id\",\n",
    "                \"description\": \"Pattern for clinical trial identifiers\",\n",
    "                \"pattern\": \"\\\\bNCT\\\\d{8}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Tags for environment tracking\n",
    "    tags=[\n",
    "        {\"key\": \"Environment\", \"value\": \"Production\"},\n",
    "        {\"key\": \"Department\", \"value\": \"Medical\"}\n",
    "    ],\n",
    "    clientRequestToken=client_request_token\n",
    ")\n",
    "\n",
    "# Retrieve and print the Guardrail ID, ARN, and version\n",
    "guardrail_id = response['guardrailId']\n",
    "print(f\"Guardrail ID: {guardrail_id}\")\n",
    "print(f\"Guardrail ARN: {response['guardrailArn']}\")\n",
    "print(f\"Version: {response['version']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create a Published Version of the Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a published version\n",
    "version_response = bedrock.create_guardrail_version(\n",
    "    guardrailIdentifier=response['guardrailId'],\n",
    "    description=\"Production version 1.0\"\n",
    ")\n",
    "guardrail_version=version_response['version']\n",
    "guardrail_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define function to apply bedrock guardrail at inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_output_guardrail(output_text):\n",
    "    \"\"\"Apply guardrails to the output after generation\"\"\"\n",
    "    \n",
    "    print(f\"\\nApply bedrock guardrails to the output using {guardrail_id} {guardrail_version}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Use only the parameters supported by your boto3 version\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion=guardrail_version,\n",
    "            source='OUTPUT',\n",
    "            content=[\n",
    "                {\n",
    "                    'text': {\n",
    "                        'text': output_text\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Process response based on what fields are available\n",
    "        if 'outputs' in response and response['outputs']:\n",
    "            return response['outputs'][0]['text']\n",
    "        else:\n",
    "            return output_text\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Output guardrail application failed: {str(e)}\")\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Define SageMaker functions\n",
    "We use the similar approach for OpenSearch retrieval and SageMaker inference as defined in the previous lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are an assistant for question-answering tasks. Answer the following question using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        <|start_header_id|>assistant<|end_header_id|> \n",
    "        Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": model_kwargs})\n",
    "        #print(\"Input prompt:\", input_str)\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        # Parse and extract generated text from response\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(\"Raw response:\", response_json)  # Debugging\n",
    "        # Handle different response formats\n",
    "        return response_json[\"generated_text\"]\n",
    "\n",
    "\n",
    "content_handler = ContentHandler()\n",
    "\n",
    "sagemaker_llm=SagemakerEndpoint(\n",
    "        endpoint_name=GENERATION_ENDPOINT_NAME,\n",
    "        region_name=region,\n",
    "        model_kwargs={\"temperature\": 1e-10, \"max_new_tokens\": 250},\n",
    "        content_handler=content_handler,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated prompt and context for medical question\n",
    "\n",
    "prompt = \"What is the role of mitochondrial dynamics in programmed cell death in lace plants?\"\n",
    "context_prompt = \"\"\"\n",
    "Based on research into Aponogeton madagascariensis (lace plant), programmed cell death (PCD) occurs in the cells at the center of areoles in leaves.\n",
    "The role of mitochondrial dynamics during this process is being investigated.\n",
    "\"\"\"\n",
    "\n",
    "input_prompt = PROMPT.format(question=prompt, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "response = sagemaker_llm(input_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. OpenSearch retrieval \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Define the OpenSearch vector database retrieval\n",
    "Note: The IAM role used will need the managed permissions `arn:aws:iam::aws:policy/AmazonOpenSearchServiceFullAccess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import boto3\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenSearch using the IAM Role of this notebook\n",
    "credentials = boto3.Session().get_credentials()\n",
    "signerauth = AWSV4SignerAuth(credentials, region, \"es\")\n",
    "\n",
    "# Create OpenSearch client\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[f\"https://{AOS_HOST}\"],\n",
    "    http_auth=signerauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=60\n",
    ")\n",
    "print(\"Connection details: \")\n",
    "aos_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test opensearch connection by listing indices\n",
    "try:\n",
    "    response = aos_client.indices.get_alias(\"*\")\n",
    "    print(\"Indices:\", response)\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to OpenSearch:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "class EmbedContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            inputs: List of input strings.\n",
    "            model_kwargs: Additional keyword arguments to be passed to the endpoint.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        # Example: inference.py expects a JSON string with a \"inputs\" key:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        # Example: inference.py returns a JSON string with the list of\n",
    "        # embeddings in a \"vectors\" key:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # print(len(response_json))\n",
    "        return response_json\n",
    "\n",
    "\n",
    "embed_content_handler = EmbedContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, embed_endpoint_name, model_kwargs=None):\n",
    "    \"\"\"\n",
    "    Call the SageMaker embedding model to embed the given text.\n",
    "    Adjust the payload and response parsing according to your model's API.\n",
    "    \"\"\"\n",
    "    embeddings = SagemakerEndpointEmbeddings(\n",
    "        endpoint_name=embed_endpoint_name,\n",
    "        region_name=region,\n",
    "        content_handler=embed_content_handler,\n",
    "    )\n",
    "\n",
    "    return embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# RAG retrieval function\n",
    "def retrieve_context(query, k=3):\n",
    "    query_embedding = get_embedding(query, EMBED_ENDPOINT_NAME)\n",
    "    \n",
    "    search_body = {\n",
    "        \"size\": k,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"context_vector\": {\n",
    "                    \"vector\": query_embedding,\n",
    "                    \"k\": k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = aos_client.search(\n",
    "        index=OPENSEARCH_INDEX_NAME,\n",
    "        body=search_body\n",
    "    )\n",
    "    \n",
    "    return [hit[\"_source\"]['contexts'] for hit in response[\"hits\"][\"hits\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve relevant context\n",
    "query = \" what are the key components of phonological processing that are believed to influence \\\n",
    "reading levels in individuals who have undergone cerebral hemispherectomy procedure?\"\n",
    "contexts = retrieve_context(query)\n",
    "print(\"Retrieved context:\", contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Retrieve context for input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opensearch_vector_context(input_query):\n",
    "    # Retrieve relevant context\n",
    "    query = \"What is Amazon?\"\n",
    "    contexts = retrieve_context(input_query)\n",
    "    \n",
    "    for context in contexts:\n",
    "        print(\"\\nRetrieved context: \", context)\n",
    "\n",
    "    # Handle empty or single returned list\n",
    "    if contexts:\n",
    "        context_prompt = contexts[0] if len(contexts) > 0 else None\n",
    "    else:\n",
    "        context_prompt = None\n",
    "    \n",
    "    return context_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Validate inference results with Bedrock guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Restrict reponse based on blocked content/topic\n",
    "\n",
    "In this example you'll test the guardrail's ability to block responses related to cures for diseases without sufficient evidence.\n",
    "\n",
    "When prompted  \"Is there a cure for HIV based on the latest research?\" the base model responds with \"No\", which is correct, but in this scenario the model shouldn't provide any affirmative answers related to cures per compliance guidlines. \n",
    "\n",
    "Therefore the guardrail should block the response and respond with \"Response blocked per compliance guidelines.\". You could additionally apply this guardrail on the input, blocking the request from ever making it to the generation model to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"Is there a cure for HIV based on the latest research?\"\n",
    "\n",
    "# Retrieve relevant context from the OpenSearch based on the query\n",
    "# TODO Update the context retrieval function\n",
    "context_prompt = get_opensearch_vector_context(query)  # Replace with your OpenSearch retrieval\n",
    "\n",
    "input_prompt = PROMPT.format(question=query, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "raw_response = sagemaker_llm(input_prompt)\n",
    "print(\"\\n\\n Initial response without guardrails: \", raw_response)\n",
    "\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"\\n\\n User's query: \", query)  # Use the variable directly, not a dictionary\n",
    "\n",
    "# Print the generated answer from the model based on the query and context\n",
    "print(\"\\n\\n Answer with guardrails: \", guardrail_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Restrict reponse based on PII Data\n",
    "\n",
    "This example shows a simple redaction of content coming back from the generation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query that will be sent to the model\n",
    "query = \"Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, who led the clinical trial NCT12345678?\"\n",
    "\n",
    "# Retrieve relevant context from the OpenSearch based on the query\n",
    "# TODO Update the context retrieval function\n",
    "context_prompt = get_opensearch_vector_context(query) # Replace with your OpenSearch retrieval\n",
    "\n",
    "input_prompt = PROMPT.format(question=query, context=context_prompt)\n",
    "\n",
    "# Invoke the model using the prompt\n",
    "raw_response = sagemaker_llm(input_prompt)\n",
    "print(\"\\n\\nInitial response without guardrails: \", raw_response)\n",
    "\n",
    "guardrail_response = apply_output_guardrail(raw_response)\n",
    "\n",
    "# Print the user's query\n",
    "print(\"\\n\\nUser's query: \", query) # Use the variable directly, not a dictionary\n",
    "\n",
    "# Print the generated answer from the model based on the query and context, with applying the guardrails\n",
    "print(\"\\n\\nAnswer with guardrails: \", guardrail_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
