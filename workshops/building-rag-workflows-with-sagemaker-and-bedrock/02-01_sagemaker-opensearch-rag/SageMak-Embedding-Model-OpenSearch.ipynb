{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082aa845",
   "metadata": {},
   "source": [
    "# Lab 3: Rag with Amazon SageMaker AI endpoint and Amazon OpenSearch and evaluate RAG with Ragas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c50906",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook demonstrates how to implement a Retrieval Augmented Generation (RAG) solution using:\n",
    "- Amazon SageMaker for hosting embedding and LLM models\n",
    "- Amazon OpenSearch for vector search\n",
    "- LangChain for orchestrating the RAG pipeline\n",
    "- You'll explore ways to evaluate the quality of Retrieval-Augmented Generation (RAG) pipelines with the opensource tools like [RAGAS](https://docs.ragas.io/en/v0.1.21/index.html). You will use the OpenSearch Vector Database and the RAG results generation to show offline evaluation and scoring.\n",
    "\n",
    "In this notebook, Question Answering solution with Large Language Models (LLMs) and Amazon OpenSearch Service. An application using the RAG(Retrieval Augmented Generation) approach retrieves information most relevant to the user’s request from the enterprise knowledge base or content, bundles it as context along with the user’s request as a prompt, and then sends it to the LLM to get a GenAI response.\n",
    "\n",
    "LLMs have limitations around the maximum word count for the input prompt, therefore choosing the right passages among thousands or millions of documents in the enterprise, has a direct impact on the LLM’s accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d37f81",
   "metadata": {},
   "source": [
    "<H2>Part 1: Build conversational search with OpenSearch Service</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7b731",
   "metadata": {},
   "source": [
    "The vector dataset used in this part of the lab is comprised of a predefined content resource from the [PubMedQA](https://pubmedqa.github.io/) dataset.\n",
    "\n",
    "You will use OpenSearch ingest pipeline with embedding processor to generate text embeddings for the dataset. Using the neural plugin in OpenSearch will allow you to generate the embeddings of the search query as well.\n",
    "You will then use the large language model (LLM) hosted on Amazon SageMaker endpoints with the RAG processor in the search pipeline to generate text. The RAG processor will combine the retrieved search results from OpenSearch with the generated answer from the LLM to send back to the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df92a75",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.1. Import libraries & initialize resources\n",
    "The code blocks below will install and import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35fc57",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip uninstall -q -y autogluon-multimodal autogluon-timeseries autogluon-features autogluon-common autogluon-core\n",
    "\n",
    "%pip install -Uq boto3==1.37.38\n",
    "%pip install -Uq sagemaker==2.243.2\n",
    "    \n",
    "%pip install -Uq opensearch-py==2.8.0\n",
    "%pip install -Uq opensearch_py_ml==1.1.0\n",
    "%pip install -Uq deprecated==1.2.18\n",
    "%pip install -Uq requests_aws4auth==1.3.1\n",
    "\n",
    "%pip install -Uq transformers==4.51.3\n",
    "\n",
    "%pip install -Uq datasets==3.5.0\n",
    "%pip install -Uq ragas==0.2.14\n",
    "%pip install -Uq python-dotenv==1.1.0\n",
    "\n",
    "%pip install -Uq langchain==0.3.24\n",
    "%pip install -Uq langchain-aws==0.2.21\n",
    "\n",
    "%pip uninstall -yq packaging\n",
    "%pip install -Uq packaging==24.1\n",
    "    \n",
    "print(\"Installs completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a96ce5f-a471-4abb-b894-fd4543a1c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d38375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "from typing import Any, Dict, List, Optional\n",
    "import boto3\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Langchain\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_community.llms import SagemakerEndpoint\n",
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "\n",
    "# Sagemaker\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# RAGAS\n",
    "import ragas\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a9721-d612-4701-a4ba-f8b4447a6f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "region = sess.boto_region_name\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "opensearch_client = boto3.client('opensearch')\n",
    "\n",
    "\n",
    "print(f\"account id: {account_id}\")\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f91e0d1-ebca-4260-80f3-b49dd65a576e",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc6270-360c-493d-b83d-1a53af086b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r OS_DOMAIN_NAME\n",
    "%store -r AOS_HOST\n",
    "%store -r EMBEDDING_MODEL_NAME\n",
    "%store -r EMBED_ENDPOINT_NAME\n",
    "%store -r GENERATION_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca3dea-13cc-4a73-8b99-798fa7daa054",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"OS_DOMAIN_NAME:{OS_DOMAIN_NAME}\")\n",
    "print(f\"AOS_HOST:{AOS_HOST}\")\n",
    "print(f\"EMBEDDING_MODEL_NAME:{EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"EMBED_ENDPOINT_NAME:{EMBED_ENDPOINT_NAME}\")\n",
    "print(f\"GENERATION_ENDPOINT_NAME:{GENERATION_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed452d3d-d89e-48ad-bdcf-a2c9acacc820",
   "metadata": {},
   "source": [
    "# 2. Build retrieval integration with OpenSearch\n",
    "\n",
    "We have taken the PubMedQA dataset and prepared it to include the contexts in the `extracted_context.json` file.\n",
    "\n",
    "The following cells will perform the steps to generate embeddings with the dataset and ingest into the OpenSearch vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61449be0-4aae-49e4-81b9-f7cf0ec8379f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.1 Establish a connection to the OpenSearch Service domain\n",
    "\n",
    "### OpenSearch Configuration\n",
    "- Establish connection to OpenSearch domain\n",
    "- Create index with KNN vector search capabilities\n",
    "- Define mapping for document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797317a5-f6df-4c08-9a98-b81df5e1a456",
   "metadata": {},
   "source": [
    "### 🚨 Authentication cell below 🚨 \n",
    "The below cell establishes an authenticated connection to our OpenSearch Service domain. The connection will periodically expire.\n",
    "If you see an `AuthorizationException` error later in this notebook it means that the connection has expired and you just need to re-run the cell to get a new security tokken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10616439-4ed6-4faf-8157-da56df28d69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to OpenSearch using the IAM Role of this notebook\n",
    "credentials = boto3.Session().get_credentials()\n",
    "signerauth = AWSV4SignerAuth(credentials, region, \"es\")\n",
    "\n",
    "# Create OpenSearch client\n",
    "aos_client = OpenSearch(\n",
    "    hosts=[f\"https://{AOS_HOST}\"],\n",
    "    http_auth=signerauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=60\n",
    ")\n",
    "print(\"Connection details: \")\n",
    "aos_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec7693-3a0b-41c7-b154-fed58a7502ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 Create the index with defined mappings.\n",
    "\n",
    "It is important to define the 'knn_vector' fields as without the propper definitions dynamic mapping would type these as simple float fields.\n",
    "\n",
    "A **k-NN (k-Nearest Neighbors)** enabled index is created in OpenSearch to store vector embeddings. The index schema defines:\n",
    "\n",
    "A **knn_vector** field (`context_vector`) for storing embeddings.\n",
    "\n",
    "**Note: ensure that the `dimension` parameter of your index mapping properties matches the dimensionality of your embedding model. In this example, `Alibaba-NLP/gte-base-en-v1.5` outputs vectors of `768` dimensions.**\n",
    "\n",
    "To learn more about OpenSearch service, you can refer to the [document](https://aws.amazon.com/opensearch-service/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd38da-4044-4795-ba44-953dc5dff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENSEARCH_INDEX_NAME = 'opensearch-rag-index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25edad41-df50-4dde-a349-f5fdf8ceb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store OPENSEARCH_INDEX_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa795f-c62c-4fb3-9207-af0ffce9029f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Create the k-NN index\n",
    "# Check if the index exists. Delete and recreate if it does. \n",
    "if aos_client.indices.exists(index=OPENSEARCH_INDEX_NAME):\n",
    "    print(\"The index exists. Deleting...\")\n",
    "    response = aos_client.indices.delete(index=OPENSEARCH_INDEX_NAME)\n",
    "    \n",
    "payload = { \n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"knn\": True\n",
    "    }\n",
    "  },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"context_vector\": {\n",
    "              \"type\": \"knn_vector\",\n",
    "              \"dimension\": 768,\n",
    "              \"method\": {\n",
    "                \"engine\": \"faiss\",\n",
    "                \"space_type\": \"l2\",\n",
    "                \"name\": \"hnsw\",\n",
    "                \"parameters\": {}\n",
    "              }\n",
    "            },\n",
    "            \"template\": {\n",
    "              \"type\": \"keyword\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "}\n",
    "\n",
    "print(\"Creating index...\")\n",
    "response = aos_client.indices.create(index=OPENSEARCH_INDEX_NAME,body=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7145ba-f124-4294-a518-b2dfa03d2fa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Use SageMaker Embedding Endpoint\n",
    "In the prerequisites, you deployed a **text embedding model (Alibaba-NLP/gte-base-en-v1.5)** to a SageMaker real-time endpoint. This model converts text into 768-dimensional vectors for semantic search.\n",
    "\n",
    "First, test the embeddings endpoint to ensure it is functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0974ac-ab63-4804-84b1-acd8a54960e0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_text = \"Is adjustment for reporting heterogeneity necessary in sleep disorders?\"\n",
    "# invoke the embedding model\n",
    "input_str = {\"inputs\": query_text}\n",
    "output = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=EMBED_ENDPOINT_NAME,\n",
    "    Body=json.dumps(input_str),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "embeddings = output[\"Body\"].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda04dc-9ae0-41b2-b002-99a56ca7ee37",
   "metadata": {},
   "source": [
    "We can wrap up our SageMaker endpoints for embedding model into `langchain.embeddings.SagemakerEndpointEmbeddings` class to make it compatible with SageMaker embedding model and can be use with other LangChain functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257c6fb-19a1-4add-84f5-f5b78f113f96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        \"\"\"\n",
    "        Transforms the input into bytes that can be consumed by SageMaker endpoint.\n",
    "        Args:\n",
    "            inputs: List of input strings.\n",
    "            model_kwargs: Additional keyword arguments to be passed to the endpoint.\n",
    "        Returns:\n",
    "            The transformed bytes input.\n",
    "        \"\"\"\n",
    "        # Example: inference.py expects a JSON string with a \"inputs\" key:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Transforms the bytes output from the endpoint into a list of embeddings.\n",
    "        Args:\n",
    "            output: The bytes output from SageMaker endpoint.\n",
    "        Returns:\n",
    "            The transformed output - list of embeddings\n",
    "        Note:\n",
    "            The length of the outer list is the number of input strings.\n",
    "            The length of the inner lists is the embedding dimension.\n",
    "        \"\"\"\n",
    "        # Example: inference.py returns a JSON string with the list of\n",
    "        # embeddings in a \"vectors\" key:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # print(len(response_json))\n",
    "        return response_json\n",
    "\n",
    "\n",
    "embed_content_handler = EmbedContentHandler()\n",
    "\n",
    "\n",
    "embeddings_function = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=EMBED_ENDPOINT_NAME,\n",
    "    region_name=region,\n",
    "    content_handler=embed_content_handler,\n",
    ")\n",
    "\n",
    "query_result = embeddings_function.embed_query(query_text)\n",
    "print(\"Output:\\n\", query_result, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fc8991-83b2-4ee4-a8e2-fb3fb2dcc9b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4 Load data into the new index\n",
    "\n",
    "### Data Processing\n",
    "- Load and process input data\n",
    "- Generate embeddings for documents\n",
    "- Index documents with their embeddings in OpenSearch\n",
    "\n",
    "We will use the [bulk API](https://opensearch.org/docs/latest/api-reference/document-apis/bulk/) to load all of the products into our newly created index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d374d4-4453-4ba9-88fc-45e0b9945071",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, embed_endpoint_name, model_kwargs=None):\n",
    "    \"\"\"\n",
    "    Call the SageMaker embedding model to embed the given text.\n",
    "    Adjust the payload and response parsing according to your model's API.\n",
    "    \"\"\"\n",
    "    embeddings = SagemakerEndpointEmbeddings(\n",
    "        endpoint_name=embed_endpoint_name,\n",
    "        region_name=region,\n",
    "        content_handler=embed_content_handler,\n",
    "    )\n",
    "\n",
    "    return embeddings.embed_query(text)\n",
    "\n",
    "get_embedding(query_text, EMBED_ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920811c-b15e-4b58-8699-598cf9992e8e",
   "metadata": {},
   "source": [
    "- **Chunking**: Long documents are split into smaller passages (max 256 tokens) using LangChain's `RecursiveCharacterTextSplitter`.\n",
    "\n",
    "- **Embedding Generation**: Each chunk is converted into a vector using the SageMaker embedding endpoint.\n",
    "\n",
    "- **Bulk Ingestion**: The embeddings and text are indexed into OpenSearch for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7077fed-8c54-4162-bde5-974713781a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer matching your embedding model (e.g., \"sentence-transformers/all-mpnet-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Configure splitter with model-aware tokenization\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=250,  # 256 - safety buffer\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\"],  # FIRST try splitting at paragraphs, then lines\n",
    "    keep_separator=True,  # Preserve paragraph/line breaks in chunks\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "def validate_chunk(chunk: str) -> bool:\n",
    "    \"\"\"Ensure chunk doesn't exceed token limit with model's actual tokenization\"\"\"\n",
    "    tokens = tokenizer.encode(chunk, add_special_tokens=True)\n",
    "    return len(tokens) <= 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5bd92-1992-42b6-b547-31f157762bc6",
   "metadata": {},
   "source": [
    "The below cell might take more than 10 mins, please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25960198-b021-4717-80ac-8066c36f67ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_filename = \"extracted_context.json\"\n",
    "output_filename = \"output_embedded.jsonl\"  # Line-delimited JSON\n",
    "\n",
    "\n",
    "# Load the input JSON file (mapping IDs to lists of context strings)\n",
    "with open(input_filename, \"r\", encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "# Open the output file for writing line-delimited JSON objects\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for key, contexts in data.items():\n",
    "        embeddings = []\n",
    "        all_chunks = []\n",
    "        for ctx_idx, context in enumerate(contexts):\n",
    "                # First attempt: split at paragraphs/lines only\n",
    "                chunks = text_splitter.split_text(context)\n",
    "\n",
    "                # Second pass: check and fix any chunks that still exceed limits\n",
    "                final_chunks = []\n",
    "                for chunk in chunks:\n",
    "                    if validate_chunk(chunk):\n",
    "                        final_chunks.append(chunk)\n",
    "                    else:\n",
    "                        # Force split at sentences ONLY if absolutely necessary\n",
    "                        emergency_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                            tokenizer=tokenizer,\n",
    "                            chunk_size=250,\n",
    "                            chunk_overlap=50,\n",
    "                            separators=[\". \"],  # Only split sentences when forced\n",
    "                            keep_separator=True\n",
    "                        )\n",
    "                        final_chunks.extend(emergency_splitter.split_text(chunk))\n",
    "\n",
    "                # Embed validated chunks\n",
    "                for chunk_idx, chunk in enumerate(final_chunks):\n",
    "                    if not validate_chunk(chunk):\n",
    "                        continue  # Skip invalid chunks or handle differently\n",
    "\n",
    "                    embedding = get_embedding(chunk, EMBED_ENDPOINT_NAME)\n",
    "                    output_obj = {\n",
    "                        \"id\": f\"{key}-{ctx_idx}-{chunk_idx}\",\n",
    "                        \"contexts\": chunk,\n",
    "                        \"context_vector\": embedding\n",
    "                    }\n",
    "                    outfile.write(json.dumps(output_obj) + \"\\n\")\n",
    "\n",
    "print(f\"Embeddings saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fdfb9-dadb-4635-9624-4ff715508e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read all JSON objects from the JSONL file\n",
    "with open(\"output_embedded.jsonl\", \"r\", encoding=\"utf-8\") as infile:\n",
    "    json_objects = [json.loads(line) for line in infile]\n",
    "\n",
    "# Write the objects as a JSON array into a new .txt file\n",
    "with open(\"merged_output.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(json_objects, outfile, indent=4)\n",
    "\n",
    "print(\"Merged JSON objects have been saved to merged_output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9818c-929a-418d-970b-3fcb48ac4852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_file(input_filename, output_filename):\n",
    "    # Load the merged file, which is expected to be a JSON array\n",
    "    with open(input_filename, 'r', encoding='utf-8') as infile:\n",
    "        records = json.load(infile)\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        # Process each record in the array\n",
    "        for record in records:\n",
    "            contexts = record.get(\"contexts\", [])\n",
    "            vectors = record.get(\"context_vector\", [])\n",
    "            # For each pair of context string and corresponding embedding vector:\n",
    "            # Create a new object without the \"id\" field.\n",
    "            new_obj = {\n",
    "                \"contexts\": contexts,\n",
    "                \"context_vector\": vectors\n",
    "            }\n",
    "            # Write the JSON object as a single line\n",
    "            outfile.write(json.dumps(new_obj) + \"\\n\")\n",
    "\n",
    "transform_file(\"merged_output.txt\", \"final_output_oneline.txt\")\n",
    "print(\"Transformation complete. Check final_output.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8332f3-8a4b-4036-b548-3181e8be0233",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Index TEXT file into index: opensearch-rag-index\n",
    "batch = 0\n",
    "count = 0\n",
    "batch_size = 5\n",
    "body_ = ''\n",
    "action = json.dumps({ 'index': { '_index': OPENSEARCH_INDEX_NAME } })\n",
    "errors = []\n",
    "with open('final_output_oneline.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        if count > 5000:\n",
    "            break # Use this to run a limited number of items.\n",
    "        body_ = body_ + action + \"\\n\" + line + \"\\n\"\n",
    "        # print(f\"body: {body_}\")\n",
    "        if count % batch_size == 0 and count != 0:\n",
    "            batch+=1\n",
    "            if count % (batch_size*30) == 0:\n",
    "                print(\"Batch: \" + str(batch) + \", count: \" + str(count)+ \", errors: \" + str(len(errors)))\n",
    "            response = aos_client.bulk(\n",
    "                index = OPENSEARCH_INDEX_NAME,\n",
    "                body = body_\n",
    "            )\n",
    "            body_ = ''\n",
    "            if response['errors'] == True:\n",
    "                for item in response['items']:\n",
    "                    if item['index']['status'] != 201:\n",
    "                        errors.append(item['index']['error']) \n",
    "        # print(response)\n",
    "        # break \n",
    "        count += 1\n",
    "if body_ !=\"\":\n",
    "    response = aos_client.bulk(\n",
    "        index = OPENSEARCH_INDEX_NAME,\n",
    "        body = body_\n",
    "    )\n",
    "if response['errors'] == True:\n",
    "    for item in response['items']:\n",
    "        if item['index']['status'] != 201:\n",
    "            errors.append(item['index']['error'])\n",
    "print(\"Last batch: \" + str(batch) + \", document count: \" + str(count)+ \", errors: \" + str(len(errors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7dfdad-d5ec-48f2-8a71-16bfc7700c00",
   "metadata": {},
   "source": [
    "## 2.5 Query OpenSearch Database\n",
    "\n",
    "Once the vectors are ingested into the database, we can run queries to retrieve relevant contexts based on the input query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb85e3e-5197-448a-b9f6-3aa5100b35c5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your natural language query\n",
    "query_vector = get_embedding(query_text, EMBED_ENDPOINT_NAME)\n",
    "\n",
    "# Now, use the embedding in a k-NN query\n",
    "knn_query = {\n",
    "    \"size\": 5,  # adjust how many results you want to retrieve\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"context_vector\": {\n",
    "                \"vector\": query_vector,\n",
    "                \"k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response_knn = aos_client.search(\n",
    "    index=OPENSEARCH_INDEX_NAME,\n",
    "    body=knn_query\n",
    ")\n",
    "\n",
    "print(\"KNN Query Results:\")\n",
    "for hit in response_knn['hits']['hits']:\n",
    "    print(hit['_source'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a1587-fb44-417a-a4d5-9ed0ca54c8e9",
   "metadata": {},
   "source": [
    "# 3. Build end-to-end RAG pipeline with LLM models hosted on SageMaker AI and LangChain\n",
    "\n",
    "We plan to use document embeddings to fetch the most relevant documents in our document knowledge library and combine them with the prompt that we provide to LLM.\n",
    "\n",
    "To achieve that, we will do following.\n",
    "\n",
    "1. **Generate embedings for each of document in the knowledge library with SageMaker hosted embedding model.**\n",
    "2. **Identify top K most relevant documents based on user query.**\n",
    "    - 2.1 **For a query of your interest, generate the embedding of the query using the same embedding model.**\n",
    "    - 2.2 **Search the indexes of top K most relevant documents in the embedding space using in-memory Faiss search.**\n",
    "    - 2.3 **Use the indexes to retrieve the corresponded documents.**\n",
    "3. **Combine the retrieved documents with prompt and question and send them into SageMaker LLM.**\n",
    "\n",
    "\n",
    "\n",
    "Note: The retrieved document/text should be large enough to contain enough information to answer a question; but small enough to fit into the LLM prompt -- maximum sequence length of 1024 tokens. \n",
    "\n",
    "---\n",
    "To build a simiplied QA application with LangChain, we need: \n",
    "1. Wrap up our SageMaker endpoints for embedding model and LLM into `langchain.embeddings.SagemakerEndpointEmbeddings` and `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. (We have already created the embedding SageMaker wrapper class in the previous section.\n",
    "2. Prepare the dataset to build the knowledge data base. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e04f987-5478-40b4-be60-a978b7e9164f",
   "metadata": {},
   "source": [
    "Now you will use the `Llama 3.1 8B` model you deployed to a SageMaker real-time endpoint in the prerequisites with LangChain.\n",
    "\n",
    "Invoke the LLM endpoint for a quick test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1b459-3d6b-4372-b29c-5172e2a3a936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_str = { \"inputs\": query_text, \n",
    "            \"parameters\": { \n",
    "                \"max_new_tokens\": 100, \n",
    "                \"top_p\": 0.9, \n",
    "                \"temperature\": 0.6 \n",
    "            }\n",
    "        }\n",
    "output = sm_runtime_client.invoke_endpoint(\n",
    "    EndpointName=GENERATION_ENDPOINT_NAME,\n",
    "    Body=json.dumps(input_str),\n",
    "    ContentType=\"application/json\"\n",
    ")\n",
    "embeddings = output[\"Body\"].read().decode(\"utf-8\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5c343-08c6-47dd-86a8-4182246413be",
   "metadata": {},
   "source": [
    "Next, we wrap up our SageMaker endpoints for LLM into `langchain.llms.sagemaker_endpoint.SagemakerEndpoint`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a82c1d-820a-499a-8160-ccacbf86426b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.9\n",
    "}\n",
    "\n",
    "\n",
    "class GenerationContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "        self.len_prompt = len(prompt)\n",
    "        input_str = json.dumps({\"inputs\": prompt, \"parameters\": {**model_kwargs}})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = output.read()\n",
    "        res = json.loads(response_json)\n",
    "        \n",
    "        ans = res['generated_text']\n",
    "        # print(ans)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebab516-c724-4433-b7f4-c8785797a413",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_content_handler = GenerationContentHandler()\n",
    "\n",
    "sm_llm = SagemakerEndpoint(\n",
    "    endpoint_name=GENERATION_ENDPOINT_NAME,\n",
    "    region_name=region,\n",
    "    model_kwargs=parameters,\n",
    "    content_handler=generation_content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8f2eb-7a9c-4de3-a2f1-cd58183950a9",
   "metadata": {},
   "source": [
    "We combine the retrieved documents with prompt and question and send them into SageMaker LLM.\n",
    "\n",
    "We define a customized prompt as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbdbe9-79e3-4386-8b81-9d878f68f857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are an assistant for question-answering tasks. Answer the following question using the provided context. If you don't know the answer, just say \"I don't know.\".\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        <|start_header_id|>assistant<|end_header_id|> \n",
    "        Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "opensearch_url = f\"https://{AOS_HOST}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffe1a9-0e21-42f6-98cc-9b7f98a78674",
   "metadata": {},
   "source": [
    "For this example, we have created the OpenSearch cluster using an IAM role as the master user (your SageMaker execution role). In real world application, you will be authenticating with other user accounts, and should store the user name and password using services that can securely store the value, for example SecretsManager as [shown here](https://github.com/aws-samples/rag-with-amazon-opensearch-and-sagemaker/blob/main/app/opensearch_retriever_llama2.py#L89)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74602217-4745-4a94-83a4-a27643a67daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    opensearch_url=opensearch_url,\n",
    "    index_name=OPENSEARCH_INDEX_NAME,\n",
    "    embedding_function=embeddings_function,\n",
    "    http_auth=signerauth,\n",
    "    connection_class=RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f1c66-7358-41ac-8a88-04a4db8e3d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = opensearch_vector_search.as_retriever(\n",
    "    search_kwargs={\"k\": 3, \"vector_field\": \"context_vector\", \"text_field\": \"contexts\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aceb1b-764b-4c33-a80f-9120a0b87d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": True}\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    sm_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    # return_source_documents=True, ## you can uncomment this line to see the detailed retrieved data source\n",
    "    # verbose=True, #DEBUG\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5c950-1a5f-4524-a287-908f6a26c11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea2a8bd-3694-4384-9e74-7ad37530b25f",
   "metadata": {},
   "source": [
    "# 📂 Test Evaluation Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2c9be-3b7d-4be4-9393-a5ce6f6b0212",
   "metadata": {},
   "source": [
    "### 📊 RAGAS Evaluation Metrics\n",
    "\n",
    "We're going to measure the following aspects of a RAG system. These metrics are defined in **[RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/)**:\n",
    "\n",
    "- 🔍 **[Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)**  \n",
    "  Measures how factually consistent the generated answer is with the retrieved context. It evaluates whether the answer could reasonably be derived from the context.\n",
    "\n",
    "- 🎯 **[Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/)**  \n",
    "  Assesses how relevant the generated answer is to the original user query. A high score indicates the answer is on-topic and useful.\n",
    "\n",
    "- 🧠 **[Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/)**  \n",
    "  Measures how many of the retrieved contexts are truly relevant to answering the question. Precision reflects the \"purity\" of the retrieved chunks.\n",
    "\n",
    "- 📥 **[Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)**  \n",
    "  Evaluates how well the retrieved context covers the information needed to answer the question completely. High recall means fewer relevant facts are missed.\n",
    "\n",
    "- 🧬 **[Answer Similarity](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_similarity/)**  \n",
    "  Compares the generated answer to a reference answer (if available), measuring how semantically close they are using embedding-based similarity.\n",
    "\n",
    "- ✅ **[Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_correctness/)**  \n",
    "  Evaluates whether the generated answer is factually correct and aligns with known ground-truth answers, if such references are available.\n",
    "\n",
    "> 📚 Want to dive deeper into how each metric is computed?  \n",
    "Check out the full [RAGAS metrics documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7c109-4ee3-4f44-ad1a-ac35ad925b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics\n",
    "metrics=[\n",
    "        ragas.metrics.answer_relevancy,\n",
    "        ragas.metrics.faithfulness,\n",
    "        ragas.metrics.context_precision,\n",
    "        ragas.metrics.context_recall,\n",
    "        # ragas.metrics.answer_similarity, # uncomment to add this metric\n",
    "        # ragas.metrics.answer_correctness, # uncomment to add this metric\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f494c7-6c9a-4691-a6e2-99b7cda1f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to init Ragas Metrics\n",
    "def init_ragas_metrics(metrics, llm, embedding):\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            print(metric.name + \" llm\")\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            print(metric.name + \" embedding\")\n",
    "            metric.embeddings = embedding\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e56d1-e46c-417f-94ef-0637fc3034c8",
   "metadata": {},
   "source": [
    "Because some RAGAS metrics contain both [LLM-based and non-LLM-based](https://docs.ragas.io/en/stable/concepts/metrics/overview/#different-types-of-metrics), we will provide the necessary LLM and embedding models to do the evaluation. Here we will use the LLM and embedding model provided by Amazon bedrock to do the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fd6f8-0902-4f57-8f48-36fba4dc239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock as LangChainBedrock\n",
    "# Use correct region for Titan Embed (e.g., us-east-1)\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=region,  # Titan Embed supported here\n",
    ")\n",
    "\n",
    "emb = BedrockEmbeddings(\n",
    "    client=client,\n",
    "    model_id=\"amazon.titan-embed-text-v1\",  # Case-sensitive!\n",
    ")\n",
    "init_ragas_metrics(\n",
    "    metrics,\n",
    "    llm=LangchainLLMWrapper(LangChainBedrock(model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\")),\n",
    "    embedding=LangchainEmbeddingsWrapper(emb),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b67d39-72d2-4bd5-8980-d06c09d798ab",
   "metadata": {},
   "source": [
    "Now we have to initialize the metrics with LLMs and embedding models of your choice. In this example we are going to use the claude-3-5-haiku model and amazon.titan-embed-text-v1 embedding model, and use the convenience wrappers from the `langchain-aws` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04689e62-03d1-40d8-b905-11c52600c166",
   "metadata": {},
   "source": [
    "### Creating the Sagemaker Endpoint for Llama 3.1 8b Instruct Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4517ee-0764-4ea0-8137-9b296616fa60",
   "metadata": {},
   "source": [
    "### Score with RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536de9a8-26de-4733-8a72-2ff9fb5f702e",
   "metadata": {},
   "source": [
    "In this example, we will demonstrate both solutions using prebuilt dataset and a live RAG pipeline with AWS Open Search and evaluate the results using RAGAS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac8b55d-c02e-49ee-bcca-7e3804f683f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def score_with_ragas(query, chunks, answer, metrics, reference):\n",
    "    scores = {}\n",
    "    for metric in metrics:\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=query,\n",
    "            retrieved_contexts=chunks,\n",
    "            response=answer,\n",
    "            reference=reference\n",
    "        )\n",
    "        print(f\"calculating {metric.name}\")\n",
    "        scores[metric.name] = await metric.single_turn_ascore(sample)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e8409-7779-44da-9c39-d65eb3bdf762",
   "metadata": {},
   "source": [
    "#### Scoring RAG\n",
    "We have already setup the Open Search Database in the first section, we can now **evaluate** the quality of its results against a test dataset - to help us **optimize** the configuration for high quality and low cost.\n",
    "\n",
    "First, let's load the sample dataset of questions, reference answers, and their source documents (to find more of how to prepare this dataset, please see more details in [this github](https://github.com/aws-samples/llm-evaluation-methodology/blob/main/datasets/Prepare-SQuAD.ipynb)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263e4f3-710b-46d5-8490-024233a4be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_df = pd.read_csv(\"ori_pqal_10_records.csv\")\n",
    "sample_df = dataset_df.head(1)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068445b5-befc-4157-adf8-23d6a7cfddeb",
   "metadata": {},
   "source": [
    "Records in this dataset include:\n",
    "\n",
    "- (`CONTEXT`) The full text of the source document for this example\n",
    "- (`QUESTION`) The user question to be asked\n",
    "- (`LONG_ANSWER`) A reference 'correct' answer, supported by the document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314689a5-54b0-4571-b5d8-de6c4d0b807e",
   "metadata": {},
   "source": [
    "As shown in [Ragas' API Reference](https://docs.ragas.io/en/latest/references/evaluation.html), records in Ragas evaluation datasets typically include:\n",
    "\n",
    "- The `question` that was asked\n",
    "- The `answer` the system generated\n",
    "- The actual text `contexts` the answer was based on (i.e. snippets of document text retrieved by the search engine)\n",
    "- The `ground_truth` answer(s)\n",
    "\n",
    "\n",
    "We can run an example question through the OpenSearch Vector database to retrieve and generate pipeline as shown below, and extract the references ready to calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d216917-725b-4751-be5f-eb8f4d88117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_and_generate(\n",
    "    question: str,\n",
    "    top_k: int = 3,\n",
    "    system_prompt: str = \"You are a helpful assistant. Use the context to answer concisely.\",\n",
    "    **kwargs,\n",
    "):\n",
    "    # Step 1: Retrieve relevant context from OpenSearch\n",
    "    # Your natural language query\n",
    "    query_vector = get_embedding(question, EMBED_ENDPOINT_NAME)\n",
    "\n",
    "    # Now, use the embedding in a k-NN query\n",
    "    knn_query = {\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"context_vector\": {\n",
    "                    \"vector\": query_vector,\n",
    "                    \"k\": top_k\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    response = aos_client.search(\n",
    "        index=OPENSEARCH_INDEX_NAME,\n",
    "        body=knn_query\n",
    "    )\n",
    "    \n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    contexts = [hit[\"_source\"][\"contexts\"] for hit in hits]\n",
    "    doc_ids = [hit[\"_id\"] for hit in hits]\n",
    "\n",
    "    # Step 2: Format prompt with retrieved context\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "    full_prompt = f\"\"\"Context:\n",
    "            {combined_context}\n",
    "            \n",
    "            Question: {question}\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "\n",
    "    # Step 3: Call your SageMaker-hosted model using LangChain\n",
    "    messages: List = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=full_prompt)\n",
    "    ]\n",
    "    \n",
    "    response_chunk = sm_llm.invoke(messages)\n",
    "    answer = response_chunk  # already joined by your handler\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_doc_ids\": doc_ids,\n",
    "        \"retrieved_doc_texts\": contexts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15856376-f58a-47c0-a90b-5a2b9d2247e2",
   "metadata": {},
   "source": [
    "Run RAG as requests come in and score the results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af56b8a-9f82-4b7e-abb2-b22b94605fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import run\n",
    "\n",
    "def rag_pipeline(\n",
    "    question: str,\n",
    "    reference: str,\n",
    "    metrics: Optional[Any] = None,\n",
    "\n",
    "):\n",
    "    generated_answer = retrieve_and_generate(\n",
    "        question=question,\n",
    "        top_k=3,  # or whatever makes sense for your context window\n",
    "        system_prompt=\"You are a helpful assistant. Use the context below to answer the question.\"\n",
    "    )\n",
    "\n",
    "    answer = generated_answer[\"answer\"]\n",
    "    contexts = generated_answer[\"retrieved_doc_texts\"]\n",
    "    print(f\"question: {question}\")\n",
    "    print(f\"answer: {generated_answer[\"answer\"]}\")\n",
    "    print(f\"reference: {reference}\")\n",
    "\n",
    "    score = run(score_with_ragas(question, contexts, answer=answer, metrics=metrics, reference=reference))\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24403a4-6b9c-4d8f-b5ed-ed40da41a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = []\n",
    "for index, row in sample_df.iterrows():\n",
    "    response = rag_pipeline(\n",
    "        question=row[\"QUESTION\"],\n",
    "        reference=row[\"LONG_ANSWER\"],\n",
    "        metrics=metrics\n",
    "    )\n",
    "    scores.append(response)\n",
    "    print(f\"\\n\\nscore: {json.dumps(response, indent=4)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce200f-b73f-441a-864a-769eb3254cd7",
   "metadata": {},
   "source": [
    "### Key Workflow Summary\n",
    "- Data Preparation: Text is split into chunks and embedded.\n",
    "- OpenSearch Setup: A vector index is created and populated.\n",
    "- Model Deployment: Embedding and LLM models are hosted on SageMaker.\n",
    "- RAG Pipeline: Queries retrieve relevant context, and the LLM generates answers.\n",
    "\n",
    "\n",
    "This notebook provides an end-to-end example of building a production-ready RAG system using AWS services. The same approach can be adapted for other domains by replacing the dataset and fine-tuning the models.\n",
    "\n",
    "# Congratulations for finishing Lab 3. Now please continue on to the next Lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
