{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running hundreds of experiments, comparing the results, and keeping a track of the ML lifecycle can become very complex. This is where MLflow can help streamline the ML lifecycle, from data preparation to model deployment. By integrating MLflow into your LLM workflow, you can efficiently manage experiment tracking, model versioning, and deployment, providing reproducibility. With MLflow, you can track and compare the performance of multiple LLM experiments, identify the best-performing models, and deploy them to production environments with confidence. \n",
    "\n",
    "You can create workflows with SageMaker Pipelines that enable you to prepare data, fine-tune models, and evaluate model performance with simple Python code for each step. \n",
    "\n",
    "Now you can use SageMaker managed MLflow to run LLM fine-tuning and evaluation experiments at scale. Specifically:\n",
    "\n",
    "- MLflow can manage tracking of fine-tuning experiments, comparing evaluation results of different runs, model versioning, deployment, and configuration (such as data and hyperparameters)\n",
    "- SageMaker Pipelines can orchestrate multiple experiments based on the experiment configuration \n",
    "  \n",
    "\n",
    "The following figure shows the overview of the solution.\n",
    "![](./ml-16670-arch-with-mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- [HuggingFace access token](https://huggingface.co/docs/hub/en/security-tokens) â€“ You need a HuggingFace login token to access the gated Llama 3.2 model and datasets used in this post.\n",
    "\n",
    "- Once you have your HuggingFace access token, navigate to the **steps/finetune_llama3b_hf.py** and update the **'hf_token'** parameter with your access token to download the Llama model for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Restart the kernel after executing below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "jupyter-ai 2.30.0 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "autogluon-common 1.2 requires psutil<7.0.0,>=5.7.3, but you have psutil 7.0.0 which is incompatible.\n",
      "autogluon-core 1.2 requires scikit-learn<1.5.3,>=1.4.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "autogluon-features 1.2 requires scikit-learn<1.5.3,>=1.4.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires scikit-learn<1.5.3,>=1.4.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.50.2 which is incompatible.\n",
      "autogluon-tabular 1.2 requires scikit-learn<1.5.3,>=1.4.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires coreforecast==0.0.12, but you have coreforecast 0.0.16 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires mlforecast==0.13.4, but you have mlforecast 0.13.6 which is incompatible.\n",
      "autogluon-timeseries 1.2 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.50.2 which is incompatible.\n",
      "jupyter-scheduler 2.10.0 requires psutil~=5.9, but you have psutil 7.0.0 which is incompatible.\n",
      "s3fs 2024.10.0 requires fsspec==2024.10.0.*, but you have fsspec 2024.9.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, as well as user-defined functions for the pipeline steps like finetune_llama3b_hf and preprocess_llama3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Fetched defaults config from location: /home/sagemaker-user/generative-ai-on-amazon-sagemaker/workshops/fine-tuning-with-sagemakerai-and-bedrock/task_05_fmops\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "processing_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "train_data_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "test_dataset_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "\n",
    "pipeline_name = \"deepseek-finetune-pipeline\"\n",
    "    \n",
    "tracking_server_arn = \"arn:aws:sagemaker:us-east-1:905418257479:mlflow-tracking-server/genai-mlflow-tracker\"\n",
    "experiment_name = \"deepseek-finetune-pipeline\"\n",
    "os.environ[\"mlflow_uri\"] = \"\"\n",
    "os.environ[\"mlflow_experiment_name\"] = \"deepseek-finetune-pipeline\"\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\")\n",
    "model_s3_destination=\"s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\"\n",
    "use_local_model = True #set to false for the training job to download from HF, otherwise True will download locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdd71d1fd4a4b07a2425d6f50075d0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e328bc0b12e94035b8f32ea452cb70a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a41efb99be486ea95e9aa9216f115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7230f0de08f47199ac1906008ef6759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3307c18fdb004866a3752ba1a4448ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab49f643231f4af89e4909e8871c6632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfefa1b16a234d89afcc3bcb3cb3cfec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5365e376f9784e828374235556d58e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "benchmark.jpg:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c7211f1a484ba3b8c890f6e69a56bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff295679c6ca4774abab33f225ff52a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3f879dca5e459db9d8a4f6a9bab91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208aedd936174b738ff02bc6e1c40b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepseek-ai/DeepSeek-R1-Distill-Llama-8B downloaded under ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n",
      "Beginning Model Upload...\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config.json to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config.json\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/README.md to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/README.md\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/generation_config.json to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/generation_config.json\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/LICENSE to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/LICENSE\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/figures/benchmark.jpg to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/figures/benchmark.jpg\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer.json to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer.json\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors\n",
      "upload: ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors to s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors\n",
      "Model Uploaded to: \n",
      " s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n",
      "s3://sagemaker-us-east-1-891377369387/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "# print(\"Downloading model \", model_id)\n",
    "# os.makedirs(model_local_location, exist_ok=True)\n",
    "# snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "# print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "\n",
    "# if default_prefix:\n",
    "#     model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "# else:\n",
    "#     model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "\n",
    "# print(f\"Beginning Model Upload...\")\n",
    "\n",
    "# subprocess.run(['aws', 's3', 'cp', model_local_location, model_s3_destination, '--recursive', '--exclude', '.cache/*', '--exclude', '.gitattributes'])\n",
    "\n",
    "# print(f\"Model Uploaded to: \\n {model_s3_destination}\")\n",
    "\n",
    "# os.environ[\"model_location\"] = model_s3_destination\n",
    "\n",
    "print(model_s3_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique for large language models. The parameters here (lora_r, lora_alpha, lora_dropout) control the behavior of LoRA during fine-tuning, affecting the trade-off between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. MLflow Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. **Update the ARN for the MLflow tracking server.**\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location.\n",
    "\n",
    "experiment_name: give appropriate name for experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset Configuration\n",
    "\n",
    "For the purpose of fine tuning and evaluation we are going too use `HuggingFaceH4/no_robots` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Steps\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "This step handles data preparation. We are going to prepare data for training and evaluation. We will log this data in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPreprocessing\",\n",
    "    instance_type=processing_instance_type,\n",
    "    display_name=\"Data Preprocessing\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def preprocess(\n",
    "    input_path: str,\n",
    "    experiment_name: str,\n",
    "    run_id: str,\n",
    ") -> tuple:\n",
    "    import boto3\n",
    "    import shutil\n",
    "    import sagemaker\n",
    "    from sagemaker.config import load_sagemaker_config\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    bucket_name = sagemaker_session.default_bucket()\n",
    "    default_prefix = sagemaker_session.default_bucket_prefix\n",
    "    configs = load_sagemaker_config()\n",
    "    \n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    import pandas as pd\n",
    "    \n",
    "    dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "    \n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    df = df[:100]\n",
    "    \n",
    "    # df.head()\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "    \n",
    "    print(\"Number of train elements: \", len(train))\n",
    "    print(\"Number of test elements: \", len(test))\n",
    "    \n",
    "    # custom instruct prompt start\n",
    "    prompt_template = f\"\"\"\n",
    "    <|begin_of_text|>\n",
    "    <|start_header_id|>system<|end_header_id|>\n",
    "    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "    Write a response that appropriately completes the request.\n",
    "    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    {{question}}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    {{complex_cot}}\n",
    "    \n",
    "    {{answer}}\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    # template dataset to add prompt to each sample\n",
    "    def template_dataset(sample):\n",
    "        sample[\"text\"] = prompt_template.format(question=sample[\"Question\"],\n",
    "                                                complex_cot=sample[\"Complex_CoT\"],\n",
    "                                                answer=sample[\"Response\"])\n",
    "        return sample\n",
    "    \n",
    "    from datasets import Dataset, DatasetDict\n",
    "    from random import randint\n",
    "    \n",
    "    train_dataset = Dataset.from_pandas(train)\n",
    "    test_dataset = Dataset.from_pandas(test)\n",
    "    \n",
    "    dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "    \n",
    "    train_dataset = dataset[\"train\"].map(template_dataset, remove_columns=list(dataset[\"train\"].features))\n",
    "    \n",
    "    print(train_dataset[randint(0, len(dataset))][\"text\"])\n",
    "    \n",
    "    test_dataset = dataset[\"test\"].map(template_dataset, remove_columns=list(dataset[\"test\"].features))\n",
    "    \n",
    "    # save train_dataset to s3 using our SageMaker session\n",
    "    # if default_prefix:\n",
    "    #     input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    # else:\n",
    "    #     input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    if default_prefix:\n",
    "        input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    else:\n",
    "        input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "    # Save datasets to s3\n",
    "    # We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "    train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "    test_dataset.to_json(\"./data/test/dataset.json\", orient=\"records\")\n",
    "    train_data_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "    test_dataset_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "    s3_client.upload_file(\"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\")\n",
    "    s3_client.upload_file(\"./data/test/dataset.json\", bucket_name, f\"{input_path}/test/dataset.json\")\n",
    "\n",
    "    print(train_data_path)\n",
    "    print(test_dataset_path)\n",
    "\n",
    "    shutil.rmtree(\"./data\")\n",
    "\n",
    "    return experiment_name, run_id, train_data_path, test_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_uri}\"\n",
    "mlflow_experiment_name: \"${mlflow_experiment_name}\"\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"       # Hugging Face model id, or S3 location\n",
    "\n",
    "# sagemaker specific parameters\n",
    "output_dir: \"/opt/ml/model\"                       # path to where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # path to where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # path to where FSx saves test dataset\n",
    "# training parameters\n",
    "max_seq_length: 1500  #512 # 2048\n",
    "lora_r: 8\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1                 \n",
    "learning_rate: 2e-4                    # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "gradient_checkpointing: true           # use gradient checkpointing\n",
    "fp16: true\n",
    "bf16: false                            # use bfloat16 precision, also enables FlashAttention2 (requires Ampere/Hopper GPU+ ex:A10, A100, H100)\n",
    "tf32: false                            # use tf32 precision\n",
    "\n",
    "#uncomment here for fsdp - start\n",
    "# fsdp: \"full_shard auto_wrap offload\"\n",
    "# fsdp_config: \n",
    "#     backward_prefetch: \"backward_pre\"\n",
    "#     cpu_ram_efficient_loading: true\n",
    "#     offload_params: true\n",
    "#     forward_prefetch: false\n",
    "#     use_orig_params: true\n",
    "#uncomment here for fsdp - end\n",
    "\n",
    "merge_weights: true                    # merge weights in the base model\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config uploaded to:\n",
      "s3://sagemaker-us-east-1-891377369387/training_config/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config/args.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning Step**\n",
    "\n",
    "This is where the actual model adaptation occurs. The step takes the preprocessed data and applies it to fine-tune the base LLM (in this case, a Llama model). It incorporates the LoRA technique for efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelFineTuning\",\n",
    "    instance_type=training_instance_type,\n",
    "    display_name=\"Model Fine Tuning\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def train(\n",
    "    train_dataset_s3_path: str,\n",
    "    test_dataset_s3_path: str,\n",
    "    train_config_s3_path: str,\n",
    "    experiment_name: str,\n",
    "    model_id: str,\n",
    "    run_id: str,\n",
    "):\n",
    "    import sagemaker\n",
    "    import boto3\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    job_name = \"deepseek-finetune-pipeline\"\n",
    "    from sagemaker.pytorch import PyTorch\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    pytorch_estimator = PyTorch(\n",
    "        entry_point='launch_fsdp_qlora.py',\n",
    "        source_dir=\"./scripts\",\n",
    "        job_name=job_name,\n",
    "        base_job_name=job_name,\n",
    "        max_run=50000,\n",
    "        role=role,\n",
    "        framework_version=\"2.2.0\",\n",
    "        py_version=\"py310\",\n",
    "        instance_count=1,\n",
    "        instance_type=\"ml.p3.2xlarge\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        volume_size=50,\n",
    "        disable_output_compression=True,\n",
    "        keep_alive_period_in_seconds=1800,\n",
    "        distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "        hyperparameters={\n",
    "            \"config\": \"/opt/ml/input/data/config/args.yaml\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # define a data input dictonary with our uploaded s3 uris\n",
    "    data = {\n",
    "      'train': train_dataset_s3_path,\n",
    "      'eval': test_dataset_s3_path,\n",
    "      'config': train_config_s3_path\n",
    "      }\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    pytorch_estimator.fit(data, wait=True)\n",
    "\n",
    "    return experiment_name, run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Step**\n",
    "\n",
    "After fine-tuning, this step assesses the model's performance. It uses built-in evaluation function in MLflow to evaluate metrices like toxicity, exact_match etc:\n",
    "\n",
    "It will then log the results in MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelEvaluation\",\n",
    "    instance_type=training_instance_type,\n",
    "    display_name=\"Model Evaluation\",\n",
    "    keep_alive_period_in_seconds=3600\n",
    ")\n",
    "def evaluate(\n",
    "    experiment_name: str,\n",
    "    model_id: str,\n",
    "    run_id: str,\n",
    "    model_s3_path: str,\n",
    "):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps. The lora_config is passed as a parameter, allowing for easy modification of LoRA settings between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_step = preprocess(\n",
    "    experiment_name=experiment_name,\n",
    "    run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    input_path=input_path,\n",
    ")\n",
    "\n",
    "training_step = train(\n",
    "    train_dataset_s3_path=preprocessing_step[2],\n",
    "    test_dataset_s3_path=preprocessing_step[3],\n",
    "    train_config_s3_path=train_config_s3_path,\n",
    "    experiment_name=preprocessing_step[0],\n",
    "    run_id=preprocessing_step[1],\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        instance_type,\n",
    "    ],\n",
    "    steps=[preprocessing_step, training_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.ImageUri\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 06:57:34,116 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-30-811/function\n",
      "2025-05-21 06:57:34,211 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-30-811/arguments\n",
      "2025-05-21 06:57:34,443 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpzoewtjvs/requirements.txt'\n",
      "2025-05-21 06:57:34,474 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-30-811/pre_exec_script_and_dependencies'\n",
      "2025-05-21 06:57:34,492 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmp2g46b2f6/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-05-21 06:57:34,507 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmp2g46b2f6/workspace.zip'\n",
      "2025-05-21 06:57:34,542 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/sm_rf_user_ws/2025-05-21-06-57-30-811/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.ImageUri\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 06:57:37,489 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-30-811/function\n",
      "2025-05-21 06:57:37,594 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-30-811/arguments\n",
      "2025-05-21 06:57:37,698 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpqgj0nkzu/requirements.txt'\n",
      "2025-05-21 06:57:37,731 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-30-811/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-05-21 06:57:38,343 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-38-343/function\n",
      "2025-05-21 06:57:38,411 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-38-343/arguments\n",
      "2025-05-21 06:57:38,626 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpcn9dcrt9/requirements.txt'\n",
      "2025-05-21 06:57:38,660 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/DataPreprocessing/2025-05-21-06-57-38-343/pre_exec_script_and_dependencies'\n",
      "2025-05-21 06:57:38,681 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmppaa_ul3v/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-05-21 06:57:38,695 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmppaa_ul3v/workspace.zip'\n",
      "2025-05-21 06:57:38,734 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/sm_rf_user_ws/2025-05-21-06-57-38-343/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-05-21 06:57:38,742 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-38-343/function\n",
      "2025-05-21 06:57:38,808 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-38-343/arguments\n",
      "2025-05-21 06:57:38,892 sagemaker.remote_function INFO     Copied dependencies file at './requirements.txt' to '/tmp/tmpqfzj4m9x/requirements.txt'\n",
      "2025-05-21 06:57:38,929 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-891377369387/deepseek-finetune-pipeline/ModelFineTuning/2025-05-21-06-57-38-343/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:891377369387:pipeline/deepseek-finetune-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '51b2b2b0-fc3b-4c9c-84b2-2e82d10af4c8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '51b2b2b0-fc3b-4c9c-84b2-2e82d10af4c8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '94',\n",
       "   'date': 'Wed, 21 May 2025 06:57:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution1 = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
