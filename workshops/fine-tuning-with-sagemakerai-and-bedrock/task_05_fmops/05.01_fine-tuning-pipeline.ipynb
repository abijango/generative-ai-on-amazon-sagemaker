{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running hundreds of experiments, comparing the results, and keeping a track of the ML lifecycle can become very complex. This is where MLflow can help streamline the ML lifecycle, from data preparation to model deployment. By integrating MLflow into your LLM workflow, you can efficiently manage experiment tracking, model versioning, and deployment, providing reproducibility. With MLflow, you can track and compare the performance of multiple LLM experiments, identify the best-performing models, and deploy them to production environments with confidence. \n",
    "\n",
    "You can create workflows with SageMaker Pipelines that enable you to prepare data, fine-tune models, and evaluate model performance with simple Python code for each step. \n",
    "\n",
    "Now you can use SageMaker managed MLflow to run LLM fine-tuning and evaluation experiments at scale. Specifically:\n",
    "\n",
    "- MLflow can manage tracking of fine-tuning experiments, comparing evaluation results of different runs, model versioning, deployment, and configuration (such as data and hyperparameters)\n",
    "- SageMaker Pipelines can orchestrate multiple experiments based on the experiment configuration \n",
    "  \n",
    "\n",
    "The following figure shows the overview of the solution.\n",
    "![](./ml-16670-arch-with-mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- [HuggingFace access token](https://huggingface.co/docs/hub/en/security-tokens) â€“ You need a HuggingFace login token to access the DeepSeek-R1-Distill-Llama-8B model and datasets used in this post.\n",
    "\n",
    "- The notebook will download the DeepSeek-R1-Distill-Llama-8B model from HuggingFace and upload it to your S3 bucket for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Restart the kernel after executing below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, which will be used to define the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.steps import CacheConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.m5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. **Update the ARN for the MLflow tracking server.**\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location.\n",
    "\n",
    "experiment_name: give appropriate name for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try:\n",
    "    response = boto3.client('sagemaker').describe_mlflow_tracking_server(\n",
    "        TrackingServerName='genai-mlflow-tracker'\n",
    "    )\n",
    "    mlflow_tracking_server_uri = response['TrackingServerArn']\n",
    "except ClientError:\n",
    "    mlflow_tracking_server_uri = \"\"\n",
    "\n",
    "if mlflow_tracking_server_uri == \"\":\n",
    "    print(\"No MLflow Tracking Server Found, experiments will not be tracked.\")\n",
    "else:\n",
    "    print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a unique experiment name with timestamp\n",
    "\n",
    "pipeline_name = \"deepseek-finetune-pipeline\"\n",
    "experiment_base_name = \"deepseek-finetune-pipeline\"\n",
    "\n",
    "\n",
    "tracking_server_arn = mlflow_tracking_server_uri # Set the MLFlow ARN here\n",
    "# os.environ[\"mlflow_uri\"] = tracking_server_arn\n",
    "# os.environ[\"mlflow_experiment_name\"] = experiment_name\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        InstanceType: ml.m5.xlarge\n",
    "        Dependencies: ./scripts/requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns: # files or directories to ignore\n",
    "          - \"*.ipynb\" # all notebook files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model Data from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.s3 import S3Uploader\n",
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple function to check if file exists in S3\n",
    "def s3_file_exists(s3_client, bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "# Simple S3 upload function that checks if files exist before uploading\n",
    "def simple_s3_upload(local_dir, s3_bucket, s3_prefix, skip_existing=True):\n",
    "    \"\"\"\n",
    "    Upload files to S3, skipping files that already exist.\n",
    "    \n",
    "    Args:\n",
    "        local_dir (str): Local directory containing files to upload\n",
    "        s3_bucket (str): S3 bucket name\n",
    "        s3_prefix (str): S3 prefix (folder path)\n",
    "        skip_existing (bool): Whether to skip files that already exist in S3\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (uploaded_files, skipped_files, failed_files)\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    uploaded_files = []\n",
    "    skipped_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    # Get all local files\n",
    "    local_files = []\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            rel_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = os.path.join(s3_prefix, rel_path).replace('\\\\', '/')\n",
    "            local_files.append((local_path, s3_key))\n",
    "    \n",
    "    print(f\"Found {len(local_files)} files in {local_dir}\")\n",
    "    \n",
    "    # Process each file sequentially\n",
    "    for local_path, s3_key in local_files:\n",
    "        try:\n",
    "            # Check if file exists in S3\n",
    "            if skip_existing and s3_file_exists(s3_client, s3_bucket, s3_key):\n",
    "                print(f\"Skipping {s3_key} (file exists in S3)\")\n",
    "                skipped_files.append(s3_key)\n",
    "                continue\n",
    "            \n",
    "            # Upload the file\n",
    "            print(f\"Uploading {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "            s3_client.upload_file(\n",
    "                local_path, \n",
    "                s3_bucket, \n",
    "                s3_key,\n",
    "                ExtraArgs={'ACL': 'bucket-owner-full-control'}\n",
    "            )\n",
    "            uploaded_files.append(s3_key)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {local_path}: {str(e)}\")\n",
    "            failed_files.append((s3_key, str(e)))\n",
    "    \n",
    "    print(f\"\\nUpload Summary:\")\n",
    "    print(f\"  - Uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"  - Skipped: {len(skipped_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    return uploaded_files, skipped_files, failed_files\n",
    "\n",
    "# Set local and S3 model paths\n",
    "model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "if default_prefix:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"/{default_prefix}/models/{model_id_filesafe}\"\n",
    "else:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"/models/{model_id_filesafe}\"\n",
    "\n",
    "print(\"Downloading model \", model_id)\n",
    "os.makedirs(model_local_location, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "    print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "    \n",
    "    print(f\"Beginning Model Upload to {model_s3_destination}...\")\n",
    "    \n",
    "    # Use the simple upload function without threads or batch processing\n",
    "    uploaded, skipped, failed = simple_s3_upload(\n",
    "        local_dir=model_local_location,\n",
    "        s3_bucket=bucket_name,\n",
    "        s3_prefix=\"\",\n",
    "        skip_existing=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"Model successfully uploaded to: \\n {model_s3_destination}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model download or upload: {e}\")\n",
    "    raise\n",
    "\n",
    "os.environ[\"model_location\"] = model_s3_destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Steps\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Step**\n",
    "\n",
    "This step handles data preparation. We are going to prepare data for training and evaluation. We will log this data in MLflow\n",
    "\n",
    "For the purpose of fine tuning and evaluation we are going to use `FreedomIntelligence/medical-o1-reasoning-SFT` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"DataPreprocessing\",\n",
    "    instance_type=instance_type,\n",
    "    display_name=\"Data Preprocessing\",\n",
    "    keep_alive_period_in_seconds=900\n",
    ")\n",
    "def preprocess(\n",
    "    tracking_server_arn: str,\n",
    "    input_path: str,\n",
    "    experiment_base_name: str,\n",
    "    run_id: str,\n",
    ") -> tuple:\n",
    "    import boto3\n",
    "    import shutil\n",
    "    import sagemaker\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from sagemaker.config import load_sagemaker_config\n",
    "    import mlflow\n",
    "    import traceback\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    experiment_name = f\"{experiment_base_name}-{timestamp}\"  # Unique experiment name for each run\n",
    "    \n",
    "    # Initialize return values with defaults to ensure pipeline failure doesn't cause hard errors\n",
    "    result_experiment_name = experiment_name\n",
    "    result_run_id = run_id\n",
    "    result_train_data_path = None\n",
    "    result_test_dataset_path = None\n",
    "    mlflow_run = None\n",
    "    mlflow_enabled = False\n",
    "    \n",
    "    # Check if MLflow tracking server ARN is valid\n",
    "    mlflow_enabled = (\n",
    "        tracking_server_arn is not None\n",
    "        and experiment_name is not None\n",
    "        and tracking_server_arn != \"\"\n",
    "        and experiment_name != \"\"\n",
    "    )\n",
    "    \n",
    "    if mlflow_enabled:\n",
    "        print(f\"MLflow tracking enabled. Using server: {tracking_server_arn}\")\n",
    "        print(f\"MLflow experiment: {experiment_name}\")\n",
    "        try:\n",
    "            mlflow.set_tracking_uri(tracking_server_arn)\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            mlflow.autolog(log_datasets=True)\n",
    "            mlflow_run = mlflow.start_run(run_name=f\"preprocess-{run_id}\")\n",
    "            active_run_id = mlflow_run.info.run_id\n",
    "            print(f\"Started MLflow run with ID: {active_run_id}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error initializing MLflow tracking: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(error_msg)\n",
    "            mlflow_enabled = False\n",
    "            mlflow_run = None\n",
    "            if mlflow_enabled:  # Only log if MLflow was supposed to be enabled but failed\n",
    "                try:\n",
    "                    mlflow.log_param(\"initialization_error\", error_msg)\n",
    "                except:\n",
    "                    pass\n",
    "    else:\n",
    "        print(\"MLflow tracking disabled or not configured properly.\")\n",
    "        mlflow_run = None\n",
    "    \n",
    "    # Preprocessing code - runs regardless of MLflow status\n",
    "    try:\n",
    "        # Initialize SageMaker and S3 clients\n",
    "        sagemaker_session = sagemaker.Session()\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        bucket_name = sagemaker_session.default_bucket()\n",
    "        default_prefix = sagemaker_session.default_bucket_prefix\n",
    "        configs = load_sagemaker_config()\n",
    "        \n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        # Load dataset with proper error handling\n",
    "        try:\n",
    "            dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\")\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_param(\"dataset_load_success\", True)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error loading dataset: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(error_msg)\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_param(\"dataset_load_success\", False)\n",
    "                mlflow.log_param(\"dataset_load_error\", error_msg)\n",
    "            raise RuntimeError(f\"Failed to load dataset: {str(e)}\")\n",
    "        \n",
    "        df = pd.DataFrame(dataset['train'])\n",
    "        df = df[:100]\n",
    "        \n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Split dataset\n",
    "        train, test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "        \n",
    "        print(\"Number of train elements: \", len(train))\n",
    "        print(\"Number of test elements: \", len(test))\n",
    "        \n",
    "        # Log dataset statistics if MLflow is enabled\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"dataset_source\", \"FreedomIntelligence/medical-o1-reasoning-SFT\")\n",
    "            mlflow.log_param(\"train_size\", len(train))\n",
    "            mlflow.log_param(\"test_size\", len(test))\n",
    "            mlflow.log_param(\"dataset_sample_size\", 100)  # Log that we're using a subset of 100 samples\n",
    "        \n",
    "        # Define prompt template\n",
    "        prompt_template = f\"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "        Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "        Write a response that appropriately completes the request.\n",
    "        Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "        <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "        {{question}}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        {{complex_cot}}\n",
    "        \n",
    "        {{answer}}\n",
    "        <|eot_id|>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Template dataset to add prompt to each sample\n",
    "        def template_dataset(sample):\n",
    "            try:\n",
    "                sample[\"text\"] = prompt_template.format(question=sample[\"Question\"],\n",
    "                                                        complex_cot=sample[\"Complex_CoT\"],\n",
    "                                                        answer=sample[\"Response\"])\n",
    "                return sample\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError in template_dataset: {str(e)}\")\n",
    "                # Provide default values for missing fields\n",
    "                missing_key = str(e).strip(\"'\")\n",
    "                if missing_key == \"Question\":\n",
    "                    sample[\"text\"] = prompt_template.format(\n",
    "                        question=\"[Missing question]\",\n",
    "                        complex_cot=sample.get(\"Complex_CoT\", \"[Missing CoT]\"),\n",
    "                        answer=sample.get(\"Response\", \"[Missing response]\")\n",
    "                    )\n",
    "                elif missing_key == \"Complex_CoT\":\n",
    "                    sample[\"text\"] = prompt_template.format(\n",
    "                        question=sample[\"Question\"],\n",
    "                        complex_cot=\"[Missing CoT]\",\n",
    "                        answer=sample.get(\"Response\", \"[Missing response]\")\n",
    "                    )\n",
    "                elif missing_key == \"Response\":\n",
    "                    sample[\"text\"] = prompt_template.format(\n",
    "                        question=sample[\"Question\"],\n",
    "                        complex_cot=sample.get(\"Complex_CoT\", \"[Missing CoT]\"),\n",
    "                        answer=\"[Missing response]\"\n",
    "                    )\n",
    "                return sample\n",
    "        \n",
    "        from datasets import Dataset, DatasetDict\n",
    "        from random import randint\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_pandas(train)\n",
    "        test_dataset = Dataset.from_pandas(test)\n",
    "        \n",
    "        dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "        \n",
    "        train_dataset = dataset[\"train\"].map(template_dataset, remove_columns=list(dataset[\"train\"].features))\n",
    "        \n",
    "        # Safely get a sample text, handling potential index errors\n",
    "        try:\n",
    "            sample_index = randint(0, len(train_dataset) - 1)\n",
    "            sample_text = train_dataset[sample_index][\"text\"]\n",
    "            print(f\"Sample text from index {sample_index}:\")\n",
    "            print(sample_text)\n",
    "        except (IndexError, KeyError) as e:\n",
    "            sample_text = \"Error retrieving sample text: \" + str(e)\n",
    "            print(sample_text)\n",
    "            \n",
    "        test_dataset = dataset[\"test\"].map(template_dataset, remove_columns=list(dataset[\"test\"].features))\n",
    "        \n",
    "        # Set paths\n",
    "        if default_prefix:\n",
    "            input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "        else:\n",
    "            input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "    \n",
    "        # Create directories with error handling\n",
    "        try:\n",
    "            os.makedirs(\"./data/train\", exist_ok=True)\n",
    "            os.makedirs(\"./data/test\", exist_ok=True)\n",
    "        except OSError as e:\n",
    "            error_msg = f\"Error creating directories: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_param(\"dir_creation_error\", error_msg)\n",
    "            # Continue with execution as we'll try to save files anyway\n",
    "        \n",
    "        # Save datasets locally with error handling\n",
    "        try:\n",
    "            train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "            test_dataset.to_json(\"./data/test/dataset.json\", orient=\"records\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error saving datasets locally: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(error_msg)\n",
    "            if mlflow_enabled:\n",
    "                mlflow.log_param(\"local_save_error\", error_msg)\n",
    "            raise RuntimeError(f\"Failed to save datasets locally: {str(e)}\")\n",
    "        \n",
    "        # Define S3 paths\n",
    "        train_data_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "        test_dataset_path = f\"s3://{bucket_name}/{input_path}/test/dataset.json\"\n",
    "        \n",
    "        # Store results for return\n",
    "        result_train_data_path = train_data_path\n",
    "        result_test_dataset_path = test_dataset_path\n",
    "        \n",
    "        # Log dataset paths if MLflow is enabled\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"train_data_path\", train_data_path)\n",
    "            mlflow.log_param(\"test_dataset_path\", test_dataset_path)\n",
    "        \n",
    "        # Upload files to S3 with retries\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(f\"Uploading train dataset to S3, attempt {attempt+1}/{max_retries}\")\n",
    "                s3_client.upload_file(\"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\")\n",
    "                print(f\"Uploading test dataset to S3, attempt {attempt+1}/{max_retries}\")\n",
    "                s3_client.upload_file(\"./data/test/dataset.json\", bucket_name, f\"{input_path}/test/dataset.json\")\n",
    "                print(\"S3 upload successful\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error in S3 upload (attempt {attempt+1}/{max_retries}): {str(e)}\"\n",
    "                print(error_msg)\n",
    "                if attempt == max_retries - 1:  # Last attempt failed\n",
    "                    if mlflow_enabled:\n",
    "                        mlflow.log_param(\"s3_upload_error\", error_msg)\n",
    "                    raise RuntimeError(f\"Failed to upload datasets to S3 after {max_retries} attempts: {str(e)}\")\n",
    "        \n",
    "        print(f\"Datasets uploaded to:\")\n",
    "        print(train_data_path)\n",
    "        print(test_dataset_path)\n",
    "        \n",
    "        # Log a sample of the dataset as an artifact if MLflow is enabled\n",
    "        if mlflow_enabled:\n",
    "            try:\n",
    "                with open(\"./data/sample.txt\", \"w\") as f:\n",
    "                    f.write(sample_text)\n",
    "                mlflow.log_artifact(\"./data/sample.txt\", \"dataset_samples\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error logging sample as artifact: {str(e)}\")\n",
    "        \n",
    "        # Clean up\n",
    "        try:\n",
    "            if os.path.exists(\"./data\"):\n",
    "                shutil.rmtree(\"./data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error cleaning up temporary files: {str(e)}\")\n",
    "\n",
    "        # End MLflow run if it was started\n",
    "        if mlflow_enabled and mlflow_run:\n",
    "            try:\n",
    "                mlflow.end_run()\n",
    "            except Exception as e:\n",
    "                print(f\"Error ending MLflow run: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Critical error in preprocessing: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        if mlflow_enabled:\n",
    "            try:\n",
    "                mlflow.log_param(\"critical_error\", error_msg)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        raise RuntimeError(f\"Preprocessing failed: {str(e)}\")\n",
    "        \n",
    "\n",
    "    return result_experiment_name, result_run_id, result_train_data_path, result_test_dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code.\n",
    "\n",
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique that reduces the number of trainable parameters by adding low-rank decomposition matrices to existing weights rather than updating all model weights. This significantly reduces memory requirements and training time while maintaining performance comparable to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_uri}\"                # The URI for the MLflow tracking server \n",
    "mlflow_experiment_name: \"${mlflow_experiment_name}\"  # Name of the MLflow experiment for organizing runs\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"              # Hugging Face model id, or S3 location of base model\n",
    "\n",
    "# SageMaker specific parameters \n",
    "output_dir: \"/opt/ml/model\"                # Path where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # Path where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # Path where FSx saves test dataset\n",
    "\n",
    "# Training parameters\n",
    "max_seq_length: 1500                       # Maximum sequence length for inputs (affects memory usage)\n",
    "                                           # Higher values allow for longer context but require more memory\n",
    "                                           # Range: 512-4096 depending on model architecture and hardware\n",
    "\n",
    "# LoRA parameters (Low-Rank Adaptation)\n",
    "lora_r: 8                                  # Rank of the LoRA update matrices\n",
    "                                           # Lower values (4-16) are more efficient, higher values (32-64) can improve quality\n",
    "                                           # Recommended range: 8-64 depending on task complexity\n",
    "lora_alpha: 16                             # Scaling factor for the LoRA update\n",
    "                                           # Generally set to 2x lora_r for good performance\n",
    "lora_dropout: 0.1                          # Dropout probability for LoRA layers\n",
    "                                           # Range: 0.0-0.5, helps prevent overfitting\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate: 2e-4                        # Learning rate for parameter updates\n",
    "                                           # Range: 1e-5 to 5e-4 for LoRA fine-tuning\n",
    "                                           # Too high: training instability, too low: slow convergence\n",
    "\n",
    "# Training loop parameters\n",
    "num_train_epochs: 1                        # Number of complete passes through the training dataset\n",
    "                                           # More epochs can improve performance but risk overfitting\n",
    "                                           # Range: 1-5 for LoRA fine-tuning\n",
    "per_device_train_batch_size: 2             # Number of samples per GPU during training\n",
    "                                           # Larger values improve training speed but require more memory\n",
    "                                           # Range: 1-8 for large models on common GPUs\n",
    "per_device_eval_batch_size: 1              # Number of samples per GPU during evaluation\n",
    "                                           # Can typically be larger than training batch size\n",
    "gradient_accumulation_steps: 2             # Accumulate gradients over multiple steps\n",
    "                                           # Effectively increases batch size by this factor\n",
    "                                           # Useful when limited by GPU memory\n",
    "\n",
    "# Memory optimization techniques\n",
    "gradient_checkpointing: true               # Reduces memory usage by recomputing activations during backward pass\n",
    "                                           # Trades computation for memory, ~20% slower but enables larger models/sequences\n",
    "fp16: true                                 # Use half-precision floating point (speeds up training, reduces memory)\n",
    "bf16: false                                # Use bfloat16 precision (better numerical stability than fp16)\n",
    "                                           # Also enables FlashAttention2 (requires Ampere/Hopper GPU+ eg:A10, A100, H100)\n",
    "tf32: false                                # Use TensorFloat-32 precision (NVIDIA Ampere+ GPUs only)\n",
    "\n",
    "#uncomment here for fsdp - start\n",
    "# fsdp: \"full_shard auto_wrap offload\"     # Fully Sharded Data Parallel training\n",
    "                                           # Splits model states across multiple GPUs\n",
    "# fsdp_config:                             # Configuration for FSDP\n",
    "#     backward_prefetch: \"backward_pre\"    # Prefetches parameters before backward pass\n",
    "#     cpu_ram_efficient_loading: true      # More memory-efficient parameter loading\n",
    "#     offload_params: true                 # Offloads parameters to CPU when not in use\n",
    "#     forward_prefetch: false              # Don't prefetch parameters for forward pass\n",
    "#     use_orig_params: true                # Use original parameter ordering\n",
    "#uncomment here for fsdp - end\n",
    "\n",
    "merge_weights: true                        # Merge adapter weights into the base model\n",
    "                                           # true: produces standalone model, false: keeps adapter separate\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning Step**\n",
    "\n",
    "This is where the actual model adaptation occurs. The step takes the preprocessed data and applies it to fine-tune the base LLM (in this case, a Deepseek model). It incorporates the LoRA technique for efficient adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelFineTuning\",\n",
    "    instance_type=instance_type,\n",
    "    display_name=\"Model Fine Tuning\",\n",
    "    keep_alive_period_in_seconds=900,\n",
    "    dependencies=\"./scripts/requirements.txt\"\n",
    ")\n",
    "def train(\n",
    "    tracking_server_arn: str,\n",
    "    train_dataset_s3_path: str,\n",
    "    test_dataset_s3_path: str,\n",
    "    train_config_s3_path: str,\n",
    "    experiment_name: str,\n",
    "    model_id: str,\n",
    "    run_id: str,\n",
    "):\n",
    "    import sagemaker\n",
    "    import boto3\n",
    "    import mlflow\n",
    "    import yaml\n",
    "    import json\n",
    "    import time\n",
    "    import datetime\n",
    "    import os\n",
    "    import traceback\n",
    "    import tempfile\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Initialize variables and tracking\n",
    "    start_time = time.time()\n",
    "    model_name = model_id.split(\"/\")[-1] if \"/\" in model_id else model_id\n",
    "    training_job_name = None\n",
    "    \n",
    "    # Initialize MLflow tracking\n",
    "    mlflow_enabled = (\n",
    "        tracking_server_arn is not None\n",
    "        and experiment_name is not None\n",
    "        and tracking_server_arn != \"\"\n",
    "        and experiment_name != \"\"\n",
    "    )\n",
    "    \n",
    "    if mlflow_enabled:\n",
    "        try:\n",
    "            print(f\"MLflow tracking enabled. Using server: {tracking_server_arn}\")\n",
    "            print(f\"MLflow experiment: {experiment_name}\")\n",
    "            mlflow.set_tracking_uri(tracking_server_arn)\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            \n",
    "            # Enable detailed tracking\n",
    "            mlflow.set_tag(\"component\", \"model_fine_tuning\")\n",
    "            mlflow.autolog(log_datasets=True, log_models=True, log_input_examples=True)\n",
    "            \n",
    "            # Start MLflow run with parent run_id if available\n",
    "            mlflow_run = mlflow.start_run(run_name=f\"finetuning-{run_id}\")\n",
    "            print(f\"Continuing MLflow run with ID: {run_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error initializing MLflow tracking: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(error_msg)\n",
    "            mlflow_enabled = False\n",
    "            mlflow_run = None\n",
    "    \n",
    "    try:\n",
    "        # Log basic parameters to MLflow\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"model_id\", model_id)\n",
    "            mlflow.log_param(\"train_dataset\", train_dataset_s3_path)\n",
    "            mlflow.log_param(\"test_dataset\", test_dataset_s3_path)\n",
    "            mlflow.log_param(\"training_start_time\", datetime.datetime.now().isoformat())\n",
    "            \n",
    "            # Download and parse the training config YAML to log hyperparameters\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
    "                s3_client = boto3.client(\"s3\")\n",
    "                \n",
    "                # Parse S3 path\n",
    "                config_parts = train_config_s3_path.replace(\"s3://\", \"\").split(\"/\", 1)\n",
    "                bucket = config_parts[0]\n",
    "                key = config_parts[1]\n",
    "                \n",
    "                # Download config file\n",
    "                try:\n",
    "                    s3_client.download_file(bucket, key, tmp.name)\n",
    "                    # Parse the YAML config\n",
    "                    with open(tmp.name, 'r') as f:\n",
    "                        config = yaml.safe_load(f)\n",
    "                    \n",
    "                    # Log all hyperparameters from config\n",
    "                    print(\"Logging hyperparameters to MLflow:\")\n",
    "                    for param_name, param_value in config.items():\n",
    "                        # Skip complex objects that can't be logged as parameters\n",
    "                        if isinstance(param_value, (str, int, float, bool)):\n",
    "                            print(f\"  {param_name}: {param_value}\")\n",
    "                            mlflow.log_param(param_name, param_value)\n",
    "                        elif param_name == \"fsdp_config\" and isinstance(param_value, dict):\n",
    "                            # Log nested config as JSON\n",
    "                            mlflow.log_param(\"fsdp_config_json\", json.dumps(param_value))\n",
    "                    \n",
    "                    # Log file as artifact for reference\n",
    "                    mlflow.log_artifact(tmp.name, \"training_config\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing config file: {e}\")\n",
    "                    \n",
    "                finally:\n",
    "                    # Clean up temp file\n",
    "                    if os.path.exists(tmp.name):\n",
    "                        os.remove(tmp.name)\n",
    "        \n",
    "        # Launch the training job\n",
    "        job_name = f\"deepseek-finetune-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "        from sagemaker.pytorch import PyTorch\n",
    "        sagemaker_session = sagemaker.Session()\n",
    "        \n",
    "        # Define metric definitions for more detailed CloudWatch metrics\n",
    "        metric_definitions = [\n",
    "            {'Name': 'loss', 'Regex': \"'loss':\\\\s*([0-9.]+)\"},\n",
    "            {'Name': 'epoch', 'Regex': \"'epoch':\\\\s*([0-9.]+)\"},\n",
    "            {'Name': 'train_loss', 'Regex': \"'train_loss':\\\\s*([0-9.]+)\"},\n",
    "            {'Name': 'lr', 'Regex': \"'learning_rate':\\\\s*([0-9.e-]+)\"},\n",
    "            {'Name': 'step', 'Regex': \"'step':\\\\s*([0-9.]+)\"},\n",
    "            {'Name': 'samples_per_second', 'Regex': \"'train_samples_per_second':\\\\s*([0-9.]+)\"},\n",
    "        ]\n",
    "        \n",
    "        # Log the metric definitions we're using\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"tracked_metrics\", [m['Name'] for m in metric_definitions])\n",
    "        \n",
    "        pytorch_estimator = PyTorch(\n",
    "            entry_point='train.py',\n",
    "            source_dir=\"./scripts\",\n",
    "            job_name=job_name,\n",
    "            base_job_name=job_name,\n",
    "            max_run=50000,\n",
    "            role=role,\n",
    "            framework_version=\"2.2.0\",\n",
    "            py_version=\"py310\",\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.p3.2xlarge\",\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            volume_size=50,\n",
    "            disable_output_compression=False,\n",
    "            keep_alive_period_in_seconds=1800,\n",
    "            distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "            hyperparameters={\n",
    "                \"config\": \"/opt/ml/input/data/config/args.yaml\"\n",
    "            },\n",
    "            metric_definitions=metric_definitions,\n",
    "            debugger_hook_config=False\n",
    "        )\n",
    "    \n",
    "        # Define a data input dictionary with our uploaded S3 URIs\n",
    "        data = {\n",
    "          'train': train_dataset_s3_path,\n",
    "          'test': test_dataset_s3_path,\n",
    "          'config': train_config_s3_path\n",
    "        }\n",
    "    \n",
    "        print(f\"Data for Training Run: {data}\")\n",
    "        \n",
    "        # Log training job information\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"job_name\", job_name)\n",
    "            mlflow.log_param(\"instance_type\", \"ml.p3.2xlarge\")\n",
    "        \n",
    "        # Start the training job\n",
    "        pytorch_estimator.fit(data, wait=True)\n",
    "    \n",
    "        # Get information about the completed training job\n",
    "        latest_run_job_name = pytorch_estimator.latest_training_job.job_name\n",
    "        print(f\"Latest Job Name: {latest_run_job_name}\")\n",
    "    \n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "        # Describe the training job\n",
    "        response = sagemaker_client.describe_training_job(TrainingJobName=latest_run_job_name)\n",
    "    \n",
    "        # Extract the model artifacts S3 path\n",
    "        model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "        # Extract the output path (this is the general output location)\n",
    "        output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "        \n",
    "        # Get training time metrics\n",
    "        training_start_time = response.get('TrainingStartTime')\n",
    "        training_end_time = response.get('TrainingEndTime')\n",
    "        billable_time = response.get('BillableTimeInSeconds', 0)\n",
    "        \n",
    "        # Calculate duration\n",
    "        total_training_time = 0\n",
    "        if training_start_time and training_end_time:\n",
    "            total_training_time = (training_end_time - training_start_time).total_seconds()\n",
    "        \n",
    "        # Log job results and metrics to MLflow\n",
    "        if mlflow_enabled:\n",
    "            # Log basic job info\n",
    "            mlflow.log_param(\"training_job_name\", latest_run_job_name)\n",
    "            mlflow.log_param(\"model_artifacts_path\", model_artifacts_s3_path)\n",
    "            mlflow.log_param(\"output_path\", output_path)\n",
    "            \n",
    "            # Log performance metrics\n",
    "            mlflow.log_metric(\"billable_time_seconds\", billable_time)\n",
    "            mlflow.log_metric(\"total_training_time_seconds\", total_training_time)\n",
    "            \n",
    "            # Log training job status\n",
    "            mlflow.log_param(\"training_job_status\", response.get('TrainingJobStatus'))\n",
    "            \n",
    "            # Log any secondary status\n",
    "            if 'SecondaryStatus' in response:\n",
    "                mlflow.log_param(\"secondary_status\", response.get('SecondaryStatus'))\n",
    "            \n",
    "            # Log any failure reason\n",
    "            if 'FailureReason' in response:\n",
    "                mlflow.log_param(\"failure_reason\", response.get('FailureReason'))\n",
    "                \n",
    "            # Get CloudWatch logs for the training job\n",
    "            logs_client = boto3.client('logs')\n",
    "            log_group = \"/aws/sagemaker/TrainingJobs\"\n",
    "            log_stream = latest_run_job_name\n",
    "            \n",
    "            try:\n",
    "                # Get the last 1000 log events\n",
    "                log_events = logs_client.get_log_events(\n",
    "                    logGroupName=log_group,\n",
    "                    logStreamName=log_stream,\n",
    "                    limit=1000\n",
    "                )\n",
    "                \n",
    "                # Extract and save logs\n",
    "                log_output = \"\\n\".join([event['message'] for event in log_events['events']])\n",
    "                \n",
    "                # Save logs to file and log as artifact\n",
    "                with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.txt') as tmp:\n",
    "                    tmp.write(log_output)\n",
    "                    log_file_path = tmp.name\n",
    "                \n",
    "                mlflow.log_artifact(log_file_path, \"training_logs\")\n",
    "                os.remove(log_file_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching training logs: {e}\")\n",
    "            \n",
    "            # Log total execution time of this step\n",
    "            step_duration = time.time() - start_time\n",
    "            mlflow.log_metric(\"step_execution_time_seconds\", step_duration)\n",
    "            \n",
    "            # Log model metadata\n",
    "            mlflow.set_tag(\"model_path\", model_artifacts_s3_path)\n",
    "            mlflow.set_tag(\"training_completed_at\", datetime.datetime.now().isoformat())\n",
    "    \n",
    "        print(f\"Model artifacts S3 path: {model_artifacts_s3_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in model fine-tuning: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        if mlflow_enabled:\n",
    "            try:\n",
    "                mlflow.log_param(\"fine_tuning_error\", error_msg)\n",
    "                mlflow.set_tag(\"training_status\", \"FAILED\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        raise RuntimeError(f\"Fine-tuning failed: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # End MLflow run if it was started\n",
    "        if mlflow_enabled and mlflow_run:\n",
    "            try:\n",
    "                mlflow.set_tag(\"step_completed\", True)\n",
    "                mlflow.end_run()\n",
    "            except Exception as e:\n",
    "                print(f\"Error ending MLflow run: {str(e)}\")\n",
    "\n",
    "    return experiment_name, run_id, model_artifacts_s3_path, output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Step\n",
    "This step deploys the model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelDeploy\",\n",
    "    instance_type=instance_type,\n",
    "    display_name=\"Model Deploy\",\n",
    "    keep_alive_period_in_seconds=900\n",
    ")\n",
    "def deploy(\n",
    "    model_artifacts_s3_path: str,\n",
    "    output_path: str,\n",
    "    experiment_name: str,\n",
    "    model_id: str,\n",
    "    run_id: str,\n",
    "):\n",
    "    import sagemaker\n",
    "    import boto3\n",
    "    from sagemaker import get_execution_role\n",
    "    from sagemaker import Model\n",
    "    from sagemaker.predictor import Predictor\n",
    "    import time\n",
    "    \n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    instance_count = 1\n",
    "    instance_type = \"ml.g5.2xlarge\"\n",
    "    health_check_timeout = 700\n",
    "    \n",
    "    # Get the name for the endpoint\n",
    "    endpoint_name = f\"{model_id.split('/')[-1].replace('.', '-').replace('_','-')}-sft-djl\"\n",
    "    \n",
    "    # Delete existing endpoint if it exists\n",
    "    print(f\"Checking for existing endpoint: {endpoint_name}\")\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Endpoint {endpoint_name} exists, deleting it before deployment\")\n",
    "        sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        \n",
    "        # Wait for endpoint to be fully deleted\n",
    "        print(\"Waiting for endpoint to be fully deleted...\")\n",
    "        wait_seconds = 10\n",
    "        total_wait_time = 0\n",
    "        max_wait_time = 300  # 5 minutes maximum wait\n",
    "        endpoint_deleted = False\n",
    "        \n",
    "        while total_wait_time < max_wait_time and not endpoint_deleted:\n",
    "            try:\n",
    "                sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "                print(f\"Endpoint still exists, waiting {wait_seconds} seconds...\")\n",
    "                time.sleep(wait_seconds)\n",
    "                total_wait_time += wait_seconds\n",
    "            except sm_client.exceptions.ClientError:\n",
    "                print(f\"Endpoint {endpoint_name} successfully deleted\")\n",
    "                endpoint_deleted = True\n",
    "                \n",
    "        if not endpoint_deleted:\n",
    "            print(f\"Warning: Endpoint still exists after {max_wait_time} seconds\")\n",
    "            \n",
    "    except sm_client.exceptions.ClientError:\n",
    "        print(f\"Endpoint {endpoint_name} does not exist, proceeding with deployment\")\n",
    "    \n",
    "    # Continue with model deployment\n",
    "    image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"djl-lmi\",\n",
    "        region=sagemaker_session.boto_session.region_name,\n",
    "        version=\"latest\"\n",
    "    )\n",
    "    \n",
    "    model_data = model_artifacts_s3_path\n",
    "    \n",
    "    # Create model only once\n",
    "    model = Model(\n",
    "        image_uri=image_uri,\n",
    "        model_data=model_data,\n",
    "        role=get_execution_role(),\n",
    "        env={\n",
    "            'HF_MODEL_ID': \"/opt/ml/model\", # path to where sagemaker stores the model\n",
    "            'OPTION_TRUST_REMOTE_CODE': 'true',\n",
    "            'OPTION_ROLLING_BATCH': \"vllm\",\n",
    "            'OPTION_DTYPE': 'bf16',\n",
    "            'OPTION_QUANTIZE': 'fp8',\n",
    "            'OPTION_TENSOR_PARALLEL_DEGREE': 'max',\n",
    "            'OPTION_MAX_ROLLING_BATCH_SIZE': '32',\n",
    "            'OPTION_MODEL_LOADING_TIMEOUT': '3600',\n",
    "            'OPTION_MAX_MODEL_LEN': '4096'\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"deploying endpoint: {endpoint_name}\")\n",
    "    \n",
    "    predictor = model.deploy(\n",
    "        endpoint_name=endpoint_name,\n",
    "        initial_instance_count=instance_count,\n",
    "        instance_type=instance_type,\n",
    "        container_startup_health_check_timeout=health_check_timeout,\n",
    "        model_data_download_timeout=3600\n",
    "    )\n",
    "    \n",
    "    return experiment_name, run_id, endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Step\n",
    "\n",
    "After fine-tuning, this step assesses the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step(\n",
    "    name=\"ModelEvaluation\",\n",
    "    instance_type=instance_type,\n",
    "    display_name=\"Model Evaluation\",\n",
    "    keep_alive_period_in_seconds=900,\n",
    "    dependencies=\"./eval/requirements.txt\"\n",
    ")\n",
    "def evaluate(\n",
    "    experiment_name: str,\n",
    "    run_id: str,\n",
    "    endpoint_name: str,\n",
    ")-> dict:\n",
    "    import os\n",
    "    import json\n",
    "    import time\n",
    "    import boto3\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm.notebook import tqdm\n",
    "    from datasets import load_dataset\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import transformers\n",
    "    import mlflow\n",
    "    import uuid\n",
    "    import traceback\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Import LightEval metrics\n",
    "    from lighteval.metrics.metrics_sample import ROUGE, Doc\n",
    "    \n",
    "    # Initialize MLflow tracking\n",
    "    tracking_server_arn = os.environ.get(\"mlflow_uri\", \"\")\n",
    "    mlflow_enabled = (tracking_server_arn != \"\" and experiment_name != \"\")\n",
    "    \n",
    "    if mlflow_enabled:\n",
    "        try:\n",
    "            print(f\"MLflow tracking enabled. Using server: {tracking_server_arn}\")\n",
    "            print(f\"MLflow experiment: {experiment_name}\")\n",
    "            mlflow.set_tracking_uri(tracking_server_arn)\n",
    "            mlflow.set_experiment(experiment_name)\n",
    "            mlflow.autolog(log_datasets=True)\n",
    "            \n",
    "            # Start MLflow run with parent run_id if available\n",
    "            mlflow_run = mlflow.start_run(run_name=f\"evaluation-{run_id}\")\n",
    "            print(f\"Continuing MLflow run with ID: {run_id}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing MLflow tracking: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "            mlflow_enabled = False\n",
    "            mlflow_run = None\n",
    "    \n",
    "    # Initialize the SageMaker client\n",
    "    sm_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    FINETUNED_MODEL_ENDPOINT = endpoint_name # Update with Fine-tuned model endpoint name\n",
    "    \n",
    "    # Define the model to evaluate\n",
    "    model_to_evaluate = {\n",
    "        \"name\": \"Fine-tuned DeepSeek-R1-Distill-Llama-8B\", \n",
    "        \"endpoint\": FINETUNED_MODEL_ENDPOINT\n",
    "    }\n",
    "    # Limit the number of samples to evaluate (for faster execution)\n",
    "    num_samples = 10\n",
    "    \n",
    "    # Log evaluation parameters to MLflow\n",
    "    if mlflow_enabled:\n",
    "        mlflow.log_param(\"evaluation_endpoint\", FINETUNED_MODEL_ENDPOINT)\n",
    "        mlflow.log_param(\"evaluation_num_samples\", num_samples)\n",
    "        mlflow.log_param(\"evaluation_timestamp\", datetime.now().isoformat())\n",
    "    \n",
    "    # Load the test split of the medical-o1 dataset\n",
    "    try:\n",
    "        dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
    "        \n",
    "        max_samples = len(dataset)\n",
    "        \n",
    "        dataset = dataset.shuffle().select(range(min(num_samples, max_samples)))\n",
    "        print(f\"Loaded medical-o1-reasoning dataset with {len(dataset)} samples out of {max_samples}\")\n",
    "        \n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"dataset_name\", \"FreedomIntelligence/medical-o1-reasoning-SFT\")\n",
    "            mlflow.log_param(\"dataset_actual_samples\", len(dataset))\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error loading dataset: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_param(\"dataset_load_error\", error_msg)\n",
    "        raise\n",
    "    \n",
    "    # Display a sample from the dataset\n",
    "    sample = dataset[0]\n",
    "    \n",
    "    print(\"\\nQuestion:\\n\", sample[\"Question\"], \"\\n\\n====\\n\")\n",
    "    print(\"Complex_CoT:\\n\", sample[\"Complex_CoT\"], \"\\n\\n====\\n\")\n",
    "    print(\"Response:\\n\", sample[\"Response\"], \"\\n\\n====\\n\")\n",
    "    \n",
    "    # This function allows you to interact with a deployed SageMaker endpoint to get predictions from the DeepSeek model\n",
    "    def invoke_sagemaker_endpoint(payload, endpoint_name):\n",
    "        \"\"\"\n",
    "        Invoke a SageMaker endpoint with the given payload.\n",
    "    \n",
    "        Args:\n",
    "            payload (dict): The input data to send to the endpoint\n",
    "            endpoint_name (str): The name of the SageMaker endpoint\n",
    "    \n",
    "        Returns:\n",
    "            dict: The response from the endpoint\n",
    "        \"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = sm_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                ContentType='application/json',\n",
    "                Body=json.dumps(payload)\n",
    "            )\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            response_body = response['Body'].read().decode('utf-8')\n",
    "            return json.loads(response_body), inference_time\n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking endpoint {endpoint_name}: {str(e)}\")\n",
    "            return None, -1\n",
    "    \n",
    "    # Initialize LightEval metrics calculators\n",
    "    rouge_metrics = ROUGE(\n",
    "        methods=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "        multiple_golds=False,\n",
    "        bootstrap=False,\n",
    "        normalize_gold=None,\n",
    "        normalize_pred=None\n",
    "    )\n",
    "    \n",
    "    def calculate_metrics(predictions, references):\n",
    "        \"\"\"\n",
    "        Calculate all evaluation metrics for summarization using LightEval.\n",
    "    \n",
    "        Args:\n",
    "            predictions (list): List of generated summaries\n",
    "            references (list): List of reference summaries\n",
    "    \n",
    "        Returns:\n",
    "            dict: Dictionary containing all metric scores\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "    \n",
    "        # Create Doc objects for the Rouge and BertScore metrics\n",
    "        docs = []\n",
    "        for reference in references:\n",
    "            docs.append(Doc(\n",
    "                {\"target\": reference},\n",
    "                choices=[reference],  # Dummy choices\n",
    "                gold_index=0  # Dummy gold_index\n",
    "            ))\n",
    "    \n",
    "        # Calculate ROUGE scores for each prediction-reference pair\n",
    "        rouge_scores = {\n",
    "            'rouge1_f': [], \n",
    "            'rouge2_f': [], \n",
    "            'rougeL_f': [],\n",
    "            # Add precision and recall scores too\n",
    "            'rouge1_precision': [],\n",
    "            'rouge1_recall': [],\n",
    "            'rouge2_precision': [],\n",
    "            'rouge2_recall': [],\n",
    "            'rougeL_precision': [],\n",
    "            'rougeL_recall': []\n",
    "        }\n",
    "    \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # For ROUGE calculation\n",
    "            rouge_result = rouge_metrics.compute(golds=[ref], predictions=[pred])\n",
    "            rouge_scores['rouge1_f'].append(rouge_result['rouge1'])\n",
    "            rouge_scores['rouge2_f'].append(rouge_result['rouge2'])\n",
    "            rouge_scores['rougeL_f'].append(rouge_result['rougeL'])\n",
    "            \n",
    "            # For more detailed ROUGE metrics (we get precision and recall too)\n",
    "            detailed_rouge = rouge_metrics.compute_detailed(golds=[ref], predictions=[pred])\n",
    "            rouge_scores['rouge1_precision'].append(detailed_rouge[0]['rouge1_precision'])\n",
    "            rouge_scores['rouge1_recall'].append(detailed_rouge[0]['rouge1_recall'])\n",
    "            rouge_scores['rouge2_precision'].append(detailed_rouge[0]['rouge2_precision'])\n",
    "            rouge_scores['rouge2_recall'].append(detailed_rouge[0]['rouge2_recall'])\n",
    "            rouge_scores['rougeL_precision'].append(detailed_rouge[0]['rougeL_precision'])\n",
    "            rouge_scores['rougeL_recall'].append(detailed_rouge[0]['rougeL_recall'])\n",
    "    \n",
    "        # Average ROUGE scores\n",
    "        for key in rouge_scores:\n",
    "            metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "        \n",
    "        # Calculate prediction statistics\n",
    "        metrics['avg_prediction_length'] = np.mean([len(pred.split()) for pred in predictions])\n",
    "        metrics['min_prediction_length'] = min([len(pred.split()) for pred in predictions])\n",
    "        metrics['max_prediction_length'] = max([len(pred.split()) for pred in predictions])\n",
    "        \n",
    "        # Calculate reference statistics\n",
    "        metrics['avg_reference_length'] = np.mean([len(ref.split()) for ref in references])\n",
    "        metrics['min_reference_length'] = min([len(ref.split()) for ref in references])\n",
    "        metrics['max_reference_length'] = max([len(ref.split()) for ref in references])\n",
    "        \n",
    "        # Calculate length ratio\n",
    "        metrics['avg_length_ratio'] = np.mean([len(pred.split()) / len(ref.split()) if len(ref.split()) > 0 else 0 \n",
    "                                              for pred, ref in zip(predictions, references)])\n",
    "    \n",
    "        print(f\"Metrics: {metrics}\")\n",
    "    \n",
    "        return metrics\n",
    "    \n",
    "    def generate_summaries_with_model(endpoint_name, dataset):\n",
    "        \"\"\"\n",
    "        Generate summaries using a model deployed on SageMaker.\n",
    "    \n",
    "        Args:\n",
    "            endpoint_name (str): SageMaker endpoint name\n",
    "            dataset: Dataset containing dialogues\n",
    "    \n",
    "        Returns:\n",
    "            list: Generated summaries\n",
    "            list: Inference times for each summary\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        inference_times = []\n",
    "        failed_generations = 0\n",
    "    \n",
    "        for example in tqdm(dataset, desc=\"Generating Responses\"):\n",
    "            question = example[\"Question\"]\n",
    "    \n",
    "            # Prepare the prompt for the model\n",
    "            prompt = f\"\"\"\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>system<|end_header_id|>\n",
    "            You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "            Write a response that appropriately completes the request.\n",
    "            Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "            <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "            {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "            # Payload for SageMaker endpoint\n",
    "            payload = {\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": 512,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"return_full_text\": False\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            # Call the model endpoint\n",
    "            try:\n",
    "                response, inference_time = invoke_sagemaker_endpoint(payload, endpoint_name)\n",
    "                \n",
    "                # Extract the generated text\n",
    "                if response is None:\n",
    "                    prediction = \"Error generating response.\"\n",
    "                    failed_generations += 1\n",
    "                elif isinstance(response, list):\n",
    "                    prediction = response[0].get('generated_text', '').strip()\n",
    "                elif isinstance(response, dict):\n",
    "                    prediction = response.get('generated_text', '').strip()\n",
    "                else:\n",
    "                    prediction = str(response).strip()\n",
    "    \n",
    "                prediction = prediction.split(\"<|eot_id|>\")[0] if \"<|eot_id|>\" in prediction else prediction\n",
    "                \n",
    "                # Log individual inference metrics\n",
    "                if mlflow_enabled:\n",
    "                    mlflow.log_metric(f\"inference_time_sample_{len(predictions)}\", inference_time)\n",
    "                \n",
    "                inference_times.append(inference_time)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "                prediction = \"Error generating response.\"\n",
    "                failed_generations += 1\n",
    "                inference_times.append(-1)\n",
    "    \n",
    "            predictions.append(prediction)\n",
    "    \n",
    "        # Log failure rate\n",
    "        if mlflow_enabled:\n",
    "            mlflow.log_metric(\"failed_generations\", failed_generations)\n",
    "            mlflow.log_metric(\"failure_rate\", failed_generations / len(dataset) if len(dataset) > 0 else 0)\n",
    "    \n",
    "        return predictions, inference_times\n",
    "    \n",
    "    def evaluate_model_on_dataset(model_config, dataset):\n",
    "        \"\"\"\n",
    "        Evaluate a fine-tuned model on a dataset using both automated and human metrics.\n",
    "    \n",
    "        Args:\n",
    "            model_config (dict): Model configuration with name and endpoint\n",
    "            dataset: dataset for evaluation\n",
    "    \n",
    "        Returns:\n",
    "            dict: Evaluation results\n",
    "        \"\"\"\n",
    "        model_name = model_config[\"name\"]\n",
    "        endpoint_name = model_config[\"endpoint\"]\n",
    "    \n",
    "        print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "        # Get references\n",
    "        references = [\"\\n\".join([example[\"Complex_CoT\"], example[\"Response\"]]) for example in dataset]\n",
    "    \n",
    "        # Generate summaries\n",
    "        print(\"\\nGenerating Responses...\")\n",
    "        predictions, inference_times = generate_summaries_with_model(endpoint_name, dataset)\n",
    "        \n",
    "        # Log inference time metrics\n",
    "        if mlflow_enabled:\n",
    "            valid_times = [t for t in inference_times if t > 0]\n",
    "            if valid_times:\n",
    "                mlflow.log_metric(\"avg_inference_time\", np.mean(valid_times))\n",
    "                mlflow.log_metric(\"min_inference_time\", min(valid_times))\n",
    "                mlflow.log_metric(\"max_inference_time\", max(valid_times))\n",
    "                mlflow.log_metric(\"p95_inference_time\", np.percentile(valid_times, 95))\n",
    "    \n",
    "        # Calculate automated metrics using LightEval\n",
    "        print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "        metrics = calculate_metrics(predictions, references)\n",
    "        \n",
    "        # Log all calculated metrics to MLflow\n",
    "        if mlflow_enabled:\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(metric_name, metric_value)\n",
    "            \n",
    "            # Create a comparison table of predictions vs references\n",
    "            comparison_data = []\n",
    "            for i, (pred, ref) in enumerate(zip(predictions[:5], references[:5])):\n",
    "                comparison_data.append({\n",
    "                    \"example_id\": i,\n",
    "                    \"prediction\": pred[:500] + (\"...\" if len(pred) > 500 else \"\"),  # Truncate for readability\n",
    "                    \"reference\": ref[:500] + (\"...\" if len(ref) > 500 else \"\"),     # Truncate for readability\n",
    "                    \"rouge1_f\": rouge_metrics.compute(golds=[ref], predictions=[pred])['rouge1']\n",
    "                })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            # Save comparison to a temporary CSV and log it as an artifact\n",
    "            temp_csv = f\"/tmp/predictions_comparison_{uuid.uuid4().hex[:8]}.csv\"\n",
    "            comparison_df.to_csv(temp_csv, index=False)\n",
    "            mlflow.log_artifact(temp_csv, \"model_predictions\")\n",
    "            \n",
    "        # Format results\n",
    "        results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"endpoint_name\": endpoint_name,\n",
    "            \"num_samples\": len(dataset),\n",
    "            \"metrics\": metrics,\n",
    "            \"predictions\": predictions[:5],  # First 5 predictions\n",
    "            \"references\": references[:5]     # First 5 references\n",
    "        }\n",
    "    \n",
    "        # Print key results\n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "        print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "        print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "        print(f\"Average Inference Time: {np.mean([t for t in inference_times if t > 0]):.3f} seconds\")\n",
    "    \n",
    "        return results, metrics['rouge1_f'], metrics['rouge2_f'], metrics['rougeL_f']\n",
    "    \n",
    "    try:\n",
    "        finetuned_model_results, rouge1_f, rouge2_f, rougeL_f = evaluate_model_on_dataset(model_to_evaluate, dataset)\n",
    "        print(f\"ROUGE-1 F1: {rouge1_f}\")\n",
    "        print(f\"ROUGE-2 F1: {rouge2_f}\")\n",
    "        print(f\"ROUGE-L F1: {rougeL_f}\")\n",
    "        \n",
    "        # Create and log visualizations if MLflow is enabled\n",
    "        if mlflow_enabled:\n",
    "            # Log model card with performance summary\n",
    "            model_card = f\"\"\"\n",
    "            # Model Evaluation Report\n",
    "            \n",
    "            ## Model Information\n",
    "            - **Model Name**: {model_to_evaluate[\"name\"]}\n",
    "            - **Endpoint**: {model_to_evaluate[\"endpoint\"]}\n",
    "            - **Evaluation Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "            - **Dataset**: FreedomIntelligence/medical-o1-reasoning-SFT\n",
    "            - **Samples Evaluated**: {len(dataset)}\n",
    "            \n",
    "            ## Performance Metrics\n",
    "            - **ROUGE-1 F1**: {rouge1_f:.4f}\n",
    "            - **ROUGE-2 F1**: {rouge2_f:.4f}\n",
    "            - **ROUGE-L F1**: {rougeL_f:.4f}\n",
    "            - **Average Inference Time**: {np.mean([t for t in finetuned_model_results[0][\"inference_times\"] if t > 0]):.3f} seconds\n",
    "            \n",
    "            ## Detailed Metrics\n",
    "            {json.dumps(finetuned_model_results[0][\"metrics\"], indent=2)}\n",
    "            \"\"\"\n",
    "            \n",
    "            with open(\"/tmp/model_card.md\", \"w\") as f:\n",
    "                f.write(model_card)\n",
    "            \n",
    "            mlflow.log_artifact(\"/tmp/model_card.md\", \"evaluation_summary\")\n",
    "            \n",
    "            # Create a simple bar chart for ROUGE metrics\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            metrics = finetuned_model_results[0][\"metrics\"]\n",
    "            rouge_metrics = {\n",
    "                'ROUGE-1 F1': metrics['rouge1_f'], \n",
    "                'ROUGE-2 F1': metrics['rouge2_f'], \n",
    "                'ROUGE-L F1': metrics['rougeL_f']\n",
    "            }\n",
    "            plt.bar(rouge_metrics.keys(), rouge_metrics.values())\n",
    "            plt.title('ROUGE Metrics')\n",
    "            plt.ylabel('Score')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            plt.savefig('/tmp/rouge_metrics.png')\n",
    "            mlflow.log_artifact('/tmp/rouge_metrics.png', \"evaluation_plots\")\n",
    "            \n",
    "        # End MLflow run if we started one\n",
    "        if mlflow_enabled and mlflow_run:\n",
    "            mlflow.end_run()\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in model evaluation: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_msg)\n",
    "        \n",
    "        if mlflow_enabled:\n",
    "            try:\n",
    "                mlflow.log_param(\"evaluation_error\", error_msg)\n",
    "                mlflow.end_run()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Return at least something even if evaluation fails\n",
    "        return {\"error\": str(e), \"rougeL_f\": 0.0}\n",
    "\n",
    "    return {\"rougeL_f\": rougeL_f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the steps of the pipeline\n",
    "preprocessing_step = preprocess(\n",
    "    tracking_server_arn=tracking_server_arn,\n",
    "    experiment_base_name=experiment_base_name,\n",
    "    run_id=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    input_path=input_path,\n",
    ")\n",
    "\n",
    "training_step = train(\n",
    "    tracking_server_arn=tracking_server_arn,\n",
    "    experiment_name=preprocessing_step[0],\n",
    "    run_id=preprocessing_step[1],\n",
    "    train_dataset_s3_path=preprocessing_step[2],\n",
    "    test_dataset_s3_path=preprocessing_step[3],\n",
    "    train_config_s3_path=train_config_s3_path,\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "\n",
    "deploy_step = deploy(\n",
    "    experiment_name=training_step[0],\n",
    "    run_id=training_step[1],\n",
    "    model_artifacts_s3_path=training_step[2],\n",
    "    output_path=training_step[3],\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "\n",
    "evaluate_step = evaluate(\n",
    "    experiment_name=deploy_step[0],\n",
    "    run_id=deploy_step[1],\n",
    "    endpoint_name=deploy_step[2],\n",
    ")\n",
    "\n",
    "# Combining the steps into the pipeline definition\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        instance_type,\n",
    "    ],\n",
    "    steps=[preprocessing_step, training_step, deploy_step, evaluate_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint to avoid incurring charges\n",
    "import boto3\n",
    "import time\n",
    "import botocore\n",
    "\n",
    "def delete_endpoint_with_retry(endpoint_name, max_retries=3, wait_seconds=10):\n",
    "    \"\"\"\n",
    "    Delete a SageMaker endpoint with retry logic\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint to delete\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        wait_seconds (int): Time to wait between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion was successful, False otherwise\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # First check if the endpoint exists\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        endpoint_exists = True\n",
    "    except sm_client.exceptions.ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            print(f\"Endpoint {endpoint_name} does not exist, no cleanup needed.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error checking endpoint existence: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # If we get here, the endpoint exists and we should delete it\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempting to delete endpoint {endpoint_name} (attempt {attempt + 1}/{max_retries})\")\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"Endpoint {endpoint_name} deletion initiated successfully\")\n",
    "            \n",
    "            # Wait for endpoint to be fully deleted\n",
    "            print(\"Waiting for endpoint to be fully deleted...\")\n",
    "            \n",
    "            # Poll until endpoint is deleted or max wait time is reached\n",
    "            total_wait_time = 0\n",
    "            max_wait_time = 300  # 5 minutes maximum wait\n",
    "            while total_wait_time < max_wait_time:\n",
    "                try:\n",
    "                    sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "                    print(f\"Endpoint still exists, waiting {wait_seconds} seconds...\")\n",
    "                    time.sleep(wait_seconds)\n",
    "                    total_wait_time += wait_seconds\n",
    "                except sm_client.exceptions.ClientError:\n",
    "                    print(f\"Endpoint {endpoint_name} successfully deleted\")\n",
    "                    return True\n",
    "            \n",
    "            # If we get here, the endpoint still exists after max_wait_time\n",
    "            print(f\"Warning: Endpoint deletion initiated but still exists after {max_wait_time} seconds\")\n",
    "            return False\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if \"ResourceInUse\" in str(e) or \"ResourceNotFound\" in str(e):\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "                print(f\"Retrying in {wait_seconds} seconds...\")\n",
    "                time.sleep(wait_seconds)\n",
    "            else:\n",
    "                print(f\"Unexpected error deleting endpoint: {e}\")\n",
    "                return False\n",
    "    \n",
    "    print(f\"Failed to delete endpoint {endpoint_name} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "# Clean up endpoint\n",
    "try:\n",
    "    model_name_safe = model_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "    endpoint_name = f\"{model_name_safe}-sft-djl\"\n",
    "    \n",
    "    print(f\"Cleaning up endpoint: {endpoint_name}\")\n",
    "    if delete_endpoint_with_retry(endpoint_name):\n",
    "        print(\"Cleanup completed successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Endpoint cleanup may have failed, please check the SageMaker console\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during endpoint cleanup: {str(e)}\")\n",
    "    print(\"You may need to manually delete the endpoint from the SageMaker console\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
