{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning and Evaluating LLMs with SageMaker Pipelines and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running hundreds of experiments, comparing the results, and keeping a track of the ML lifecycle can become very complex. This is where MLflow can help streamline the ML lifecycle, from data preparation to model deployment. By integrating MLflow into your LLM workflow, you can efficiently manage experiment tracking, model versioning, and deployment, providing reproducibility. With MLflow, you can track and compare the performance of multiple LLM experiments, identify the best-performing models, and deploy them to production environments with confidence. \n",
    "\n",
    "You can create workflows with SageMaker Pipelines that enable you to prepare data, fine-tune models, and evaluate model performance with simple Python code for each step. \n",
    "\n",
    "Now you can use SageMaker managed MLflow to run LLM fine-tuning and evaluation experiments at scale. Specifically:\n",
    "\n",
    "- MLflow can manage tracking of fine-tuning experiments, comparing evaluation results of different runs, model versioning, deployment, and configuration (such as data and hyperparameters)\n",
    "- SageMaker Pipelines can orchestrate multiple experiments based on the experiment configuration \n",
    "  \n",
    "\n",
    "The following figure shows the overview of the solution.\n",
    "![](./ml-16670-arch-with-mlflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites \n",
    "Before you begin, make sure you have the following prerequisites in place:\n",
    "\n",
    "- MLflow tracking server: If you're running this lab in a workshop environment, a MLflow tracking server has already been created for you. If you need to create a MLflow tracking server, follow the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Dependencies\n",
    "Restart the kernel after executing below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T16:13:00.445227Z",
     "iopub.status.busy": "2025-09-15T16:13:00.445009Z",
     "iopub.status.idle": "2025-09-15T16:13:03.807709Z",
     "shell.execute_reply": "2025-09-15T16:13:03.807149Z",
     "shell.execute_reply.started": "2025-09-15T16:13:00.445211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T16:13:03.808639Z",
     "iopub.status.busy": "2025-09-15T16:13:03.808465Z",
     "iopub.status.idle": "2025-09-15T16:13:03.812578Z",
     "shell.execute_reply": "2025-09-15T16:13:03.812157Z",
     "shell.execute_reply.started": "2025-09-15T16:13:03.808620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and Setting Up Environment**\n",
    "\n",
    "This part imports all necessary Python modules. It includes SageMaker-specific imports for pipeline creation and execution, which will be used to define the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:28.064470Z",
     "iopub.status.busy": "2025-09-16T14:35:28.064246Z",
     "iopub.status.idle": "2025-09-16T14:35:28.067404Z",
     "shell.execute_reply": "2025-09-16T14:35:28.066927Z",
     "shell.execute_reply.started": "2025-09-16T14:35:28.064454Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.function_step import step\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SageMaker Session and IAM Role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_execution_role()`: Retrieves the IAM role that SageMaker will use to access AWS resources. This role needs appropriate permissions for tasks like accessing S3 buckets and creating SageMaker resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:29.735000Z",
     "iopub.status.busy": "2025-09-16T14:35:29.734829Z",
     "iopub.status.idle": "2025-09-16T14:35:30.443973Z",
     "shell.execute_reply": "2025-09-16T14:35:30.443476Z",
     "shell.execute_reply.started": "2025-09-16T14:35:29.734987Z"
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "pipeline_name = \"AIM405-deepseek-finetune-pipeline\"\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "if default_prefix:\n",
    "    input_path = f'{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "else:\n",
    "    input_path = f'datasets/llm-fine-tuning-modeltrainer-sft'\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "model_id_filesafe = model_id.replace(\"/\",\"_\").replace(\".\", \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow integration is crucial for experiment tracking and management. **Update the ARN for the MLflow tracking server.**\n",
    "\n",
    "mlflow_arn: The ARN for the MLflow tracking server. You can get this ARN from SageMaker Studio UI. This allows the pipeline to log metrics, parameters, and artifacts to a central location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires a SageMaker with MLflow tracking server to track experiments and manage model artifacts. To create your own tracking server please refer to the [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html). Once you have created your tracking server, please copy the tracking server ARN to the `mlflow_tracking server_arn` variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:32.635785Z",
     "iopub.status.busy": "2025-09-16T14:35:32.635599Z",
     "iopub.status.idle": "2025-09-16T14:35:32.638818Z",
     "shell.execute_reply": "2025-09-16T14:35:32.638297Z",
     "shell.execute_reply.started": "2025-09-16T14:35:32.635770Z"
    }
   },
   "outputs": [],
   "source": [
    "mlflow_tracking_server_arn = \"arn:aws:sagemaker:us-east-1:329542461890:mlflow-tracking-server/my-tracking-server\"\n",
    "\n",
    "if not mlflow_tracking_server_arn:\n",
    "    try:\n",
    "        response = boto3.client('sagemaker').describe_mlflow_tracking_server(\n",
    "            TrackingServerName='genai-mlflow-tracker'\n",
    "        )\n",
    "        mlflow_tracking_server_arn = response['TrackingServerArn']\n",
    "        print(f\"MLflow Tracking Server ARN: {mlflow_tracking_server_arn}\")\n",
    "    except ClientError:\n",
    "        print(\"No MLflow Tracking Server Found, please input a value for mlflow_tracking_server_arn\")\n",
    "\n",
    "os.environ[\"mlflow_tracking_server_arn\"] = mlflow_tracking_server_arn\n",
    "os.environ[\"pipeline_name\"] = pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:33.270087Z",
     "iopub.status.busy": "2025-09-16T14:35:33.269889Z",
     "iopub.status.idle": "2025-09-16T14:35:33.273350Z",
     "shell.execute_reply": "2025-09-16T14:35:33.272883Z",
     "shell.execute_reply.started": "2025-09-16T14:35:33.270070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        InstanceType: ml.m5.xlarge\n",
    "        Dependencies: ./scripts/requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns: # files or directories to ignore\n",
    "          - \"*.ipynb\" # all notebook files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:33.824825Z",
     "iopub.status.busy": "2025-09-16T14:35:33.824662Z",
     "iopub.status.idle": "2025-09-16T14:35:33.827211Z",
     "shell.execute_reply": "2025-09-16T14:35:33.826755Z",
     "shell.execute_reply.started": "2025-09-16T14:35:33.824812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set path to config file\n",
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Download Model Data from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:34.997039Z",
     "iopub.status.busy": "2025-09-16T14:35:34.996877Z",
     "iopub.status.idle": "2025-09-16T14:35:35.727842Z",
     "shell.execute_reply": "2025-09-16T14:35:35.727329Z",
     "shell.execute_reply.started": "2025-09-16T14:35:34.997026Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model  deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38b47851b9e473e8420a76ca216b534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deepseek-ai/DeepSeek-R1-Distill-Llama-8B downloaded under ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n",
      "Beginning Model Upload to s3://sagemaker-us-east-1-329542461890/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B...\n",
      "Found 34 files in ../models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/generation_config.json (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model.safetensors.index.json (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/README.md (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config.json (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer_config.json (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.gitattributes (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/LICENSE (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/tokenizer.json (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00002-of-000002.safetensors (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/model-00001-of-000002.safetensors (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/.gitignore (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/.gitattributes.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model-00001-of-000002.safetensors.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/generation_config.json.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/LICENSE.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model-00002-of-000002.safetensors.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/README.md.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/generation_config.json.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model.safetensors.index.json.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/tokenizer.json.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/config.json.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model.safetensors.index.json.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/tokenizer_config.json.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/README.md.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/config.json.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/tokenizer_config.json.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/.gitattributes.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/LICENSE.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/tokenizer.json.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model-00002-of-000002.safetensors.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/model-00001-of-000002.safetensors.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/figures/benchmark.jpg.lock (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/.cache/huggingface/download/figures/benchmark.jpg.metadata (file exists in S3)\n",
      "Skipping models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/figures/benchmark.jpg (file exists in S3)\n",
      "\n",
      "Upload Summary:\n",
      "  - Uploaded: 0 files\n",
      "  - Skipped: 34 files\n",
      "  - Failed: 0 files\n",
      "Model successfully uploaded to: \n",
      " s3://sagemaker-us-east-1-329542461890/models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "# Simple function to check if file exists in S3\n",
    "def s3_file_exists(s3_client, bucket, key):\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "# Simple S3 upload function that checks if files exist before uploading\n",
    "def simple_s3_upload(local_dir, s3_bucket, s3_prefix, skip_existing=True):\n",
    "    \"\"\"\n",
    "    Upload files to S3, skipping files that already exist.\n",
    "    \n",
    "    Args:\n",
    "        local_dir (str): Local directory containing files to upload\n",
    "        s3_bucket (str): S3 bucket name\n",
    "        s3_prefix (str): S3 prefix (folder path)\n",
    "        skip_existing (bool): Whether to skip files that already exist in S3\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (uploaded_files, skipped_files, failed_files)\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    uploaded_files = []\n",
    "    skipped_files = []\n",
    "    failed_files = []\n",
    "    \n",
    "    # Get all local files\n",
    "    local_files = []\n",
    "    for root, _, files in os.walk(local_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            rel_path = os.path.relpath(local_path, local_dir)\n",
    "            s3_key = os.path.join(s3_prefix, rel_path).replace('\\\\', '/')\n",
    "            local_files.append((local_path, s3_key))\n",
    "    \n",
    "    print(f\"Found {len(local_files)} files in {local_dir}\")\n",
    "    \n",
    "    # Process each file sequentially\n",
    "    for local_path, s3_key in local_files:\n",
    "        try:\n",
    "            # Check if file exists in S3\n",
    "            if skip_existing and s3_file_exists(s3_client, s3_bucket, s3_key):\n",
    "                print(f\"Skipping {s3_key} (file exists in S3)\")\n",
    "                skipped_files.append(s3_key)\n",
    "                continue\n",
    "            \n",
    "            # Upload the file\n",
    "            print(f\"Uploading {local_path} to s3://{s3_bucket}/{s3_key}\")\n",
    "            s3_client.upload_file(\n",
    "                local_path, \n",
    "                s3_bucket, \n",
    "                s3_key,\n",
    "                ExtraArgs={'ACL': 'bucket-owner-full-control'}\n",
    "            )\n",
    "            uploaded_files.append(s3_key)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {local_path}: {str(e)}\")\n",
    "            failed_files.append((s3_key, str(e)))\n",
    "    \n",
    "    print(f\"\\nUpload Summary:\")\n",
    "    print(f\"  - Uploaded: {len(uploaded_files)} files\")\n",
    "    print(f\"  - Skipped: {len(skipped_files)} files\")\n",
    "    print(f\"  - Failed: {len(failed_files)} files\")\n",
    "    \n",
    "    return uploaded_files, skipped_files, failed_files\n",
    "\n",
    "# Set local and S3 model paths\n",
    "model_local_location = f\"../models/{model_id_filesafe}\"\n",
    "if default_prefix:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/{default_prefix}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"{default_prefix}/models/{model_id_filesafe}\"\n",
    "else:\n",
    "    model_s3_destination = f\"s3://{bucket_name}/models/{model_id_filesafe}\"\n",
    "    prefix = f\"models/{model_id_filesafe}\"\n",
    "\n",
    "print(\"Downloading model \", model_id)\n",
    "os.makedirs(model_local_location, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    snapshot_download(repo_id=model_id, local_dir=model_local_location)\n",
    "    print(f\"Model {model_id} downloaded under {model_local_location}\")\n",
    "    \n",
    "    print(f\"Beginning Model Upload to {model_s3_destination}...\")\n",
    "    \n",
    "    # Use the simple upload function without threads or batch processing\n",
    "    uploaded, skipped, failed = simple_s3_upload(\n",
    "        local_dir=model_local_location,\n",
    "        s3_bucket=bucket_name,\n",
    "        s3_prefix=prefix,\n",
    "        skip_existing=True\n",
    "    )\n",
    " \n",
    "    print(f\"Model successfully uploaded to: \\n {model_s3_destination}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during model download or upload: {e}\")\n",
    "    raise\n",
    "\n",
    "os.environ[\"model_location\"] = model_s3_destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Configure Fine-Tuning Job\n",
    "\n",
    "This section defines the core components of the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "The train_config dictionary is comprehensive, including:\n",
    "\n",
    "Experiment naming for tracking purposes\n",
    "Model specifications (ID, version, name)\n",
    "Infrastructure details (instance types and counts for fine-tuning and deployment)\n",
    "Training hyperparameters (epochs, batch size)\n",
    "\n",
    "This configuration allows for easy adjustment of the training process without changing the core pipeline code.\n",
    "\n",
    "**LoRA Parameters**\n",
    "\n",
    "Low-Rank Adaptation (LoRA) is an efficient fine-tuning technique that reduces the number of trainable parameters by adding low-rank decomposition matrices to existing weights rather than updating all model weights. This significantly reduces memory requirements and training time while maintaining performance comparable to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:49.097813Z",
     "iopub.status.busy": "2025-09-16T14:35:49.097613Z",
     "iopub.status.idle": "2025-09-16T14:35:49.114904Z",
     "shell.execute_reply": "2025-09-16T14:35:49.114482Z",
     "shell.execute_reply.started": "2025-09-16T14:35:49.097799Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "\n",
    "# MLflow Config\n",
    "mlflow_uri: \"${mlflow_tracking_server_arn}\"                # The URI for the MLflow tracking server \n",
    "mlflow_experiment_name: \"${pipeline_name}\"  # Name of the MLflow experiment for organizing runs\n",
    "\n",
    "\n",
    "model_id: \"${model_location}\"              # Hugging Face model id, or S3 location of base model\n",
    "\n",
    "# SageMaker specific parameters \n",
    "output_dir: \"/opt/ml/model\"                # Path where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # Path where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"     # Path where FSx saves test dataset\n",
    "\n",
    "# Training parameters\n",
    "max_seq_length: 1500                       # Maximum sequence length for inputs (affects memory usage)\n",
    "                                           # Higher values allow for longer context but require more memory\n",
    "                                           # Range: 512-4096 depending on model architecture and hardware\n",
    "\n",
    "# LoRA parameters (Low-Rank Adaptation)\n",
    "lora_r: 8                                  # Rank of the LoRA update matrices\n",
    "                                           # Lower values (4-16) are more efficient, higher values (32-64) can improve quality\n",
    "                                           # Recommended range: 8-64 depending on task complexity\n",
    "lora_alpha: 16                             # Scaling factor for the LoRA update\n",
    "                                           # Generally set to 2x lora_r for good performance\n",
    "lora_dropout: 0.1                          # Dropout probability for LoRA layers\n",
    "                                           # Range: 0.0-0.5, helps prevent overfitting\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate: 2e-4                        # Learning rate for parameter updates\n",
    "                                           # Range: 1e-5 to 5e-4 for LoRA fine-tuning\n",
    "                                           # Too high: training instability, too low: slow convergence\n",
    "\n",
    "# Training loop parameters\n",
    "num_train_epochs: 1                        # Number of complete passes through the training dataset\n",
    "                                           # More epochs can improve performance but risk overfitting\n",
    "                                           # Range: 1-5 for LoRA fine-tuning\n",
    "per_device_train_batch_size: 2             # Number of samples per GPU during training\n",
    "                                           # Larger values improve training speed but require more memory\n",
    "                                           # Range: 1-8 for large models on common GPUs\n",
    "per_device_eval_batch_size: 1              # Number of samples per GPU during evaluation\n",
    "                                           # Can typically be larger than training batch size\n",
    "gradient_accumulation_steps: 2             # Accumulate gradients over multiple steps\n",
    "                                           # Effectively increases batch size by this factor\n",
    "                                           # Useful when limited by GPU memory\n",
    "\n",
    "# Memory optimization techniques\n",
    "gradient_checkpointing: true               # Reduces memory usage by recomputing activations during backward pass\n",
    "                                           # Trades computation for memory, ~20% slower but enables larger models/sequences\n",
    "fp16: true                                 # Use half-precision floating point (speeds up training, reduces memory)\n",
    "bf16: false                                # Use bfloat16 precision (better numerical stability than fp16)\n",
    "                                           # Also enables FlashAttention2 (requires Ampere/Hopper GPU+ eg:A10, A100, H100)\n",
    "tf32: false                                # Use TensorFloat-32 precision (NVIDIA Ampere+ GPUs only)\n",
    "\n",
    "#uncomment here for fsdp - start\n",
    "# fsdp: \"full_shard auto_wrap offload\"     # Fully Sharded Data Parallel training\n",
    "                                           # Splits model states across multiple GPUs\n",
    "# fsdp_config:                             # Configuration for FSDP\n",
    "#     backward_prefetch: \"backward_pre\"    # Prefetches parameters before backward pass\n",
    "#     cpu_ram_efficient_loading: true      # More memory-efficient parameter loading\n",
    "#     offload_params: true                 # Offloads parameters to CPU when not in use\n",
    "#     forward_prefetch: false              # Don't prefetch parameters for forward pass\n",
    "#     use_orig_params: true                # Use original parameter ordering\n",
    "#uncomment here for fsdp - end\n",
    "\n",
    "merge_weights: true                        # Merge adapter weights into the base model\n",
    "                                           # true: produces standalone model, false: keeps adapter separate\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:49.478634Z",
     "iopub.status.busy": "2025-09-16T14:35:49.478441Z",
     "iopub.status.idle": "2025-09-16T14:35:49.809496Z",
     "shell.execute_reply": "2025-09-16T14:35:49.808966Z",
     "shell.execute_reply.started": "2025-09-16T14:35:49.478617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config uploaded to:\n",
      "s3://sagemaker-us-east-1-329542461890/training_config/deepseek-ai_DeepSeek-R1-Distill-Llama-8B/config/args.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/training_config/{model_id_filesafe}\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/training_config/{model_id_filesafe}\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Pipeline Creation and Execution\n",
    "\n",
    "This final section brings all the components together into an executable pipeline.\n",
    "\n",
    "**Creating the Pipeline**\n",
    "\n",
    "The pipeline object is created with all defined steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:50.241633Z",
     "iopub.status.busy": "2025-09-16T14:35:50.241441Z",
     "iopub.status.idle": "2025-09-16T14:35:50.499086Z",
     "shell.execute_reply": "2025-09-16T14:35:50.498627Z",
     "shell.execute_reply.started": "2025-09-16T14:35:50.241615Z"
    }
   },
   "outputs": [],
   "source": [
    "from steps import (\n",
    "    preprocess_step,\n",
    "    finetune_step,\n",
    "    deploy_step,\n",
    "    quantitative_eval_step,\n",
    "    qualitative_eval_step,\n",
    "    model_registration_step\n",
    ")\n",
    "from sagemaker.workflow.step_collections import StepCollection\n",
    "\n",
    "preprocessing_step = preprocess_step.preprocess(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_name=ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "    input_path=input_path\n",
    ")\n",
    "\n",
    "training_step = finetune_step.train(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=preprocessing_step[0],\n",
    "    train_dataset_s3_path=preprocessing_step[1],\n",
    "    test_dataset_s3_path=preprocessing_step[2],\n",
    "    train_config_s3_path=train_config_s3_path,\n",
    "    role=role,\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "run_id=training_step[0]\n",
    "model_artifacts_s3_path=training_step[2]\n",
    "output_path=training_step[3]\n",
    "\n",
    "deploy_step = deploy_step.deploy(\n",
    "    model_artifacts_s3_path=model_artifacts_s3_path,\n",
    "    output_path=output_path,\n",
    "    model_id=model_s3_destination,\n",
    ")\n",
    "endpoint_name=deploy_step\n",
    "\n",
    "quantitative_eval_step = quantitative_eval_step.quantitative_evaluate(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=run_id,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "qualitative_eval_step = qualitative_eval_step.qualitative_evaluate(\n",
    "    tracking_server_arn=mlflow_tracking_server_arn,\n",
    "    experiment_name=pipeline_name,\n",
    "    run_id=run_id,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "evaluation_gate = ConditionStep(\n",
    "    name=\"EvaluationGate\",\n",
    "    depends_on=[qualitative_eval_step],\n",
    "    conditions=[\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=quantitative_eval_step[\"rougeL_f\"],\n",
    "            right=0.2\n",
    "        ),\n",
    "        ConditionGreaterThanOrEqualTo(\n",
    "            left=qualitative_eval_step[\"avg_medical_accuracy\"],\n",
    "            right=3.0\n",
    "        )\n",
    "    ],\n",
    "    if_steps=[\n",
    "        model_registration_step.register_model(\n",
    "            tracking_server_arn=mlflow_tracking_server_arn,\n",
    "            experiment_name=pipeline_name,\n",
    "            run_id=run_id,  # Assuming training_step returns run_id as first output\n",
    "            model_artifacts_s3_path=model_artifacts_s3_path,  # Assuming training_step returns artifacts path as second output\n",
    "            model_id=model_id,\n",
    "            model_name=f\"Fine-Tuned-Medical-DeepSeek\",\n",
    "            endpoint_name=endpoint_name,\n",
    "            evaluation_score=quantitative_eval_step[\"rougeL_f\"],  # Get the evaluation score\n",
    "            pipeline_name=pipeline_name,\n",
    "            model_description=\"Fine-tuned medical LLM for clinical reasoning and diagnostics\"\n",
    "        )\n",
    "    ],\n",
    "    else_steps=[\n",
    "        FailStep(\n",
    "            name=\"EvaluationFailed\",\n",
    "            error_message=\"Model evaluation failed to meet quality thresholds.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Combining the steps into the pipeline definition\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        instance_type,\n",
    "    ],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        training_step,\n",
    "        deploy_step,\n",
    "        quantitative_eval_step,\n",
    "        evaluation_gate\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upserting the Pipeline**\n",
    "\n",
    "This step either creates a new pipeline in SageMaker or updates an existing one with the same name. It's a key part of the MLOps process, allowing for iterative refinement of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:35:51.438724Z",
     "iopub.status.busy": "2025-09-16T14:35:51.438526Z",
     "iopub.status.idle": "2025-09-16T14:36:03.033854Z",
     "shell.execute_reply": "2025-09-16T14:36:03.033402Z",
     "shell.execute_reply.started": "2025-09-16T14:35:51.438708Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:35:53,162 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:35:53,242 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:35:53,481 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpf1zzpu1n/requirements.txt'\n",
      "2025-09-16 14:35:53,510 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "2025-09-16 14:35:53,549 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmpmxw04fpm/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-09-16 14:35:53,569 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmpmxw04fpm/workspace.zip'\n",
      "2025-09-16 14:35:53,608 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/sm_rf_user_ws/2025-09-16-14-35-51-670/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:35:54,922 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:35:54,984 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:35:55,067 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpcib0a28y/requirements.txt'\n",
      "2025-09-16 14:35:55,100 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:35:56,387 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:35:56,466 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:35:56,524 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpg704q7r8/requirements.txt'\n",
      "2025-09-16 14:35:56,558 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:35:57,871 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:35:57,931 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:35:57,992 sagemaker.remote_function INFO     Copied dependencies file at './eval/requirements.txt' to '/tmp/tmpxkun2rpv/requirements.txt'\n",
      "2025-09-16 14:35:58,019 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.Dependencies\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:35:59,335 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:35:59,397 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:35:59,464 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmp1gnvr42f/requirements.txt'\n",
      "2025-09-16 14:35:59,495 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.IncludeLocalWorkDir\n",
      "sagemaker.config INFO - Applied value from config key = SageMaker.PythonSDK.Modules.RemoteFunction.CustomFileFilter.IgnoreNamePatterns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 14:36:00,788 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-35-51-670/function\n",
      "2025-09-16 14:36:00,860 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-35-51-670/arguments\n",
      "2025-09-16 14:36:00,950 sagemaker.remote_function INFO     Copied dependencies file at './eval/requirements.txt' to '/tmp/tmpor_54auq/requirements.txt'\n",
      "2025-09-16 14:36:00,975 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-35-51-670/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:01,391 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:01,453 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:01,683 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmprimt2lpj/requirements.txt'\n",
      "2025-09-16 14:36:01,717 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/DataPreprocessing/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "2025-09-16 14:36:01,746 sagemaker.remote_function INFO     Copied user workspace to '/tmp/tmptfj83y8i/temp_workspace/sagemaker_remote_function_workspace'\n",
      "2025-09-16 14:36:01,766 sagemaker.remote_function INFO     Successfully created workdir archive at '/tmp/tmptfj83y8i/workspace.zip'\n",
      "2025-09-16 14:36:01,816 sagemaker.remote_function INFO     Successfully uploaded workdir to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/sm_rf_user_ws/2025-09-16-14-36-01-391/workspace.zip'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:01,819 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:01,921 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:01,981 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpu0yq2tsp/requirements.txt'\n",
      "2025-09-16 14:36:02,010 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelFineTuning/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:02,011 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:02,082 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:02,142 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpxkiz79e3/requirements.txt'\n",
      "2025-09-16 14:36:02,178 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelDeploy/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:02,179 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:02,245 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:02,331 sagemaker.remote_function INFO     Copied dependencies file at './eval/requirements.txt' to '/tmp/tmpa0d6usij/requirements.txt'\n",
      "2025-09-16 14:36:02,356 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QuantitativeModelEvaluation/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:02,358 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:02,416 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:02,474 sagemaker.remote_function INFO     Copied dependencies file at './scripts/requirements.txt' to '/tmp/tmpi0adcfk6/requirements.txt'\n",
      "2025-09-16 14:36:02,501 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/ModelRegistration/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "2025-09-16 14:36:02,503 sagemaker.remote_function INFO     Uploading serialized function code to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-36-01-391/function\n",
      "2025-09-16 14:36:02,614 sagemaker.remote_function INFO     Uploading serialized function arguments to s3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-36-01-391/arguments\n",
      "2025-09-16 14:36:02,679 sagemaker.remote_function INFO     Copied dependencies file at './eval/requirements.txt' to '/tmp/tmpt0_qqprn/requirements.txt'\n",
      "2025-09-16 14:36:02,704 sagemaker.remote_function INFO     Successfully uploaded dependencies and pre execution scripts to 's3://sagemaker-us-east-1-329542461890/AIM405-deepseek-finetune-pipeline/QualitativeModelEvaluation/2025-09-16-14-36-01-391/pre_exec_script_and_dependencies'\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:329542461890:pipeline/AIM405-deepseek-finetune-pipeline',\n",
       " 'PipelineVersionId': 39,\n",
       " 'ResponseMetadata': {'RequestId': 'b481daae-11fd-4116-82d5-07329e5940b1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b481daae-11fd-4116-82d5-07329e5940b1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '124',\n",
       "   'date': 'Tue, 16 Sep 2025 14:36:03 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starting the Pipeline Execution**\n",
    "\n",
    "This command kicks off the actual execution of the pipeline in SageMaker. From this point, SageMaker will orchestrate the execution of each step, managing resources and data flow between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:36:11.406720Z",
     "iopub.status.busy": "2025-09-16T14:36:11.406482Z",
     "iopub.status.idle": "2025-09-16T14:36:11.594174Z",
     "shell.execute_reply": "2025-09-16T14:36:11.593622Z",
     "shell.execute_reply.started": "2025-09-16T14:36:11.406703Z"
    }
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint to avoid incurring charges\n",
    "import boto3\n",
    "import time\n",
    "import botocore\n",
    "\n",
    "def delete_endpoint_with_retry(endpoint_name, max_retries=3, wait_seconds=10):\n",
    "    \"\"\"\n",
    "    Delete a SageMaker endpoint with retry logic\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): Name of the SageMaker endpoint to delete\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "        wait_seconds (int): Time to wait between retries in seconds\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if deletion was successful, False otherwise\n",
    "    \"\"\"\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # First check if the endpoint exists\n",
    "    try:\n",
    "        sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        endpoint_exists = True\n",
    "    except sm_client.exceptions.ClientError as e:\n",
    "        if \"Could not find endpoint\" in str(e):\n",
    "            print(f\"Endpoint {endpoint_name} does not exist, no cleanup needed.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Error checking endpoint existence: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # If we get here, the endpoint exists and we should delete it\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Attempting to delete endpoint {endpoint_name} (attempt {attempt + 1}/{max_retries})\")\n",
    "            sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "            print(f\"Endpoint {endpoint_name} deletion initiated successfully\")\n",
    "            \n",
    "            # Wait for endpoint to be fully deleted\n",
    "            print(\"Waiting for endpoint to be fully deleted...\")\n",
    "            \n",
    "            # Poll until endpoint is deleted or max wait time is reached\n",
    "            total_wait_time = 0\n",
    "            max_wait_time = 300  # 5 minutes maximum wait\n",
    "            while total_wait_time < max_wait_time:\n",
    "                try:\n",
    "                    sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "                    print(f\"Endpoint still exists, waiting {wait_seconds} seconds...\")\n",
    "                    time.sleep(wait_seconds)\n",
    "                    total_wait_time += wait_seconds\n",
    "                except sm_client.exceptions.ClientError:\n",
    "                    print(f\"Endpoint {endpoint_name} successfully deleted\")\n",
    "                    return True\n",
    "            \n",
    "            # If we get here, the endpoint still exists after max_wait_time\n",
    "            print(f\"Warning: Endpoint deletion initiated but still exists after {max_wait_time} seconds\")\n",
    "            return False\n",
    "            \n",
    "        except botocore.exceptions.ClientError as e:\n",
    "            if \"ResourceInUse\" in str(e) or \"ResourceNotFound\" in str(e):\n",
    "                print(f\"Error deleting endpoint: {e}\")\n",
    "                print(f\"Retrying in {wait_seconds} seconds...\")\n",
    "                time.sleep(wait_seconds)\n",
    "            else:\n",
    "                print(f\"Unexpected error deleting endpoint: {e}\")\n",
    "                return False\n",
    "    \n",
    "    print(f\"Failed to delete endpoint {endpoint_name} after {max_retries} attempts\")\n",
    "    return False\n",
    "\n",
    "# Clean up endpoint\n",
    "try:\n",
    "    model_name_safe = model_id.split('/')[-1].replace('.', '-').replace('_', '-')\n",
    "    endpoint_name = f\"{model_name_safe}-sft-djl\"\n",
    "    \n",
    "    print(f\"Cleaning up endpoint: {endpoint_name}\")\n",
    "    if delete_endpoint_with_retry(endpoint_name):\n",
    "        print(\"Cleanup completed successfully\")\n",
    "    else:\n",
    "        print(\"Warning: Endpoint cleanup may have failed, please check the SageMaker console\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during endpoint cleanup: {str(e)}\")\n",
    "    print(\"You may need to manually delete the endpoint from the SageMaker console\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
