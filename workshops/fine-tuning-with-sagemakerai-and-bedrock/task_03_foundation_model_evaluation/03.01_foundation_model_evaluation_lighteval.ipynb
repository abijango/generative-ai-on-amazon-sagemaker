{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91a4f64-2568-4b90-8fb2-17f5ccde1c4d",
   "metadata": {},
   "source": [
    "# Comparing Model Performance after Fine-Tuning\n",
    "In this example, we will take the pre-existing SageMaker endpoints that you deployed in previous exercises and use them to generate data that can be leveraged for quality comparison. This data can be used to take a quantitative approach to judge the efficacy of fine-tuning your models.\n",
    "\n",
    "This example will run through samples of the medical-o1-reasoning dataset (FreedomIntelligence/medical-o1-reasoning-SFT) on the Hugging Face data hub for medical Q&A and use the [lighteval](https://huggingface.co/docs/lighteval/index) from Hugging Face for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d5ff2-dda1-450e-a098-976986747f62",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353628ef-4cf9-4957-85ee-1667ac4de611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae700e-fd5a-4f22-b45f-b50429a5d110",
   "metadata": {},
   "source": [
    "## This cell will restart the kernel. Click \"OK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27721bb8-d259-4f28-83aa-8847b5e4af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20c83d-2050-494c-acd6-0c9f575eb488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import LightEval metrics\n",
    "from lighteval.metrics.metrics_sample import ROUGE, Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1341fb-a37e-4f9f-9d3f-32233d58427f",
   "metadata": {},
   "source": [
    "#### Fetch the saved endpoint names from previous sections, or set them manually by uncommenting the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1557b-06dc-4b9b-8a4e-bd543f37a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BASE_ENDPOINT_NAME\n",
    "%store -r TUNED_ENDPOINT_NAME\n",
    "\n",
    "#BASE_ENDPOINT_NAME = \"\"\n",
    "#TUNED_ENDPOINT_NAME = \"\"\n",
    "\n",
    "print(f\"Base Endpoint: {BASE_ENDPOINT_NAME}\")\n",
    "print(f\"Tuned Endpoint: {TUNED_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e1176-f2af-4e7f-9273-48b2d67e22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model to evaluate\n",
    "model_to_evaluate = {\n",
    "    \"name\": \"Fine-tuned Model\", \n",
    "    \"endpoint\": TUNED_ENDPOINT_NAME\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de5205-0203-441b-b8d6-e2a8ef6c7fae",
   "metadata": {},
   "source": [
    "Here you will use the the medical-o1-reasoning dataset. The dataset is pre-split into training and test data. We will limit the number of samples to evaluate for the fine-tuned and base models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89eec7-cac2-4295-a0d6-495bb445d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the number of samples to evaluate (for faster execution)\n",
    "num_samples = 10\n",
    "\n",
    "# Load the test split of the medical-o1-reasoning dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train\")\n",
    "\n",
    "max_samples = len(dataset)\n",
    "\n",
    "dataset = dataset.shuffle().select(range(min(num_samples, max_samples)))\n",
    "print(f\"Loaded medical-o1-reasoning dataset with {len(dataset)} samples out of {max_samples}\")\n",
    "\n",
    "# Display a sample from the dataset\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"\\nQuestion:\\n\", sample[\"Question\"], \"\\n\\n====\\n\")\n",
    "print(\"Complex_CoT:\\n\", sample[\"Complex_CoT\"], \"\\n\\n====\\n\")\n",
    "print(\"Response:\\n\", sample[\"Response\"], \"\\n\\n====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dc243-5b54-454e-ab78-47455591a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\"\"\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def convert_to_messages(sample, system_prompt=\"\", include_answer=True):\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": sample[\"Question\"]},\n",
    "    ]\n",
    "\n",
    "    if include_answer:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{sample[\"Complex_CoT\"]}\\n\\n{sample[\"Response\"]}\"})\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a5e2c-39e9-4d51-a394-b666ffde44f2",
   "metadata": {},
   "source": [
    "#### Next, we will create functions to interact with the SageMaker endpoints, define metrics we want to calculate (ROUGE), and define how to evaluate the models with the medical-o1-reasoning dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd7d2a-ed91-45fa-8f1c-dfe42f9ebc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LightEval metrics calculators\n",
    "rouge_metrics = ROUGE(\n",
    "    methods=[\"rouge1\", \"rouge2\", \"rougeL\"],\n",
    "    multiple_golds=False,\n",
    "    bootstrap=False,\n",
    "    normalize_gold=None,\n",
    "    normalize_pred=None\n",
    ")\n",
    "\n",
    "def calculate_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calculate all evaluation metrics for summarization using LightEval.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of generated summaries\n",
    "        references (list): List of reference summaries\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all metric scores\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Create Doc objects for the Rouge and BertScore metrics\n",
    "    docs = []\n",
    "    for reference in references:\n",
    "        docs.append(Doc(\n",
    "            {\"target\": reference},\n",
    "            choices=[reference],  # Dummy choices\n",
    "            gold_index=0  # Dummy gold_index\n",
    "        ))\n",
    "    \n",
    "    # Calculate ROUGE scores for each prediction-reference pair\n",
    "    rouge_scores = {'rouge1_f': [], 'rouge2_f': [], 'rougeL_f': []}\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # For ROUGE calculation\n",
    "        rouge_result = rouge_metrics.compute(golds=[ref], predictions=[pred])\n",
    "        rouge_scores['rouge1_f'].append(rouge_result['rouge1'])\n",
    "        rouge_scores['rouge2_f'].append(rouge_result['rouge2'])\n",
    "        rouge_scores['rougeL_f'].append(rouge_result['rougeL'])\n",
    "    \n",
    "    # Average ROUGE scores\n",
    "    for key in rouge_scores:\n",
    "        metrics[key] = sum(rouge_scores[key]) / len(rouge_scores[key])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac55f4-463b-4a25-bbbd-529f76fbc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_with_model(predictor, dataset):\n",
    "    \"\"\"\n",
    "    Generate summaries using a model deployed on SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_name (str): SageMaker endpoint name\n",
    "        dataset: Dataset containing dialogues\n",
    "        \n",
    "    Returns:\n",
    "        list: Generated summaries\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Generating Responses\"):\n",
    "\n",
    "        messages = convert_to_messages(example, system_prompt=SYSTEM_PROMPT, include_answer=False)\n",
    "        \n",
    "        # Payload for SageMaker endpoint\n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 512,\n",
    "                \"top_p\": 0.9,\n",
    "                \"temperature\": 0.6,\n",
    "                \"return_full_text\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Call the model endpoint\n",
    "        try:\n",
    "            response = predictor.predict(payload)\n",
    "            \n",
    "            # Extract the generated text\n",
    "            if isinstance(response, list):\n",
    "                prediction = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            elif isinstance(response, dict):\n",
    "                prediction = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                prediction = str(response).strip\n",
    "\n",
    "\n",
    "            #prediction = prediction.split(\"<|eot_id|>\")[0]\n",
    "            # Clean up the generated text\n",
    "            #if \"Summary:\" in prediction:\n",
    "            #    prediction = prediction.split(\"Summary:\", 1)[1].strip()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error invoking SageMaker endpoint {endpoint_name}: {e}\")\n",
    "            prediction = \"Error generating summary.\"\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a872572-5d5d-4a0e-98f6-e656f26ba29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(model_config, dataset):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned model on the medical-o1-reasoning dataset using both automated and human metrics.\n",
    "    \n",
    "    Args:\n",
    "        model_config (dict): Model configuration with name and endpoint\n",
    "        dataset: medical-o1-reasoning dataset for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    model_name = model_config[\"name\"]\n",
    "    endpoint_name = model_config[\"endpoint\"]\n",
    "\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name,\n",
    "        sagemaker_session=sagemaker.Session(),\n",
    "        serializer=sagemaker.serializers.JSONSerializer(),\n",
    "        deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nEvaluating model: {model_name} on endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Get references\n",
    "    references = [\"\\n\".join([example[\"Complex_CoT\"], example[\"Response\"]]) for example in dataset]\n",
    "    \n",
    "    # Generate summaries\n",
    "    print(\"\\nGenerating Responses...\")\n",
    "    predictions = generate_summaries_with_model(predictor, dataset)\n",
    "    \n",
    "    # Calculate automated metrics using LightEval\n",
    "    print(\"\\nCalculating evaluation metrics with LightEval...\")\n",
    "    metrics = calculate_metrics(predictions, references)\n",
    "    \n",
    "    # Format results\n",
    "    results = {\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"num_samples\": len(dataset),\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": predictions[:5],  # First 5 predictions\n",
    "        \"references\": references[:5]     # First 5 references\n",
    "    }\n",
    "    \n",
    "    # Print key results\n",
    "    print(f\"\\nResults for {model_name}:\")\n",
    "    print(f\"ROUGE-1 F1: {metrics['rouge1_f']:.4f}\")\n",
    "    print(f\"ROUGE-2 F1: {metrics['rouge2_f']:.4f}\")\n",
    "    print(f\"ROUGE-L F1: {metrics['rougeL_f']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080aa020-0aaf-438d-b0cb-dd503d248feb",
   "metadata": {},
   "source": [
    "#### In this section, we evaluate the performance of both our base model (Qwen3-4B-Instruct-2507) and our fine-tuned model on the medical-o1-reasoning dataset using ROUGE metrics, which are standard for evaluating text summarization quality.\n",
    "\n",
    "The evaluation process:\n",
    "\n",
    "We first evaluate the base model against the medical-o1-reasoning test set to establish a baseline performance. Then, we evaluate our fine-tuned model on the same dataset to measure improvements. Both evaluations calculate ROUGE-1, ROUGE-2, and ROUGE-L scores, which respectively measure:\n",
    "\n",
    "ROUGE-1: Unigram overlap between generated and reference summaries\n",
    "ROUGE-2: Bigram overlap (captures more fluency and coherence)\n",
    "ROUGE-L: Longest common subsequence (measures sentence structure similarity)\n",
    "\n",
    "The results are saved to JSON files for later analysis and comparison. These metrics will help us quantify how much our fine-tuning process has improved the model's summarization capabilities compared to the original base model.\n",
    "\n",
    "**Note: Since the model you trained in this example was only exposed to a small amount of training data and the testing sample is small, you may see varied results. There are evaluation examples at the end of this section on a larger training and testing set for comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db13a7-81a3-4279-bde5-afbc75ca2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the base and fine-tuned models using LightEval metrics\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate both models for comparison\n",
    "base_model_config = {\n",
    "    \"name\": \"Base Model\",\n",
    "    \"endpoint\": BASE_ENDPOINT_NAME\n",
    "}\n",
    "\n",
    "# Evaluate base model\n",
    "base_model_results = evaluate_model_on_dataset(base_model_config, dataset)\n",
    "base_model_results[\"evaluation_time\"] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf903b-6605-4e22-a56e-f5c916e10e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing fine-tuned model\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "finetuned_model_results = evaluate_model_on_dataset(model_to_evaluate, dataset)\n",
    "finetuned_model_results[\"evaluation_time\"] = time.time() - start_time\n",
    "\n",
    "# Save results\n",
    "base_file_name = base_model_config[\"name\"].replace(' ', '_').lower()\n",
    "finetuned_file_name = model_to_evaluate[\"name\"].replace(' ', '_').lower()\n",
    "\n",
    "with open(f\"{base_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(base_model_results, f)\n",
    "    \n",
    "with open(f\"{finetuned_file_name}_results.json\", \"w\") as f:\n",
    "    json.dump(finetuned_model_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4944d-ad64-448d-91aa-e7222d7ebe8a",
   "metadata": {},
   "source": [
    "Create a tablular view to compare the base model and fine-tuned model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336396df-7175-4069-9794-b425501aee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add base model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": base_model_config[\"name\"],\n",
    "    \"ROUGE-1 F1\": base_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": base_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": base_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": base_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "# Add fine-tuned model metrics\n",
    "comparison_data.append({\n",
    "    \"Model\": model_to_evaluate[\"name\"],\n",
    "    \"ROUGE-1 F1\": finetuned_model_results[\"metrics\"][\"rouge1_f\"],\n",
    "    \"ROUGE-2 F1\": finetuned_model_results[\"metrics\"][\"rouge2_f\"],\n",
    "    \"ROUGE-L F1\": finetuned_model_results[\"metrics\"][\"rougeL_f\"],\n",
    "    \"Evaluation Time (s)\": finetuned_model_results[\"evaluation_time\"]\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07ee65-270a-41ce-bdff-620318c673f6",
   "metadata": {},
   "source": [
    "Show a bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fa953-173a-433b-ac83-7f75a45907eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROUGE and BERTScore metrics for both models\n",
    "metrics_to_plot = [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]\n",
    "models = comparison_df[\"Model\"].tolist()\n",
    "\n",
    "# Create a grouped bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(metrics_to_plot))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = [comparison_df.loc[i, metric] for metric in metrics_to_plot]\n",
    "    plt.bar(index + i*bar_width, values, bar_width, label=model)\n",
    "\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Summarization Performance Comparison')\n",
    "plt.xticks(index + bar_width/2, metrics_to_plot)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf665-2e81-4b17-8ce2-3b7aec06e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement from base to fine-tuned model\n",
    "improvement_data = {}\n",
    "\n",
    "for metric in [\"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"]:\n",
    "    base_value = comparison_df.loc[0, metric]\n",
    "    finetuned_value = comparison_df.loc[1, metric]\n",
    "    \n",
    "    if not pd.isna(base_value) and not pd.isna(finetuned_value):\n",
    "        abs_improvement = finetuned_value - base_value\n",
    "        pct_improvement = (abs_improvement / base_value) * 100 if base_value > 0 else float('inf')\n",
    "        \n",
    "        improvement_data[metric] = {\n",
    "            \"Base Model\": base_value,\n",
    "            \"Fine-tuned Model\": finetuned_value,\n",
    "            \"Absolute Improvement\": abs_improvement,\n",
    "            \"% Improvement\": pct_improvement\n",
    "        }\n",
    "\n",
    "# Create DataFrame for improvement metrics\n",
    "improvement_df = pd.DataFrame({\n",
    "    \"Metric\": list(improvement_data.keys()),\n",
    "    \"Base Score\": [improvement_data[m][\"Base Model\"] for m in improvement_data],\n",
    "    \"Fine-tuned Score\": [improvement_data[m][\"Fine-tuned Model\"] for m in improvement_data],\n",
    "    \"Absolute Improvement\": [improvement_data[m][\"Absolute Improvement\"] for m in improvement_data],\n",
    "    \"% Improvement\": [f\"{improvement_data[m]['% Improvement']:.2f}%\" for m in improvement_data]\n",
    "})\n",
    "\n",
    "print(\"Improvement Analysis:\")\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04e3d5-c865-45c1-93a4-54457b207db7",
   "metadata": {},
   "source": [
    "## Larger Training/Evaluation Results\n",
    "\n",
    "If you were to train **Qwen3-4B-Instruct-2507** on **5000** samples and evaluate on **100** test items (total training time 32 mins on an ml.g5.12xlarge instance), you would see the following results:\n",
    "\n",
    "![](./images/sft_5000_train_100_test_scores.png)\n",
    "\n",
    "![](images/sft_5000_train_100_test_bars.png)\n",
    "\n",
    "![](images/sft_5000_train_100_test_compare.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8213dea-cfac-42c2-acca-1e9bcb32dd86",
   "metadata": {},
   "source": [
    "## Detailed Comparison Between Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fd3c6-19a9-41c7-97e2-c03ab2023ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example predictions from both models\n",
    "num_examples = min(2, len(dataset))\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {dataset[i]['Question']}\")\n",
    "\n",
    "    ref_cot_answer = '\\n'.join([dataset[i]['Complex_CoT'],dataset[i]['Response']])\n",
    "    print(f\"\\nReference CoT+Answer: {ref_cot_answer}\")\n",
    "    \n",
    "    # Show predictions from both models\n",
    "    print(f\"\\nBase Model Summary: {base_model_results['predictions'][i]}\")\n",
    "    print(f\"\\nFine-tuned Model Summary: {finetuned_model_results['predictions'][i]}\")\n",
    "    \n",
    "    # Calculate ROUGE scores for this example using LightEval\n",
    "    base_rouge = rouge_metrics.compute(golds=[ref_cot_answer], predictions=[base_model_results['predictions'][i]])\n",
    "    finetuned_rouge = rouge_metrics.compute(golds=[ref_cot_answer], predictions=[finetuned_model_results['predictions'][i]])\n",
    "    \n",
    "    print(\"\\nROUGE Scores (LightEval):\")\n",
    "    print(f\"Base Model - ROUGE-1: {base_rouge['rouge1']:.4f}, ROUGE-2: {base_rouge['rouge2']:.4f}, ROUGE-L: {base_rouge['rougeL']:.4f}\")\n",
    "    print(f\"Fine-tuned - ROUGE-1: {finetuned_rouge['rouge1']:.4f}, ROUGE-2: {finetuned_rouge['rouge2']:.4f}, ROUGE-L: {finetuned_rouge['rougeL']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dd59d-2ac8-4260-9f38-b84dc06360eb",
   "metadata": {},
   "source": [
    "# Clean Up Endpoints\n",
    "\n",
    "Run the following code to clean up your base endpoint. It is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c0442-9a3d-4e2e-ba5c-23e2f3ab5953",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "delete_base_response = sagemaker_client.delete_endpoint(\n",
    "    EndpointName=BASE_ENDPOINT_NAME\n",
    ")\n",
    "\n",
    "print(delete_base_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadd677-aa8f-494e-9db6-df5d8c25eabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_basecfg_response = sagemaker_client.delete_endpoint_config(\n",
    "    EndpointConfigName=BASE_ENDPOINT_NAME\n",
    ")\n",
    "print(delete_basecfg_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc5ebf7-d70c-4537-808a-9478a3eb88c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
