{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfca55-c771-4e26-a13b-3451f6bef06a",
   "metadata": {},
   "source": [
    "# Applying Bedrock Guardrails to the DeepSeek R1 Distill Llama 8B Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41c9b6-e40e-4408-8209-faa9db60da8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "Guardrails can be used to implement safeguards for your generative AI applications that are customized to your use cases and aligned with your responsible AI policies. Guardrails allows you to:\n",
    "\n",
    "- Configure denied topics\n",
    "- Filter harmful content\n",
    "- Remove sensitive information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072fa5fb-589a-4d83-8083-ba926f327b4e",
   "metadata": {},
   "source": [
    "## The`ApplyGuardrail` API allows you to assess any text using pre-configured Bedrock Guardrails, without invoking the foundation models.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Content Validation**: Send any text input or output to the ApplyGuardrail API to have it evaluated against your defined topic avoidance rules, content filters, PII detectors, and word blocklists. You can evaluate user inputs and FM generated outputs independently.\n",
    "\n",
    "2. **Flexible Deployment**: Integrate the Guardrails API anywhere in your application flow to validate data before processing or serving results to users. E.g. For a RAG application, you can now evaluate the user input prior to performing the retrieval instead of waiting until the final response generation.\n",
    "\n",
    "3. **Decoupled from Foundation Models**: ApplyGuardrail is decoupled from foundational models. You can now use Guardrails without invoking Foundation Models.\n",
    "\n",
    "You can use the assessment results to design the experience on your generative AI application. Let's now walk through a code-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd569723-3c66-4569-82a0-a2538eb73921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Start by installing the dependencies to ensure we have a recent version\n",
    "%pip install -Uq boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c60726-0f0a-4d09-8d4f-4923f015f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa341fb6-dce9-4b79-a070-a98020870221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import Dict, Any\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bedrock_client = boto3.client('bedrock', region_name=sess.boto_region_name)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=sess.boto_region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3480e72-04ac-4caf-86bd-0fae2cb8fcc1",
   "metadata": {},
   "source": [
    "### Important: Create a Guardrail First\n",
    "\n",
    "Before running the code to apply a guardrail, you need to create a guardrail in Amazon Bedrock. We will create a guardrail that blocks input prompts and output responses from the model providing fiduciary advice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d82166-6725-4dab-96fa-f0ab2b1a0143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique client request token\n",
    "client_request_token = str(uuid.uuid4())\n",
    "\n",
    "# Create a Guardrail with specific filtering and compliance policies for medical use-case\n",
    "response = bedrock_client.create_guardrail(\n",
    "    name=\"MedicalContextGuardrails\",\n",
    "    description=\"Restrict responses to verified medical content only\",\n",
    "    blockedInputMessaging=\"Input Blocked: Sorry, I cannot provide non-medical advice, confirm/deny unverified medical information, or make any claims of guaranteed/definitive cures without sufficient evidence.\",\n",
    "    blockedOutputsMessaging=\"Output Blocked: Sorry, I cannot provide non-medical advice, confirm/deny unverified medical information, or make any claims of guaranteed/definitive cures without sufficient evidence.\",\n",
    "\n",
    "    # Topic-based restrictions (e.g., denying non-medical advice)\n",
    "    topicPolicyConfig={\n",
    "        'topicsConfig': [\n",
    "            {'name': 'non-medical-advice', 'definition': 'Any recommendations outside medical expertise or context', 'type': 'DENY'},\n",
    "            {'name': 'misinformation', 'definition': 'Dissemination of inaccurate or unverified medical information', 'type': 'DENY'},\n",
    "            {'name': 'medical-cure-claims', 'definition': 'Claims of guaranteed or definitive cures for medical conditions without sufficient evidence', 'type': 'DENY'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Content filtering policies (e.g., blocking harmful or unethical content)\n",
    "    contentPolicyConfig={\n",
    "        'filtersConfig': [\n",
    "            {'type': 'HATE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'INSULTS', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'SEXUAL', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'VIOLENCE', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'MISCONDUCT', 'inputStrength': 'HIGH', 'outputStrength': 'HIGH'},\n",
    "            {'type': 'PROMPT_ATTACK', 'inputStrength': 'HIGH', 'outputStrength': 'NONE'}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # List of restricted words related to sensitive medical topics\n",
    "    wordPolicyConfig={\n",
    "        # Example: blocking inappropriate usage of critical medical terms\n",
    "        'wordsConfig': [\n",
    "            {'text': \"malpractice\"}, {'text': \"misdiagnosis\"}, {'text': \"unauthorized treatment\"},\n",
    "            {'text': \"experimental drug\"}, {'text': \"unapproved therapy\"}, {'text': \"medical fraud\"},\n",
    "            {'text': \"cure\"}, {'text': \"guaranteed cure\"}, {'text': \"permanent remission\"}\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Sensitive data anonymization (e.g., patient information)\n",
    "    sensitiveInformationPolicyConfig={\n",
    "        # Anonymize identifiable patient information\n",
    "        'piiEntitiesConfig': [\n",
    "            {'type': \"NAME\", \"action\": \"ANONYMIZE\"}, {'type': \"EMAIL\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PHONE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_SOCIAL_SECURITY_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"ADDRESS\", \"action\": \"ANONYMIZE\"}, {'type': \"CA_HEALTH_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"PASSWORD\", \"action\": \"ANONYMIZE\"}, {'type': \"IP_ADDRESS\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"CA_SOCIAL_INSURANCE_NUMBER\", \"action\": \"ANONYMIZE\"}, {'type': \"CREDIT_DEBIT_CARD_NUMBER\", \"action\": \"ANONYMIZE\"},\n",
    "            {'type': \"AGE\", \"action\": \"ANONYMIZE\"}, {'type': \"US_BANK_ACCOUNT_NUMBER\", \"action\": \"ANONYMIZE\"}\n",
    "        ],\n",
    "        # Example regex patterns for anonymizing sensitive medical data\n",
    "        'regexesConfig': [\n",
    "            {\n",
    "                \"name\": \"medical_procedure_code\",\n",
    "                \"description\": \"Pattern for medical procedure codes\",\n",
    "                \"pattern\": \"\\\\b[A-Z]{1,5}\\\\d{1,5}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"clinical_trial_id\",\n",
    "                \"description\": \"Pattern for clinical trial identifiers\",\n",
    "                \"pattern\": \"\\\\bNCT\\\\d{8}\\\\b\",\n",
    "                \"action\": \"ANONYMIZE\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    # Tags for environment tracking\n",
    "    tags=[\n",
    "        {\"key\": \"Environment\", \"value\": \"Production\"},\n",
    "        {\"key\": \"Department\", \"value\": \"Medical\"}\n",
    "    ],\n",
    "    clientRequestToken=client_request_token,\n",
    ")\n",
    "\n",
    "# Retrieve and print the Guardrail ID, ARN, and version\n",
    "guardrail_id = response['guardrailId']\n",
    "\n",
    "print(f\"Guardrail ID: {guardrail_id}\")\n",
    "print(f\"Guardrail ARN: {response['guardrailArn']}\")\n",
    "print(f\"Version: {response['version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a845db-851d-405d-b6f5-819a214ffa52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# First create a published version\n",
    "version_response = bedrock_client.create_guardrail_version(\n",
    "    guardrailIdentifier=response['guardrailId'],\n",
    "    description=\"Production version 1.0\"\n",
    ")\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "guardrail_version=version_response['version']\n",
    "guardrail_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98f4b1-129f-4c1a-8ce9-15482a52c52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of Input Prompt being Analyzed\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Are there any cures for COVID-19?\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Here's an example of something that should pass\n",
    "\n",
    "#content = [\n",
    "    #{\n",
    "    #    \"text\": {\n",
    "   #         \"text\": \"What is the rate you offer for the AB503 Product?\"\n",
    "  #      }\n",
    " #   }\n",
    "#]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='INPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. \\nOutput:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189e5ab-4a38-417e-b766-e84efbea17b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# An Example of Analyzing an Output Response, This time using Contexual Grounding\n",
    "\n",
    "content = [\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Clinical Trial NCT12345678 was performed by Dr. Vivek Murthy of Gainesville, Florida in 2022. Throughout a series of double-blind trials he was able to show the effects of catnip improving the cognitive functions of senior cats by 20%, but only for a few minutes.\",\n",
    "            \"qualifiers\": [\"grounding_source\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, who led the clinical trial NCT12345678?\",\n",
    "            \"qualifiers\": [\"query\"],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"text\": {\n",
    "            \"text\": \"Dr. Vivek Murthy is based in Gainesville, Florida.\",\n",
    "            \"qualifiers\": [\"guard_content\"],\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "# Call the ApplyGuardrail API\n",
    "try:\n",
    "    response = bedrock_runtime.apply_guardrail(\n",
    "        guardrailIdentifier=guardrail_id,\n",
    "        guardrailVersion=guardrail_version,\n",
    "        source='OUTPUT',  # or 'INPUT' depending on your use case\n",
    "        content=content\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    print(\"API Response:\")\n",
    "    print(json.dumps(response, indent=2))\n",
    "    \n",
    "    # Check the action taken by the guardrail\n",
    "    if response['action'] == 'GUARDRAIL_INTERVENED':\n",
    "        print(\"\\nGuardrail intervened. Output:\")\n",
    "        for output in response['outputs']:\n",
    "            print(output['text'])\n",
    "    else:\n",
    "        print(\"\\nGuardrail did not intervene.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    print(\"\\nAPI Response (if available):\")\n",
    "    try:\n",
    "        print(json.dumps(response, indent=2))\n",
    "    except NameError:\n",
    "        print(\"No response available due to early exception.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225dd51-c1a3-4621-a902-f9700d8ac6b4",
   "metadata": {},
   "source": [
    "## Using ApplyGuardrail API with a Third-Party or Self-Hosted Model\n",
    "\n",
    "A common use case for the ApplyGuardrail API is in conjunction with a Language Model from a non Amazon Bedrock provider, or a model that you self-host. This combination allows you to apply guardrails to the input or output of any request.\n",
    "\n",
    "The general flow would be:\n",
    "1. Receive an input for your Model\n",
    "2. Apply the guardrail to this input using the ApplyGuardrail API\n",
    "3. If the input passes the guardrail, send it to your Model for Inference\n",
    "4. Receive the output from your Model\n",
    "5. Apply the Guardrail to your output\n",
    "6. Return the final (potentially modified) output\n",
    "\n",
    "### Here's a diagram illustrating this process:\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/applyguardrail.png\" alt=\"ApplyGuardrail API Flow\" style=\"max-width: 100%;\">\n",
    "</div>\n",
    "\n",
    "Let's walk through this with a code example that demonstrates this process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00340f-253e-451b-840e-ebd77165d740",
   "metadata": {},
   "source": [
    "### For our examples today we will use a Self-Hosted SageMaker Model, but this could be any third-party model as well\n",
    "\n",
    "We will use the `DeepSeek-R1-Distill-Llama-8B` model that we deployed earlier on a SageMaker Endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540f2797-24ec-4361-bd09-20d31efb5509",
   "metadata": {},
   "source": [
    "### Incorporating the ApplyGuardrail API into our Self-Hosted Model\n",
    "\n",
    "---\n",
    "We've created a `TextGenerationWithGuardrails` class that integrates the ApplyGuardrail API with our SageMaker endpoint to ensure protected text generation. This class includes the following key methods:\n",
    "\n",
    "1. `generate_text`: Calls our Language Model via a SageMaker endpoint to generate text based on the input.\n",
    "\n",
    "2. `analyze_text`: A core method that applies our guardrail using the ApplyGuardrail API. It int|erprets the API response to determine if the guardrail passed or intervened.\n",
    "\n",
    "3. `analyze_prompt` and `analyze_output`: These methods use `analyze_text` to apply our guardrail to the input prompt and generated output, respectively. They return a tuple indicating whether the guardrail passed and any associated message.\n",
    "\n",
    "The class looks to implement the diagram above. It works as follows:\n",
    "\n",
    "1. It first checks the input prompt using `analyze_prompt`.\n",
    "2. If the input passes the guardrail, it generates text using `generate_text`.\n",
    "3. The generated text is then checked using `analyze_output`.\n",
    "4. If both guardrails pass, the generated text is returned. Otherwise, an intervention message is provided.\n",
    "\n",
    "This structure allows for comprehensive safety checks both before and after text generation, with clear handling of cases where guardrails intervene. It's designed to easily integrate with larger applications while providing flexibility for error handling and customization based on guardrail results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96fdd1-8eff-4351-8063-4bd8e5883c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "class TextGenerationWithGuardrails:\n",
    "    def __init__(self, endpoint_name: str, guardrail_id: str, guardrail_version: str, sagemaker_session=None):\n",
    "        \"\"\"\n",
    "        Initialize the text generation class with guardrails.\n",
    "        \n",
    "        Args:\n",
    "            endpoint_name: The SageMaker endpoint name\n",
    "            model_id: The model ID (optional but useful for documentation)\n",
    "            guardrail_id: The AWS Bedrock guardrail ID\n",
    "            guardrail_version: The AWS Bedrock guardrail version\n",
    "            sagemaker_session: SageMaker session object\n",
    "        \"\"\"\n",
    "        # Create predictor directly instead of using retrieve_default\n",
    "        self.predictor = Predictor(\n",
    "            endpoint_name=endpoint_name,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer()\n",
    "        )\n",
    "        self.bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "        self.guardrail_id = guardrail_id\n",
    "        self.guardrail_version = guardrail_version\n",
    "\n",
    "    def generate_text(self, inputs: str, max_new_tokens: int = 256, temperature: float = 0.0) -> str:\n",
    "        \"\"\"Generate text using the specified SageMaker endpoint.\"\"\"\n",
    "        payload = {\n",
    "            \"inputs\": inputs,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": max_new_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"stop\": \"<|eot_id|>\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "        response = self.predictor.predict(payload)\n",
    "        return response.get('generated_text', '')\n",
    "\n",
    "    def analyze_text(self, grounding_source: str, query: str, guard_content: str, source: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Analyze text using the ApplyGuardrail API with contextual grounding.\n",
    "        Returns a tuple (passed, message, details) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - details is a dictionary containing the full API response for further analysis if needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = [\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": grounding_source,\n",
    "                        \"qualifiers\": [\"grounding_source\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": query,\n",
    "                        \"qualifiers\": [\"query\"]\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"text\": {\n",
    "                        \"text\": guard_content,\n",
    "                        \"qualifiers\": [\"guard_content\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "\n",
    "            response = self.bedrock_runtime.apply_guardrail(\n",
    "                guardrailIdentifier=self.guardrail_id,\n",
    "                guardrailVersion=self.guardrail_version,\n",
    "                source=source,\n",
    "                content=content\n",
    "            )\n",
    "            \n",
    "            action = response.get(\"action\", \"\")\n",
    "            if action == \"NONE\":\n",
    "                return True, \"\", response\n",
    "            elif action == \"GUARDRAIL_INTERVENED\":\n",
    "                message = response.get(\"outputs\", [{}])[-1].get(\"text\", \"Guardrail intervened\")\n",
    "                return False, message, response\n",
    "            else:\n",
    "                return False, f\"Unknown action: {action}\", response\n",
    "        except ClientError as e:\n",
    "            print(f\"Error applying guardrail: {e}\")\n",
    "            raise\n",
    "\n",
    "    def analyze_prompt(self, grounding_source: str, query: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the input prompt.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, query, \"INPUT\")\n",
    "\n",
    "    def analyze_output(self, grounding_source: str, query: str, generated_text: str) -> Tuple[bool, str, Dict[str, Any]]:\n",
    "        \"\"\"Analyze the generated output.\"\"\"\n",
    "        return self.analyze_text(grounding_source, query, generated_text, \"OUTPUT\")\n",
    "\n",
    "    def generate_and_analyze(self, grounding_source: str, query: str, max_new_tokens: int = 256, temperature: float = 0.0) -> Tuple[bool, str, str]:\n",
    "        \"\"\"\n",
    "        Generate text and analyze it with guardrails.\n",
    "        Returns a tuple (passed, message, generated_text) where:\n",
    "        - passed is a boolean indicating if the guardrail passed,\n",
    "        - message is either the guardrail message or an empty string,\n",
    "        - generated_text is the text generated by the model (if guardrail passed) or an empty string.\n",
    "        \"\"\"\n",
    "        # First, analyze the prompt\n",
    "        prompt_passed, prompt_message, _ = self.analyze_prompt(grounding_source, query)\n",
    "        if not prompt_passed:\n",
    "            return False, prompt_message, \"\"\n",
    "\n",
    "        # If prompt passes, generate text\n",
    "        generated_text = self.generate_text(query, max_new_tokens, temperature)\n",
    "\n",
    "        # Analyze the generated text\n",
    "        output_passed, output_message, _ = self.analyze_output(grounding_source, query, generated_text)\n",
    "        if not output_passed:\n",
    "            return False, output_message, \"\"\n",
    "\n",
    "        return True, \"\", generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ac6a0-293b-4ea4-b314-0aead2a5af75",
   "metadata": {},
   "source": [
    "### Now let's see a Sample Usage in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81517bf-45a3-495a-a5f6-eee5f94e0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bold text function\n",
    "def bold(text):\n",
    "    return f\"\\033[1m{text}\\033[0m\"\n",
    "\n",
    "def test_generation_with_guardrail(text_gen: TextGenerationWithGuardrails, query, grounding_source=\"\", max_new_tokens=512, temperature=0.0, print_api_responses=False):\n",
    "    # Analyze input\n",
    "    print(bold(\"\\n=== Input Analysis ===\\n\"))\n",
    "    input_passed, input_message, input_details = text_gen.analyze_prompt(grounding_source, query)\n",
    "    if not input_passed:\n",
    "        print(f\"Input Guardrail Intervened. The response to the User is: \\n\\n{input_message}\\n\")\n",
    "        if print_api_responses:\n",
    "            print(\"Full API Response:\")\n",
    "            print(json.dumps(input_details, indent=2))\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Input Prompt Passed The Guardrail Check - Moving to Generate the Response\\n\")\n",
    "    \n",
    "        # Generate text\n",
    "        print(bold(\"\\n=== Text Generation ===\\n\"))\n",
    "        generated_text = text_gen.generate_text(query, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        print(f\"Here is what the Model Responded with: \\n\\n{generated_text}\\n\")\n",
    "        \n",
    "        # Analyze output\n",
    "        print(bold(\"\\n=== Output Analysis ===\\n\"))\n",
    "        print(\"Analyzing Model Response with the Response Guardrail\\n\")\n",
    "        output_passed, output_message, output_details = text_gen.analyze_output(grounding_source, query, generated_text)\n",
    "        if not output_passed:\n",
    "            print(f\"Output Guardrail Intervened. The response to the User is: \\n\\n{output_message}\\n\")\n",
    "            if print_api_responses:\n",
    "                print(\"Full API Response:\")\n",
    "                print(json.dumps(output_details[\"outputs\"], indent=2))\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Model Response Passed. The information presented to the user is: \\n\\n{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c1e7b-dc61-4ea2-bdbf-519981d15f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"<<YOUR ENDPOINT NAME HERE>>\"\n",
    "\n",
    "assert endpoint_name != \"<<YOUR ENDPOINT NAME HERE>>\", \"Please enter your endpoint name here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a0c10-45ac-4353-a04a-dc05c4602577",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = TextGenerationWithGuardrails(\n",
    "    endpoint_name=endpoint_name,\n",
    "    guardrail_id=guardrail_id,\n",
    "    guardrail_version=guardrail_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf7135-51c5-48b9-b1cb-69611cdb46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_with_guardrail(\n",
    "    text_gen,\n",
    "    query=\"Is there a cure for COVID-19?\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0056b5a-2d18-4421-bd9c-8bf3859bbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_with_guardrail(\n",
    "    text_gen,\n",
    "    query=\"Can you provide the contact information, including the phone number and email address, for Dr. Vivek Murthy, who led the clinical trial NCT12345678?\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beb4f3b-ce07-4706-a1af-680197dd3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generation_with_guardrail(\n",
    "    text_gen,\n",
    "    query=\"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f33392-61ab-4fc5-9279-a62fbdc62a12",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61498705-28fa-485d-9371-244c9353b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client.delete_guardrail(guardrailIdentifier=guardrail_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b94020-3181-41eb-aab7-fd03628af406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
