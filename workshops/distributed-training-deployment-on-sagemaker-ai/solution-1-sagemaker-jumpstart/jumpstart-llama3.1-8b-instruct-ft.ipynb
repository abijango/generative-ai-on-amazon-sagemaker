{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fine-tune Meta Llama 3.1 - 8B instruct with SageMaker Jumpstart models\n",
    "\n",
    "In this notebook, we fine-tune Meta Llama 3.1 - 8B Instruct using SageMaker Jumpstart models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca016c-d4fa-4213-a7b3-03b449551449",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df060-2a39-44d5-ad98-df2865836d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U datasets==3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1c1f7-eec0-441b-88be-ad6ecd19352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9c879-adec-4f5d-a5f5-cce7f7597407",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "dataset_name = \"telco_promotions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c797ed3-72d0-430b-b5fa-44d72bd06833",
   "metadata": {},
   "source": [
    "Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810225a-6845-4e45-adbd-f8e0a71b4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(f\"./{dataset_name}.json\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ca80b-6362-41d2-83d3-e28c3f3eac11",
   "metadata": {},
   "source": [
    "Next, create a prompt template for using the data in an instruction format for the training job. You will also use this template during model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62d134-acc2-4c4c-b5a1-8e9567e0d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = {\n",
    "    \"prompt\": (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|>\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>{instruction}<|eot_id|>\"\n",
    "    ),\n",
    "    \"completion\": \"<|start_header_id|>assistant<|end_header_id|>{completion}<|eot_id|>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb38166-a892-4945-b7e3-e4df96d74115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df.to_json(\"train.jsonl\")\n",
    "\n",
    "# Write JSON template to local dir\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b07ea9-fbeb-4b46-a860-6d4d23dbcc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f11632-e9ef-4400-b814-fcfc28c8fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    default_path = f\"{bucket_name}/{default_prefix}/datasets/workshop-fine-tuning\"\n",
    "else:\n",
    "    default_path = f\"{bucket_name}/datasets/workshop-fine-tuning\"\n",
    "\n",
    "train_data_location = f\"s3://{default_path}/{dataset_name}\"\n",
    "\n",
    "S3Uploader.upload(\"train.jsonl\", train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "\n",
    "print(f\"Training data location: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b809ab6c-9f44-4e87-b087-8457854fa110",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model\n",
    "\n",
    "In this section you will fine-tune the model. Finetuning scripts are based on scripts provided by [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). To learn more about the fine-tuning scripts, please checkout section [5. Few notes about the fine-tuning method](#5-few-notes-about-the-fine-tuning-method). For a list of supported hyper-parameters and their default values, please see section [3. Supported Hyper-parameters for fine-tuning](#3-supported-hyper-parameters-for-fine-tuning). By default, these models train via domain adaptation, so you must indicate instruction tuning through the `instruction_tuned` hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc932f4-f514-4f14-a4ad-c84937c99160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b9f1d-63e8-4f53-b988-a44d89296756",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf802d6-213d-42af-b78e-277d89842088",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "model_id = \"meta-textgeneration-llama-3-1-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3400fa-35a5-462b-b05d-b7cf2b134612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    environment={\"accept_eula\": \"true\"},  # set \"accept_eula\": \"true\" to accept the EULA for gated models\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    disable_output_compression=False,\n",
    "    hyperparameters={\n",
    "        \"instruction_tuned\": \"True\",\n",
    "        \"epoch\": \"5\",\n",
    "        \"chat_dataset\": \"False\",\n",
    "        \"enable_fsdp\": \"True\",\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85f1a3-16d1-4e03-a519-b5c0d5ff8207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7904d9d5-f3eb-404a-90b2-d73607e73b39",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy and invoke the fine-tuned model\n",
    "\n",
    "You can deploy the fine-tuned model to an endpoint directly from the estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15818c56-7121-4833-84e7-0ecafee7b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3516ac7-93a4-4689-8a6e-a57a15d76bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feba2d3-fb6d-4fcc-b6a3-60f3886ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_job_name(job_name_prefixes):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    latest_job = None\n",
    "    # Set latest_creation_time to the minimum possible datetime with timezone info\n",
    "    latest_creation_time = datetime.min.replace(tzinfo=pytz.UTC)\n",
    "\n",
    "    for prefix in job_name_prefixes:\n",
    "        search_response = sagemaker_client.search(\n",
    "            Resource='TrainingJob',\n",
    "            SearchExpression={\n",
    "                'Filters': [\n",
    "                    {\n",
    "                        'Name': 'TrainingJobName',\n",
    "                        'Operator': 'Contains',\n",
    "                        'Value': prefix\n",
    "                    },\n",
    "                    {\n",
    "                        'Name': 'TrainingJobStatus',\n",
    "                        'Operator': 'Equals',\n",
    "                        'Value': \"Completed\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            SortBy='CreationTime',\n",
    "            SortOrder='Descending',\n",
    "            MaxResults=1\n",
    "        )\n",
    "\n",
    "        if search_response['Results']:\n",
    "            current_job = search_response['Results'][0]['TrainingJob']\n",
    "            creation_time = current_job['CreationTime']\n",
    "\n",
    "            if creation_time > latest_creation_time:\n",
    "                latest_job = current_job\n",
    "                latest_creation_time = creation_time\n",
    "\n",
    "    if latest_job:\n",
    "        return latest_job['TrainingJobName']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d24279-a385-4e37-ac6b-8dd6d462d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefixes = [\n",
    "    \"llama-3-1-8b-instruct\",\n",
    "    \"jumpstart-dft-meta-textgeneration-l\"\n",
    "]\n",
    "\n",
    "# Invoke the function with the prefixes\n",
    "job_name = get_last_job_name(job_name_prefixes)\n",
    "\n",
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1dc46-d7dc-422a-8afb-f8212bd8890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator.attach(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50280dc1-9431-438c-a1c3-05ccbe5d5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count = 1\n",
    "instance_type = \"ml.g5.4xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd174097-8cfb-4a32-8b04-c542a9e0b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1841bf1-3d4b-4d24-b04f-acf7009974d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we use the test data to invoke the fine-tuned model. To do this, create a helper function to template an example datapoint and query the model. For instruction fine-tuning, we insert a special demarkation key between the example input and output text, so this is added here in the templating process for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e701941-77e9-424f-a243-e621bcfe58ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Get the current date\n",
    "current_date = date.today()\n",
    "\n",
    "# Format the date as \"YYYY-MM-DD\"\n",
    "date_string = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "base_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system}<|eot_id|><|start_header_id|>user<|end_header_id|>{instruction}. Today is {current_date}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "prompt = base_prompt.format(\n",
    "    system=\"You are a marketing bot for a US-based telecom company called AnyCompany. Based on a customer profile, you choose an appropriate promotion for that customer and write a personalized marketing message to send to that customer.\",\n",
    "    instruction=\"Choose an appropriate promotion and write a personalized marketing message for a customer by following these steps: 1. Read the customer profile, 2. Think step-by-step to choose an appropriate promotion from the list of approved promotions and enclose it in <promotion> tags, 3. Write a personalized message for the customer based on the chosen promotion, the customer profile, and the time of year and enclose it in <personalized_message> tags. For the next customer, you can choose from the list of approved promotions. <approved_promotions> - $5 monthly winter holiday discount every month for the months of November-January - 10GB extra phone data for winter holidays every month from November to January - 20GB extra internet data for winter holidays every month from November to January - 10 extra minutes for winter holidays every month from November to January - 2GB extra phone data for birthday month - 5GB extra internet data for birthday month - $5 discount for birthday month - 30 extra minutes for birthday month - $2 discount on annual plan for customer with 2+ year tenure - $5 discount on annual plan for customer with 5+ year tenure - 5% discount for 6 months on new internet plan for customer with existing phone plan - 5% discount for 6 months on new phone plan for customer with existing internet plan - $5 voucher to spend on any phone or internet product </approved_promotions> Choose an appropriate promotion and write a personalized message for the customer below. Remember to use <promotion> and <personalized_message> tags. <customer_data> Name: Emily State: California DOB: 1985-11-12 Job: Software Engineer Join Date: 2018-05-15 Internet Service: 200Mbps Fiber Internet Contract: Annual Monthly Internet Costs: $65.99 Phone Service: 100GB Unlimited Minutes SmartPhone Contract: Annual Monthly Phone Costs: $89.99</customer_data>\",\n",
    "    current_date=date_string\n",
    ")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015c02b-9f30-4f58-b7eb-e14529c6e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "\t\"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"return_full_text\": False,\n",
    "        \"stop\": ['<|eot_id|>', '<|end_of_text|>']\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7f7e0-7155-40a4-a7e5-67b1d0f12edd",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662fea0-4f9e-4238-bd75-24a852f26e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845fbbb9",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa727a7b",
   "metadata": {},
   "source": [
    "### 1. Supported Inference Parameters\n",
    "\n",
    "This model supports the following payload parameters. You may specify any subset of these parameters when invoking an endpoint.\n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **truncate:** Truncate inputs tokens to the given size.\n",
    "* **typical_p:** Typical decoding mass, according to [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666).\n",
    "* **best_of:** Generate best_of sequences and return the one if the highest token logprobs.\n",
    "* **watermark:** Whether to perform watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226).\n",
    "* **details:** Return generation details, to include output token logprobs and IDs.\n",
    "* **decoder_input_details:** Return decoder input token logprobs and IDs.\n",
    "* **top_n_tokens:** Return the N most likely tokens at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbde98",
   "metadata": {},
   "source": [
    "### 2. Dataset formatting instruction for training\n",
    "\n",
    "We currently offer two types of fine-tuning: instruction fine-tuning and domain adaption fine-tuning. You can easily switch to one of the training \n",
    "methods by specifying parameter `instruction_tuned` being 'True' or 'False'.\n",
    "\n",
    "\n",
    "#### 2.1. Domain adaptation fine-tuning\n",
    "The Text Generation model can also be fine-tuned on any domain specific dataset. After being fine-tuned on the domain specific dataset, the model\n",
    "is expected to generate domain specific text and solve various NLP tasks in that specific domain with **few shot prompting**.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file. \n",
    "  - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "  - The number of files under train and validation (if provided) should equal to one, respectively. \n",
    "- **Output:** A trained model that can be deployed for inference. \n",
    "\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "```Note About Forward-Looking Statements\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "GENERAL\n",
    "Embracing Our Future ...\n",
    "```\n",
    "\n",
    "\n",
    "#### 2.2. Instruction fine-tuning\n",
    "The Text generation model can be instruction-tuned on any text data provided that the data \n",
    "is in the expected format. The instruction-tuned model can be further deployed for inference. \n",
    "Below are the instructions for how the training data should be formatted for input to the \n",
    "model.\n",
    "\n",
    "Below are the instructions for how the training data should be formatted for input to the model.\n",
    "\n",
    "- **Input:** A train and an optional validation directory. Train and validation directories should contain one or multiple JSON lines (`.jsonl`) formatted files. In particular, train directory can also contain an optional `*.json` file describing the input and output formats. \n",
    "  - The best model is selected according to the validation loss, calculated at the end of each epoch.\n",
    "  If a validation set is not given, an (adjustable) percentage of the training data is\n",
    "  automatically split and used for validation.\n",
    "  - The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary\n",
    "representing a single data sample. All training data must be in a single folder, however\n",
    "it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no\n",
    "template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\",\n",
    "    \"completion\": \"{response}\"\n",
    "  }\n",
    "  ```\n",
    "  - In this case, the data in the JSON lines entries must include `instruction`, `context` and `response` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define\n",
    "  the input and output templates.\n",
    "  Below is a sample custom template:\n",
    "\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"question: {question} context: {context}\",\n",
    "    \"completion\": \"{answer}\"\n",
    "  }\n",
    "  ```\n",
    "Here, the data in the JSON lines entries must include `question`, `context` and `answer` fields. \n",
    "- **Output:** A trained model that can be deployed for inference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91928b",
   "metadata": {},
   "source": [
    "### 3. Supported Hyper-parameters for fine-tuning\n",
    "\n",
    "- epoch: The number of passes that the fine-tuning algorithm takes through the training dataset. Must be an integer greater than 1. Default: 5\n",
    "- learning_rate: The rate at which the model weights are updated after working through each batch of training examples. Must be a positive float greater than 0. Default: 1e-4.\n",
    "- instruction_tuned: Whether to instruction-train the model or not. Must be 'True' or 'False'. Default: 'False'\n",
    "- per_device_train_batch_size: The batch size per GPU core/CPU for training. Must be a positive integer. Default: 4.\n",
    "- per_device_eval_batch_size: The batch size per GPU core/CPU for evaluation. Must be a positive integer. Default: 1\n",
    "- max_train_samples: For debugging purposes or quicker training, truncate the number of training examples to this value. Value -1 means using all of training samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_val_samples: For debugging purposes or quicker training, truncate the number of validation examples to this value. Value -1 means using all of validation samples. Must be a positive integer or -1. Default: -1. \n",
    "- max_input_length: Maximum total input sequence length after tokenization. Sequences longer than this will be truncated. If -1, max_input_length is set to the minimum of 1024 and the maximum model length defined by the tokenizer. If set to a positive value, max_input_length is set to the minimum of the provided value and the model_max_length defined by the tokenizer. Must be a positive integer or -1. Default: -1. \n",
    "- validation_split_ratio: If validation channel is none, ratio of train-validation split from the train data. Must be between 0 and 1. Default: 0.2. \n",
    "- train_data_split_seed: If validation data is not present, this fixes the random splitting of the input training data to training and validation data used by the algorithm. Must be an integer. Default: 0.\n",
    "- preprocessing_num_workers: The number of processes to use for the preprocessing. If None, main process is used for preprocessing. Default: \"None\"\n",
    "- lora_r: Lora R. Must be a positive integer. Default: 8.\n",
    "- lora_alpha: Lora Alpha. Must be a positive integer. Default: 32\n",
    "- lora_dropout: Lora Dropout. must be a positive float between 0 and 1. Default: 0.05. \n",
    "- int8_quantization: If True, model is loaded with 8 bit precision for training. Default for 7B/13B: False. Default for 70B: True.\n",
    "- enable_fsdp: If True, training uses Fully Sharded Data Parallelism. Default for 7B/13B: True. Default for 70B: False.\n",
    "\n",
    "Note 1: int8_quantization is not supported with FSDP. Also, int8_quantization = 'False' and enable_fsdp = 'False' is not supported due to CUDA memory issues for any of the g5 family instances. Thus, we recommend setting exactly one of int8_quantization or enable_fsdp to be 'True'\n",
    "Note 2: Due to the size of the model, 70B model can not be fine-tuned with enable_fsdp = 'True' for any of the supported instance types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021959b",
   "metadata": {},
   "source": [
    "### 4. Supported Instance types\n",
    "\n",
    "We have tested our scripts on the following instances types:\n",
    "\n",
    "| Model | Model ID | All Supported Instances Types for fine-tuning |\n",
    "| - | - | - |\n",
    "| Llama 2 7B | meta-textgeneration-llama-2-7b | ml.g5.12xlarge, ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge |\n",
    "| Llama 2 13B | meta-textgeneration-llama-2-13b | ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge |\n",
    "| Llama 2 70B | meta-textgeneration-llama-2-70b | ml.g5.48xlarge |\n",
    "| Llama 3 8B | meta-textgeneration-llama-3-8b | ml.g5.12xlarge, ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge, ml.g4dn.12xlarge |\n",
    "| Llama 3 70B | meta-textgeneration-llama-3-70b | ml.g5.48xlarge, ml.p4d.24xlarge |\n",
    "| Llama 3.1 8B | meta-textgeneration-llama-3-1-8b | ml.g5.12xlarge, ml.g5.24xlarge, ml.g5.48xlarge, ml.p3dn.24xlarge, ml.g4dn.12xlarge |\n",
    "| Llama 3.1 70B | meta-textgeneration-llama-3-1-70b | ml.g5.48xlarge, ml.p4d.24xlarge |\n",
    "| Llama 3.1 405B FP8 | meta-textgeneration-llama-3-1-405b-fp8 | ml.p5.48xlarge |\n",
    "\n",
    "\n",
    "Other instance types may also work to fine-tune. Note: When using p3 instances, training will be done with 32 bit precision as bfloat16 is not supported on these instances. Thus, training job would consume double the amount of CUDA memory when training on p3 instances compared to g5 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed8e34",
   "metadata": {},
   "source": [
    "### 5. Few notes about the fine-tuning method\n",
    "\n",
    "- Fine-tuning scripts are based on [this repo](https://github.com/facebookresearch/llama-recipes/tree/main). \n",
    "- Instruction tuning dataset is first converted into domain adaptation dataset format before fine-tuning. \n",
    "- Fine-tuning scripts utilize Fully Sharded Data Parallel (FSDP) as well as Low Rank Adaptation (LoRA) method fine-tuning the models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
