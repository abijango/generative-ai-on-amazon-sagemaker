{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c3687f-1307-479a-a7be-12ef4d9b84b3",
   "metadata": {},
   "source": [
    "# Experiment Management with Comet ML and Amazon SageMaker\n",
    "## Example: Fraud Detection\n",
    "\n",
    "This notebook demonstrates how to build, track, and evaluate machine learning \n",
    "models for credit card fraud detection using Comet ML's experiment tracking \n",
    "platform integrated with Amazon SageMaker's managed ML services.\n",
    "\n",
    "What you'll learn:\n",
    "- Setting up Comet ML for experiment tracking in SageMaker\n",
    "- Dataset Tracking with Comet Artifacts\n",
    "- Logging a SageMaker train job with Comet\n",
    "- End to end experiment management with Comet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a7687-6a9e-47f3-b071-4d4fd20a6e16",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5a20b-117c-459d-9924-d502808c09a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T18:59:34.414888Z",
     "iopub.status.busy": "2025-07-07T18:59:34.414383Z",
     "iopub.status.idle": "2025-07-07T18:59:39.942404Z",
     "shell.execute_reply": "2025-07-07T18:59:39.941611Z",
     "shell.execute_reply.started": "2025-07-07T18:59:34.414856Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install comet_ml --upgrade --quiet\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# AWS and ML libraries\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "# Comet ML for experiment tracking\n",
    "import comet_ml\n",
    "from comet_ml import Experiment, API, Artifact\n",
    "from comet_ml.integration.sagemaker import log_sagemaker_training_job_v1\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, f1_score, \n",
    "                           precision_score, recall_score, roc_auc_score, \n",
    "                           precision_recall_curve, roc_curve)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0240f-e422-41b5-9754-c08d4450c61f",
   "metadata": {},
   "source": [
    "### Configuration and Authentication\n",
    "**Make sure the following environment variables are set:**\n",
    "- COMET_API_KEY\n",
    "- AWS_PARTNER_APP_ARN\n",
    "- AWS_PARTNER_APP_AUTH='true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd856fd-7d2a-4441-a8b6-79b32c640bd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:17.977112Z",
     "iopub.status.busy": "2025-07-07T19:00:17.976049Z",
     "iopub.status.idle": "2025-07-07T19:00:18.155916Z",
     "shell.execute_reply": "2025-07-07T19:00:18.155170Z",
     "shell.execute_reply.started": "2025-07-07T19:00:17.977076Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Set Environment Variables\n",
    "# os.environ['AWS_PARTNER_APP_AUTH'] = 'true'\n",
    "# os.environ['AWS_PARTNER_APP_ARN'] = 'Your AWS PARTNER APP ARN'\n",
    "# os.environ['COMET_API_KEY'] = ''\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Comet ML configuration\n",
    "COMET_WORKSPACE = 'your-workspace'  # Replace with your Comet workspace\n",
    "comet_api = API()\n",
    "\n",
    "# SageMaker configuration\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket_name = 'your-sagemaker-bucket' #sagemaker_session.default_bucket()  # Or set your own bucket\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "input_data_prefix = 'fraud-detection-demo/datasets'\n",
    "processed_data_prefix = 'fraud-detection-demo/datasets/processed'\n",
    "model_output_prefix = 'fraud-detection-demo/models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b820381-8433-4efd-9c37-23c764e56521",
   "metadata": {},
   "source": [
    "## Data Preparation and Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d652c-79f0-4438-bf31-c06a1aec2928",
   "metadata": {},
   "source": [
    "### Data Loading and Exploration\n",
    "We'll be working with the credit card fraud dataset.\n",
    "Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cba5b7-52a0-4808-8016-fd0ab7d8daef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:21.481927Z",
     "iopub.status.busy": "2025-07-07T19:00:21.481284Z",
     "iopub.status.idle": "2025-07-07T19:00:24.002483Z",
     "shell.execute_reply": "2025-07-07T19:00:24.001575Z",
     "shell.execute_reply.started": "2025-07-07T19:00:21.481897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'creditcard.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Analyze the dataset structure\n",
    "print(\"\\n Dataset Overview:\")\n",
    "print(f\"   Dataset shape: {df.shape}\")\n",
    "print(f\"   Columns: {list(df.columns)}\")\n",
    "print(f\"   Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"   Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Analyze class distribution - this is crucial for fraud detection!\n",
    "class_dist = df['Class'].value_counts()\n",
    "fraud_percentage = df['Class'].mean() * 100\n",
    "\n",
    "print(f\"\\n Class Distribution Analysis:\")\n",
    "print(f\"   Normal transactions: {class_dist[0]:,} ({100-fraud_percentage:.2f}%)\")\n",
    "print(f\"   Fraudulent transactions: {class_dist[1]:,} ({fraud_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b02832-9887-46c1-ba67-6ff887e953f1",
   "metadata": {},
   "source": [
    "### Upload file to S3\n",
    "\n",
    "Now, let us upload the CSV to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8842a0-ab60-462b-9d04-ae72935b21bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:29.429308Z",
     "iopub.status.busy": "2025-07-07T19:00:29.429029Z",
     "iopub.status.idle": "2025-07-07T19:00:30.322719Z",
     "shell.execute_reply": "2025-07-07T19:00:30.322085Z",
     "shell.execute_reply.started": "2025-07-07T19:00:29.429288Z"
    }
   },
   "outputs": [],
   "source": [
    "s3_key = 'creditcard.csv'\n",
    "\n",
    "a = s3.upload_file(file_path, bucket_name, os.path.join(input_data_prefix, s3_key))\n",
    "# The S3 path is constructed from the bucket and key\n",
    "s3_data_path = f\"s3://{bucket_name}/{input_data_prefix}/{s3_key}\"\n",
    "print(f\"File uploaded to {s3_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91d103-4295-4dbb-bcb7-632ce7d8a337",
   "metadata": {},
   "source": [
    "### Dataset Artifact Creation\n",
    "We'll create a Comet Dataset Artifact to begin tracking and versioning our dataset. The can now be linked to any experiment we start in Comet. We can update it, add assets, or create a new version any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a809d-c935-445c-ab9a-62e627036359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:40.891160Z",
     "iopub.status.busy": "2025-07-07T19:00:40.890809Z",
     "iopub.status.idle": "2025-07-07T19:00:41.103176Z",
     "shell.execute_reply": "2025-07-07T19:00:41.102416Z",
     "shell.execute_reply.started": "2025-07-07T19:00:40.891136Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Comet Artifact to track our raw dataset\n",
    "dataset_artifact = Artifact(\n",
    "    name=\"fraud-dataset\",\n",
    "    artifact_type=\"dataset\",\n",
    "    aliases=[\"raw\"]\n",
    ")\n",
    "\n",
    "# Add the raw dataset file to the artifact\n",
    "dataset_artifact.add_remote(s3_data_path, metadata={\n",
    "    \"dataset_stage\": \"raw\", \n",
    "    \"dataset_split\": \"not_split\", \n",
    "    \"preprocessing\": \"none\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195a6df-21cd-4aed-a592-7d6842cb2e85",
   "metadata": {},
   "source": [
    "## Initialize an Experiemnt\n",
    "\n",
    "To begin tracking our work, we'll create a Comet Experiment. As soon as we initialize the experiment, Comet gets to work, already tracking our code, installed libraries, and other metadata in the background. To begin tracking data lineage for this experiment, we simply need to log the dataset artifact we just created to the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da0071-524d-42ae-be40-66a3153b541b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:48.451314Z",
     "iopub.status.busy": "2025-07-07T19:00:48.450953Z",
     "iopub.status.idle": "2025-07-07T19:00:49.399857Z",
     "shell.execute_reply": "2025-07-07T19:00:49.398995Z",
     "shell.execute_reply.started": "2025-07-07T19:00:48.451289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new Comet experiment\n",
    "experiment_1 = comet_ml.Experiment(\n",
    "    project_name=COMET_PROJECT_NAME,\n",
    "    workspace=COMET_WORKSPACE,\n",
    ")\n",
    "\n",
    "# Log the dataset artifact to this experiment for lineage tracking\n",
    "experiment_1.log_artifact(dataset_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b88c9e-4335-4e59-ba06-de3de5dbee4d",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The code for data processing can be found in `preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcbe53-2509-4eb0-9bb7-5fe281f6da5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:00:53.021228Z",
     "iopub.status.busy": "2025-07-07T19:00:53.020771Z",
     "iopub.status.idle": "2025-07-07T19:00:54.361702Z",
     "shell.execute_reply": "2025-07-07T19:00:54.360774Z",
     "shell.execute_reply.started": "2025-07-07T19:00:53.021193Z"
    }
   },
   "outputs": [],
   "source": [
    "!pygmentize preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6951eb-bcc3-4c05-b954-20130ee00690",
   "metadata": {},
   "source": [
    "### Run the processing job\n",
    "\n",
    "Now, let us run the data processing as a SageMaker Processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c8e56-952b-459a-9ac9-1b202ab02216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:01:22.043019Z",
     "iopub.status.busy": "2025-07-07T19:01:22.042215Z",
     "iopub.status.idle": "2025-07-07T19:06:47.031817Z",
     "shell.execute_reply": "2025-07-07T19:06:47.030858Z",
     "shell.execute_reply.started": "2025-07-07T19:01:22.042983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run SageMaker processing job\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version='1.0-1',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.t3.medium'\n",
    ")\n",
    "\n",
    "processor.run(\n",
    "    code='preprocess.py',\n",
    "    inputs=[ProcessingInput(source=s3_data_path, destination='/opt/ml/processing/input')],\n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f's3://{bucket_name}/{processed_data_prefix}')]\n",
    ")\n",
    "print('Processing job started')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009a8f0-bee0-4e04-afa7-b8a3fa709d78",
   "metadata": {},
   "source": [
    "### Log Preprocessed Dataset\n",
    "Now that we've modified our dataset, we'll log the preprocessed version to track lineage. We'll create a new dataset artifact for the preprocessed data, with the same name as our original dataset artifact. Comet will track this as a new version of the dataset. Next we will **log our S3 dataset path as a remote asset**. By default Comet will now link to and track all files in this path, meaning that we don't need to explicitly log each file separately.\n",
    "\n",
    "Once we've logged the dataset to our first experiment, we'll be able to access all of these assets through Comet in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9bb01-ae60-4267-b41f-0cf82b16f9cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:06:47.033914Z",
     "iopub.status.busy": "2025-07-07T19:06:47.033482Z",
     "iopub.status.idle": "2025-07-07T19:06:48.394313Z",
     "shell.execute_reply": "2025-07-07T19:06:48.393588Z",
     "shell.execute_reply.started": "2025-07-07T19:06:47.033882Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an updated version of the 'fraud-dataset' Artifact for the preprocessed data\n",
    "preprocessed_dataset_artifact = Artifact(\n",
    "    name=\"fraud-dataset\",\n",
    "    artifact_type=\"dataset\", \n",
    "    aliases=[\"preprocessed\"],\n",
    "    metadata={\n",
    "        \"description\": \"Credit card fraud detection dataset\",\n",
    "        \"source\": \"Kaggle - Credit Card Fraud Detection\",\n",
    "        \"data_card\": \"https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\",\n",
    "        \"total_samples\": len(df),\n",
    "        \"fraud_samples\": int(df['Class'].sum()),\n",
    "        \"fraud_percentage\": f\"{fraud_percentage:.3f}%\",\n",
    "        \"dataset_stage\": \"preprocessed\",\n",
    "        \"preprocessing\": \"StandardScaler + train/val/test split\",\n",
    "        \"columns\": ['Class'] + list(df.columns.drop(['Time'])),\n",
    "        \"feature_columns\": list(df.columns.drop(['Time', 'Class'])),\n",
    "        \"target\": \"Class\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add our train, validation, and test dataset files as remote assets \n",
    "preprocessed_dataset_artifact.add_remote(\n",
    "    uri=f's3://{bucket_name}/{processed_data_prefix}',\n",
    "    logical_path='split_data'\n",
    ")\n",
    "\n",
    "# Add our preprocessing code as an asset to be uploaded to Comet\n",
    "preprocessed_dataset_artifact.add(\"preprocess.py\")\n",
    "\n",
    "# Log the updated dataset to the experiment to track the updates\n",
    "experiment_1.log_artifact(preprocessed_dataset_artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18352f1-296e-4b90-bb6c-880e71a8f3e8",
   "metadata": {},
   "source": [
    "## Define common functions to be used across experiments\n",
    "\n",
    "Now, we will define the steps of an experiment which are:\n",
    "1. Training\n",
    "2. Logging the training job and model artifact\n",
    "3. Logging the model metrics\n",
    "4. Deploy and evaluate the model and log evaluation metrics\n",
    "\n",
    "For each of the steps, we create a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a8601-7e3a-49e1-a0fd-0b715ff2aea3",
   "metadata": {},
   "source": [
    "### Function for Training the model\n",
    "This function runs a straightforward training job on SageMaker, no additional logging needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a20c2-30a7-479d-a287-fa1ce75944a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:17:52.730788Z",
     "iopub.status.busy": "2025-07-07T19:17:52.730488Z",
     "iopub.status.idle": "2025-07-07T19:17:52.736572Z",
     "shell.execute_reply": "2025-07-07T19:17:52.735476Z",
     "shell.execute_reply.started": "2025-07-07T19:17:52.730764Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model_output_path,\n",
    "    execution_role,\n",
    "    sagemaker_session_obj,\n",
    "    hyperparameters_dict,\n",
    "    train_channel_loc,\n",
    "    val_channel_loc\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an XGBoost model using SageMaker.\n",
    "\n",
    "    Args:\n",
    "        model_output_path (str): Path where the trained model will be saved\n",
    "        execution_role (str): IAM role for SageMaker execution\n",
    "        sagemaker_session_obj: SageMaker session object\n",
    "        hyperparameters_dict (dict): Dictionary of hyperparameters\n",
    "        train_channel_loc (str): Location of training data\n",
    "        val_channel_loc (str): Location of validation data\n",
    "\n",
    "    Returns:\n",
    "        Estimator: Trained XGBoost estimator\n",
    "    \"\"\"\n",
    "    # Get XGBoost container image\n",
    "    xgboost_image = sagemaker.image_uris.retrieve(\n",
    "        \"xgboost\",\n",
    "        sagemaker_session.boto_region_name,\n",
    "        version='1.5-1'\n",
    "    )\n",
    "    print(f\"Using XGBoost image: {xgboost_image}\")\n",
    "    print(f\"Model output location: {model_output_path}\")\n",
    "\n",
    "    # Create SageMaker estimator\n",
    "    estimator = Estimator(\n",
    "        image_uri=xgboost_image,\n",
    "        role=execution_role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        output_path=model_output_path,\n",
    "        sagemaker_session=sagemaker_session_obj,\n",
    "        hyperparameters=hyperparameters_dict,\n",
    "        max_run=1800  # Maximum training time in seconds\n",
    "    )\n",
    "\n",
    "    # Set up data channels for SageMaker\n",
    "    train_channel = TrainingInput(\n",
    "        train_channel_loc,\n",
    "        content_type='text/csv'\n",
    "    )\n",
    "    val_channel = TrainingInput(\n",
    "        val_channel_loc,\n",
    "        content_type='text/csv'\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    estimator.fit({\n",
    "        'train': train_channel,\n",
    "        'validation': val_channel\n",
    "    })\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83d596-4b4a-4004-8320-4cdf709039ab",
   "metadata": {},
   "source": [
    "### Function to Log the SageMaker Training Job with Comet\n",
    "Once training is complete, we'll use this function to log the training job to Comet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8b1e9-0a63-4a9f-b629-5f3a475c8ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:45:47.517273Z",
     "iopub.status.busy": "2025-07-07T19:45:47.516378Z",
     "iopub.status.idle": "2025-07-07T19:45:47.520936Z",
     "shell.execute_reply": "2025-07-07T19:45:47.520155Z",
     "shell.execute_reply.started": "2025-07-07T19:45:47.517243Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_training_job(experiment_key, training_estimator):\n",
    "    \"\"\"\n",
    "    Log SageMaker training job details to Comet.\n",
    "\n",
    "    Args:\n",
    "        experiment_key: Key identifier for the experiment\n",
    "        training_estimator: SageMaker estimator object\n",
    "    \"\"\"\n",
    "    # Get the API experiment object associated with our current experiment\n",
    "    api_experiment = comet_api.get_experiment(\n",
    "        COMET_WORKSPACE,\n",
    "        COMET_PROJECT_NAME,\n",
    "        experiment_key # accessed thorugh experiment.get_key()\n",
    "    )\n",
    "\n",
    "    # Log SageMaker training job details to Comet\n",
    "    # (Warnings are expected here. They're not a problem!)\n",
    "    log_sagemaker_training_job_v1(\n",
    "        estimator=training_estimator,\n",
    "        experiment=api_experiment\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384b422-462f-4f2a-98d3-1b760696dbb4",
   "metadata": {},
   "source": [
    "### Function to log the model artifact\n",
    "We'll log our model as a remote artifact, linking to the model saved on S3.  Logging the model artifact will enable us to register it to our model registry in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f161d3c-b637-4649-9a45-cdeb9bc6fef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:45:54.316942Z",
     "iopub.status.busy": "2025-07-07T19:45:54.316068Z",
     "iopub.status.idle": "2025-07-07T19:45:54.320286Z",
     "shell.execute_reply": "2025-07-07T19:45:54.319602Z",
     "shell.execute_reply.started": "2025-07-07T19:45:54.316912Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_model_to_comet(experiment, model_name, model_artifact_path, metadata):\n",
    "    # Log model to Comet\n",
    "    experiment.log_remote_model(\n",
    "        model_name=model_name,\n",
    "        uri=model_artifact_path,\n",
    "        metadata=metadata\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34609689-4aa7-4789-a793-174f32f56689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T09:19:56.299146Z",
     "iopub.status.busy": "2025-07-04T09:19:56.298728Z",
     "iopub.status.idle": "2025-07-04T09:19:56.303098Z",
     "shell.execute_reply": "2025-07-04T09:19:56.302334Z",
     "shell.execute_reply.started": "2025-07-04T09:19:56.299116Z"
    }
   },
   "source": [
    "### Function to deploy and evaluate the model\n",
    "This function will deploy our model to an endpoint and evaluate its performance on our test dataset. We'll log relevant metrics and analytics to Comet to help with model debugging and improvement:\n",
    "\n",
    "- Model performance metrics\n",
    "- Confusion matrix\n",
    "- Performance curves (ROC and precision-recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fda3b-e7c4-487c-a55a-91fd3759129d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:46:03.003424Z",
     "iopub.status.busy": "2025-07-07T19:46:03.002932Z",
     "iopub.status.idle": "2025-07-07T19:46:03.014832Z",
     "shell.execute_reply": "2025-07-07T19:46:03.013852Z",
     "shell.execute_reply.started": "2025-07-07T19:46:03.003396Z"
    }
   },
   "outputs": [],
   "source": [
    "def deploy_and_evaluate_model(\n",
    "    experiment,\n",
    "    estimator,\n",
    "    X_test_scaled,\n",
    "    y_test\n",
    "):\n",
    "    \"\"\"\n",
    "    Deploy model to endpoint and evaluate its performance.\n",
    "\n",
    "    Args:\n",
    "        experiment: The currently running Comet experiment\n",
    "        estimator: The trained estimator model\n",
    "        X_test_scaled: Scaled test features\n",
    "        y_test: Test labels\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing model performance metrics\n",
    "    \"\"\"\n",
    "    # Deploy to endpoint\n",
    "    predictor = estimator.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=\"ml.m5.xlarge\"\n",
    "    )\n",
    "    print(f\"Endpoint deployed: {predictor.endpoint_name}\")\n",
    "\n",
    "    # Prepare test data and make predictions\n",
    "    predictor.serializer = CSVSerializer()\n",
    "\n",
    "    # Process in batches to handle large datasets\n",
    "    batch_size = 1000\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(0, len(X_test_scaled), batch_size):\n",
    "        batch = X_test_scaled.iloc[i:i+batch_size].values\n",
    "        batch_pred = predictor.predict(batch)\n",
    "        batch_decoded = batch_pred.decode('utf-8')\n",
    "        batch_probs = [float(x.strip()) for x in batch_decoded.split('\\n') if x.strip()]\n",
    "        all_predictions.extend(batch_probs)\n",
    "\n",
    "    y_pred_prob_as_np_array = np.array(all_predictions)\n",
    "\n",
    "    # Validate prediction count matches test data\n",
    "    if len(y_pred_prob_as_np_array) != len(y_test):\n",
    "        raise ValueError(f\"Prediction count ({len(y_pred_prob_as_np_array)}) doesn't match test data ({len(y_test)})\")\n",
    "\n",
    "    decision_threshold = 0.5\n",
    "    y_pred = (y_pred_prob_as_np_array > decision_threshold).astype(int)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate comprehensive metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1_score\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_pred_prob_as_np_array),\n",
    "        \"true_positives\": int(tp),\n",
    "        \"true_negatives\": int(tn),\n",
    "        \"false_positives\": int(fp),\n",
    "        \"false_negatives\": int(fn),\n",
    "        \"total_fraud_cases\": int(y_test.sum()),\n",
    "        \"total_test_samples\": len(y_test),\n",
    "        \"decision_threshold\": decision_threshold\n",
    "    }\n",
    "\n",
    "    # Log metrics to Comet\n",
    "    experiment.log_metrics(metrics)\n",
    "\n",
    "    # Print performance results\n",
    "    print(\"\\nModel Performance Results:\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"   F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"   ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"   True Negatives: {tn:,}\")\n",
    "    print(f\"   False Positives: {fp:,}\")\n",
    "    print(f\"   False Negatives: {fn:,}\")\n",
    "    print(f\"   True Positives: {tp:,}\")\n",
    "\n",
    "    # Log confusion matrix to Comet\n",
    "    labels = ['Normal', 'Fraud']\n",
    "    experiment.log_confusion_matrix(matrix=cm, labels=labels)\n",
    "\n",
    "    # Log precision-recall and ROC curves\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_prob_as_np_array)\n",
    "    experiment.log_curve(\"precision_recall_curve\", x=recall_curve, y=precision_curve)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob_as_np_array)\n",
    "    experiment.log_curve(\"roc_curve\", x=fpr, y=tpr)\n",
    "\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5ad61-5466-48b4-888d-91b2421227ee",
   "metadata": {},
   "source": [
    "## Run the experiments\n",
    "\n",
    "Now that we have set up all common utility functions, let us use them to run our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473de08c-e162-44b4-b95d-48e19f756993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-04T10:13:28.540491Z",
     "iopub.status.busy": "2025-07-04T10:13:28.540185Z",
     "iopub.status.idle": "2025-07-04T10:13:28.553125Z",
     "shell.execute_reply": "2025-07-04T10:13:28.552026Z",
     "shell.execute_reply.started": "2025-07-04T10:13:28.540466Z"
    }
   },
   "source": [
    "### Set up common variables for all experiments.\n",
    "\n",
    "Some variables will remain the same across all experiments. Lets put them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c78456-af69-4198-a252-54bdcdded3a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:18:54.885323Z",
     "iopub.status.busy": "2025-07-07T19:18:54.884912Z",
     "iopub.status.idle": "2025-07-07T19:18:54.888809Z",
     "shell.execute_reply": "2025-07-07T19:18:54.887844Z",
     "shell.execute_reply.started": "2025-07-07T19:18:54.885296Z"
    }
   },
   "outputs": [],
   "source": [
    "train_channel_location = f's3://{bucket_name}/{processed_data_prefix}/train_data.csv'\n",
    "validation_channel_location = f's3://{bucket_name}/{processed_data_prefix}/val_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0d668-9c31-496d-875b-f299d286048e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:18:56.076800Z",
     "iopub.status.busy": "2025-07-07T19:18:56.075765Z",
     "iopub.status.idle": "2025-07-07T19:18:56.737916Z",
     "shell.execute_reply": "2025-07-07T19:18:56.736975Z",
     "shell.execute_reply.started": "2025-07-07T19:18:56.076170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the test dataset locally for evaluation\n",
    "s3.download_file(bucket_name, f'{processed_data_prefix}/test_data.csv', 'test_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv', header=None)\n",
    "y_test = test_data[0]\n",
    "X_test_scaled = test_data.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55bb8c0-61c0-4c58-a69e-ee7e53078b10",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bed9171-11d3-4ea0-82d0-4cf891d136ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:18:58.601115Z",
     "iopub.status.busy": "2025-07-07T19:18:58.600749Z",
     "iopub.status.idle": "2025-07-07T19:23:16.551154Z",
     "shell.execute_reply": "2025-07-07T19:23:16.550357Z",
     "shell.execute_reply.started": "2025-07-07T19:18:58.601090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters for first experiment\n",
    "hyperparameters_v1 = {\n",
    "    'objective': 'binary:logistic',          # Binary classification\n",
    "    'num_round': 100,                        # Number of boosting rounds\n",
    "    'eval_metric': 'auc',                    # Evaluation metric (good for imbalanced data)\n",
    "    'learning_rate': 0.15,                   # Learning rate\n",
    "    'max_depth': 6,                          # Maximum tree depth\n",
    "    'subsample': 0.7,                        # Fraction of samples for each tree\n",
    "    'colsample_bytree': 0.7,                 # Fraction of features for each tree\n",
    "    'booster': 'gbtree'                      # Booster algorithm: 'gbtree', 'gblinear', or 'dart'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "estimator_1 = train(\n",
    "    model_output_path=f\"s3://{bucket_name}/{model_output_prefix}/1\",\n",
    "    execution_role=role,\n",
    "    sagemaker_session_obj=sagemaker_session,\n",
    "    hyperparameters_dict=hyperparameters_v1,\n",
    "    train_channel_loc=train_channel_location,\n",
    "    val_channel_loc=validation_channel_location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d254f-aa78-4874-bed9-f481bcc33761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:46:41.177892Z",
     "iopub.status.busy": "2025-07-07T19:46:41.177492Z",
     "iopub.status.idle": "2025-07-07T19:46:44.690205Z",
     "shell.execute_reply": "2025-07-07T19:46:44.689330Z",
     "shell.execute_reply.started": "2025-07-07T19:46:41.177866Z"
    }
   },
   "outputs": [],
   "source": [
    "# log the training job\n",
    "log_training_job(experiment_key = experiment_1.get_key(), training_estimator=estimator_1)\n",
    "\n",
    "# log the model artifact to comet\n",
    "metadata = {\n",
    "    \"framework\": \"XGBoost-SageMaker-Built-In\", \n",
    "    \"algorithm\": \"Gradient Boosting\",\n",
    "    \"use_case\": \"fraud_detection\",\n",
    "    \"data_type\": \"tabular\",\n",
    "    \"target_metric\": \"auc\"\n",
    "}\n",
    "log_model_to_comet(experiment = experiment_1,\n",
    "                   model_name=\"fraud-detection-xgb-v1\", \n",
    "                   model_artifact_path=estimator_1.model_data, \n",
    "                   metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160a2c0-8685-494d-80b2-84eb1181632e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:47:24.285170Z",
     "iopub.status.busy": "2025-07-07T19:47:24.284759Z",
     "iopub.status.idle": "2025-07-07T19:50:28.847822Z",
     "shell.execute_reply": "2025-07-07T19:50:28.847001Z",
     "shell.execute_reply.started": "2025-07-07T19:47:24.285146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deploy and evaluate the model with Comet logging\n",
    "deploy_and_evaluate_model(experiment=experiment_1,\n",
    "                          estimator=estimator_1,\n",
    "                          X_test_scaled=X_test_scaled,\n",
    "                          y_test=y_test\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647fdd52-d8ab-4384-9786-942f52fc9cf7",
   "metadata": {},
   "source": [
    "#### End the first Experiemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a2313-9c8b-417a-ba02-62db2cf3f426",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T19:50:28.849606Z",
     "iopub.status.busy": "2025-07-07T19:50:28.849288Z",
     "iopub.status.idle": "2025-07-07T19:50:29.869341Z",
     "shell.execute_reply": "2025-07-07T19:50:29.868587Z",
     "shell.execute_reply.started": "2025-07-07T19:50:28.849575Z"
    }
   },
   "outputs": [],
   "source": [
    "# When running a Comet experiment from a Jupyter Notebook make sure to end the experiment to make sure everything is captured\n",
    "experiment_1.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def77d5-cd6e-4466-8772-509e86718d7f",
   "metadata": {},
   "source": [
    "### Experiment 2: Weight positive class to improve true positive rate\n",
    "Now that we've run our first experiment and logged it to Comet,let's run another experiment to compare side by side.\n",
    "Here we'll train a new model using the same dataset and different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec30ffd-324e-4cd6-bf96-e0870f71cda3",
   "metadata": {},
   "source": [
    "#### Access Comet Artifacts\n",
    "\n",
    "Since the preprocessed data was logged to the first experiment, we can access and re-use the same dataset through Comet. This is useful when collaborating on a project with a team across different notebooks and avoids the hassle of keeping track of dataset versions manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bf4123-b19d-4845-b15f-a8758469a6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T17:38:21.241828Z",
     "iopub.status.busy": "2025-07-07T17:38:21.241461Z",
     "iopub.status.idle": "2025-07-07T17:38:22.396438Z",
     "shell.execute_reply": "2025-07-07T17:38:22.395584Z",
     "shell.execute_reply.started": "2025-07-07T17:38:21.241804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new experiment\n",
    "experiment_2 = comet_ml.Experiment(\n",
    "    project_name=COMET_PROJECT_NAME,\n",
    "    workspace=COMET_WORKSPACE,\n",
    ")\n",
    "\n",
    "# Fetch the Artifact object from Comet and attach it to the new experiment\n",
    "recovered_artifact = experiment_2.get_artifact('fraud-dataset',\n",
    "                                       version_or_alias='preprocessed') # This automatically retrieves the latest version with the 'preprocessed' alias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d15a57-53a0-40f9-ac3c-34c1b8b076d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T20:00:10.111115Z",
     "iopub.status.busy": "2025-07-07T20:00:10.110489Z",
     "iopub.status.idle": "2025-07-07T20:00:10.227677Z",
     "shell.execute_reply": "2025-07-07T20:00:10.226907Z",
     "shell.execute_reply.started": "2025-07-07T20:00:10.111083Z"
    }
   },
   "outputs": [],
   "source": [
    "# If we were running the next experiment from a separate notebook, we could use the S3 paths saved in Comet to access our datasets \n",
    "for asset in recovered_artifact.assets:\n",
    "    if asset.logical_path == \"split_data/test_data.csv\":\n",
    "        print(asset.link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28a24ef-5084-4bd2-bee8-780a0556bf48",
   "metadata": {},
   "source": [
    "#### Train, log, and evaluate the second model\n",
    "Train a new model using the same dataset and different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f0ecda-ff30-4263-8383-d0221563209e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:45:20.495373Z",
     "iopub.status.busy": "2025-07-07T21:45:20.494869Z",
     "iopub.status.idle": "2025-07-07T21:54:48.453208Z",
     "shell.execute_reply": "2025-07-07T21:54:48.452251Z",
     "shell.execute_reply.started": "2025-07-07T21:45:20.495339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 2: Weight positive class to improve true positive rate\n",
    "\n",
    "hyperparameters_v2 = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'num_round': 175,\n",
    "    'eval_metric': 'auc',\n",
    "    'learning_rate': 0.14,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 1.0,\n",
    "    'scale_pos_weight': 1500,  # Handle class imbalance\n",
    "    'booster': 'gbtree'\n",
    "}\n",
    "\n",
    "tags_v2 = [\"fraud-detection\", \"sagemaker\", \"xgboost\", \"class-weighted\"]\n",
    "experiment_2.add_tags(tags_v2)\n",
    "\n",
    "\n",
    "estimator_2 = train(\n",
    "    model_output_path=f\"s3://{bucket_name}/{model_output_prefix}/2\",\n",
    "    execution_role=role,\n",
    "    sagemaker_session_obj=sagemaker_session,\n",
    "    hyperparameters_dict=hyperparameters_v2,\n",
    "    train_channel_loc=train_channel_location,\n",
    "    val_channel_loc=validation_channel_location\n",
    ")\n",
    "\n",
    "#log training job metrics\n",
    "log_training_job(experiment_key = experiment_2.get_key(), training_estimator=estimator_2)\n",
    "\n",
    "#log model artifact to comet\n",
    "metadata={\n",
    "    \"framework\": \"XGBoost-SageMaker-Built-In\", \n",
    "    \"algorithm\": \"Gradient Boosting\",\n",
    "    \"use_case\": \"fraud_detection\",\n",
    "    \"data_type\": \"tabular\",\n",
    "    \"target_metric\": \"auc\",\n",
    "}\n",
    "log_model_to_comet(experiment = experiment_2,\n",
    "                   model_name=\"fraud-detection-xgb-v2\", \n",
    "                   model_artifact_path=estimator_2.model_data, \n",
    "                   metadata=metadata)\n",
    "\n",
    "#deploy the model and log eval metrics\n",
    "deploy_and_evaluate_model(experiment=experiment_2,\n",
    "                          estimator=estimator_2, \n",
    "                          X_test_scaled=X_test_scaled, \n",
    "                          y_test=y_test\n",
    "                         )\n",
    "\n",
    "# End the Experiment\n",
    "experiment_2.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b20342-4376-4046-85a3-bf26ee546633",
   "metadata": {},
   "source": [
    "## View Comet Experiments in the UI\n",
    "The experiment URL links us to our experiment in the Comet UI, where we'll be able to debug, view metrics, and register our model. From the current experiment, navigate back to the project in the UI to compare it against our previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dea018-52fa-4aaf-86a6-c8fa2a52b7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T21:54:48.454983Z",
     "iopub.status.busy": "2025-07-07T21:54:48.454661Z",
     "iopub.status.idle": "2025-07-07T21:54:48.459869Z",
     "shell.execute_reply": "2025-07-07T21:54:48.459015Z",
     "shell.execute_reply.started": "2025-07-07T21:54:48.454950Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_2.url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5642e858-d97b-46ec-bf2a-8521cf974a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
