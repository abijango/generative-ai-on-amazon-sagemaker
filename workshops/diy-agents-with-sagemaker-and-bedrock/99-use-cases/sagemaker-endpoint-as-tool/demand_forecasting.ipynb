{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Forecasting with XGBoost on Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Prepare a synthetic demand forecasting dataset\n",
    "2. Train an XGBoost model on Amazon SageMaker\n",
    "3. Deploy the model to a SageMaker endpoint for real-time inference\n",
    "\n",
    "The workflow includes:\n",
    "- Generating synthetic time series data with trend, seasonality, and noise\n",
    "- Feature engineering for time series forecasting\n",
    "- Training an XGBoost model on SageMaker\n",
    "- Deploying the model to a SageMaker endpoint\n",
    "- Testing the endpoint with sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker session\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = input(\"> Enter the role ARN for your SageMaker execution role: \")\n",
    "    role = role.strip()\n",
    "\n",
    "print(f\"SageMaker session initialized in region: {region}\")\n",
    "print(f\"Using S3 bucket: {bucket}\")\n",
    "print(f\"Using IAM role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Demand Data\n",
    "\n",
    "We'll create synthetic time series data with trend, seasonality, and noise components to simulate demand patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demand_data(n_periods=365*2, seasonality=True, trend=True, noise_level=0.2):\n",
    "    \"\"\"Generate synthetic demand data with trend, seasonality, and noise.\"\"\"\n",
    "    time_idx = np.arange(n_periods)\n",
    "    \n",
    "    # Base demand\n",
    "    base_demand = 100\n",
    "    \n",
    "    # Add trend component\n",
    "    trend_component = 0\n",
    "    if trend:\n",
    "        trend_component = time_idx * 0.1\n",
    "    \n",
    "    # Add seasonality component\n",
    "    seasonality_component = 0\n",
    "    if seasonality:\n",
    "        # Weekly seasonality\n",
    "        weekly = 10 * np.sin(2 * np.pi * time_idx / 7)\n",
    "        # Monthly seasonality\n",
    "        monthly = 20 * np.sin(2 * np.pi * time_idx / 30)\n",
    "        # Yearly seasonality\n",
    "        yearly = 50 * np.sin(2 * np.pi * time_idx / 365)\n",
    "        \n",
    "        seasonality_component = weekly + monthly + yearly\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level * base_demand, n_periods)\n",
    "    \n",
    "    # Combine components\n",
    "    demand = base_demand + trend_component + seasonality_component + noise\n",
    "    \n",
    "    # Ensure no negative values\n",
    "    demand = np.maximum(demand, 0)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    dates = pd.date_range(start='2021-01-01', periods=n_periods)\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'demand': demand\n",
    "    })\n",
    "    \n",
    "    # Add date features\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating synthetic demand forecasting data...\")\n",
    "df = generate_demand_data()\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Data\n",
    "\n",
    "Let's visualize the synthetic demand data to understand its patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(df):\n",
    "    \"\"\"Visualize the demand data.\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['date'], df['demand'])\n",
    "    plt.title('Synthetic Demand Data')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "# Visualize data\n",
    "visualize_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Create lag and rolling window features for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Create lag and rolling features for time series forecasting.\"\"\"\n",
    "    # Create lag features (previous demand values)\n",
    "    for lag in [1, 7, 14, 30]:\n",
    "        df[f'lag_{lag}'] = df['demand'].shift(lag)\n",
    "\n",
    "    # Create rolling window features\n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'rolling_mean_{window}'] = df['demand'].rolling(window=window).mean()\n",
    "        df[f'rolling_std_{window}'] = df['demand'].rolling(window=window).std()\n",
    "\n",
    "    # Drop rows with NaN values (due to lag and rolling features)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Prepare features\n",
    "df = prepare_features(df)\n",
    "print(f\"Dataset shape after feature engineering: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"Split data into features and target, then into train/val/test sets.\"\"\"\n",
    "    # Define features and target\n",
    "    features = [col for col in df.columns if col not in ['date', 'demand']]\n",
    "    X = df[features]\n",
    "    y = df['demand']\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data(df)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to S3\n",
    "\n",
    "Save the datasets to CSV files and upload them to S3 for SageMaker training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_s3(X_train, y_train, X_val, y_val, X_test, y_test, session, bucket):\n",
    "    \"\"\"Save datasets to CSV and upload to S3.\"\"\"\n",
    "    # Create local directories for data\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "\n",
    "    # Save datasets to CSV\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    val_data = pd.concat([X_val, y_val], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_data.to_csv('data/train.csv', index=False)\n",
    "    val_data.to_csv('data/validation.csv', index=False)\n",
    "    test_data.to_csv('data/test.csv', index=False)\n",
    "\n",
    "    # Upload to S3\n",
    "    train_s3_path = session.upload_data('data/train.csv', bucket=bucket, key_prefix='demand-forecast/data')\n",
    "    val_s3_path = session.upload_data('data/validation.csv', bucket=bucket, key_prefix='demand-forecast/data')\n",
    "    test_s3_path = session.upload_data('data/test.csv', bucket=bucket, key_prefix='demand-forecast/data')\n",
    "    \n",
    "    return train_s3_path, val_s3_path, test_s3_path\n",
    "\n",
    "# Save data to S3\n",
    "train_s3_path, val_s3_path, test_s3_path = save_data_to_s3(\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, session, bucket\n",
    ")\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")\n",
    "print(f\"Validation data uploaded to: {val_s3_path}\")\n",
    "print(f\"Test data uploaded to: {test_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model on SageMaker\n",
    "\n",
    "Define and train an XGBoost model using SageMaker's training infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_s3_path, val_s3_path, role, bucket, instance_type='ml.m5.4xlarge'):\n",
    "    \"\"\"Train XGBoost model on SageMaker.\"\"\"\n",
    "    # Define XGBoost hyperparameters\n",
    "    hyperparameters = {\n",
    "        'max_depth': 6,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'num_round': 100,\n",
    "        'verbosity': 1\n",
    "    }\n",
    "\n",
    "    # Create XGBoost estimator\n",
    "    xgb_estimator = XGBoost(\n",
    "        entry_point='script.py',  # SageMaker will use a built-in script\n",
    "        framework_version='1.5-1',\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        output_path=f's3://{bucket}/demand-forecast/output',\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_estimator.fit({'train': train_s3_path, 'validation': val_s3_path})\n",
    "    \n",
    "    return xgb_estimator\n",
    "\n",
    "# Train model (Note: This cell will take some time to execute)\n",
    "# You can change the instance_type parameter to use a different instance type\n",
    "instance_type = 'ml.m5.4xlarge'  # You can change this to a smaller instance if needed\n",
    "xgb_estimator = train_model(train_s3_path, val_s3_path, role, bucket, instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model to SageMaker Endpoint\n",
    "\n",
    "Deploy the trained model to a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "def deploy_model(xgb_estimator, instance_type='ml.m5.xlarge'):\n",
    "    \"\"\"Deploy model to SageMaker endpoint.\"\"\"\n",
    "    print(\"Deploying model to SageMaker endpoint...\")\n",
    "    predictor = xgb_estimator.deploy(\n",
    "        endpoint_name=\"ml-models-as-tools\",\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        serializer=JSONSerializer(),\n",
    "        deserializer=JSONDeserializer()\n",
    "    )\n",
    "    \n",
    "    endpoint_name = predictor.endpoint_name\n",
    "    print(f\"Model deployed to endpoint: {endpoint_name}\")\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "# Deploy model\n",
    "# You can change the instance_type parameter to use a different instance type\n",
    "deploy_instance_type = 'ml.m5.xlarge'  # You can change this to a smaller instance if needed\n",
    "predictor = deploy_model(xgb_estimator, deploy_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Endpoint\n",
    "\n",
    "Test the deployed endpoint with sample data from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_sample = X_test.iloc[:5].values.tolist()\n",
    "actual = y_test.iloc[:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, json\n",
    "\n",
    "def generate_prediction_with_boto3(endpoint_name, test_sample):\n",
    "    sagemaker_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(test_sample),\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\"\n",
    "    )\n",
    "    predictions = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    return np.array(predictions)\n",
    "\n",
    "generate_prediction_with_boto3(\"ml-models-as-tools\", test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_endpoint(predictor, test_sample, actual):\n",
    "    \"\"\"Test the endpoint with sample data.\"\"\"\n",
    "\n",
    "    # Make predictions\n",
    "    print(\"Testing the endpoint with sample data...\")\n",
    "    predictions = predictor.predict(test_sample)\n",
    "    predicted = np.array(predictions)\n",
    "\n",
    "    # Compare predictions with actual values\n",
    "    print(\"Sample predictions:\")\n",
    "    for i in range(len(actual)):\n",
    "        print(f\"Actual: {actual[i]:.2f}, Predicted: {predicted[i]:.2f}\")\n",
    "\n",
    "    # Calculate error metrics\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
    "    print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "    print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "    \n",
    "    return mae, rmse\n",
    "\n",
    "# Test endpoint\n",
    "mae, rmse = test_endpoint(predictor, test_sample, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions\n",
    "\n",
    "Let's visualize the predictions against actual values for a larger portion of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for more test samples\n",
    "n_samples = 50  # Number of samples to predict\n",
    "test_samples = X_test.iloc[:n_samples].values.tolist()\n",
    "predictions = predictor.predict(test_samples)\n",
    "actual = y_test.iloc[:n_samples].values\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(actual)), actual, label='Actual')\n",
    "plt.plot(range(len(predictions)), predictions, label='Predicted')\n",
    "plt.title('Actual vs Predicted Demand')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Demand')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Resources (Optional)\n",
    "\n",
    "Delete the endpoint to avoid incurring charges when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to delete the endpoint\n",
    "#print(\"Cleaning up: deleting endpoint...\")\n",
    "#predictor.delete_endpoint()\n",
    "#print(\"Endpoint deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Generate synthetic demand data with trend, seasonality, and noise\n",
    "2. Create time series features using lag and rolling window techniques\n",
    "3. Train an XGBoost model on Amazon SageMaker\n",
    "4. Deploy the model to a SageMaker endpoint\n",
    "5. Test the endpoint with real-time predictions\n",
    "\n",
    "This workflow can be adapted for real-world demand forecasting applications by replacing the synthetic data with actual historical demand data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
