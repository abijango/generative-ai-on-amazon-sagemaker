{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbdcdb7f-236e-49de-a551-dc9d8121b10a",
   "metadata": {},
   "source": [
    "# Agentic Way of Creating Automated Python Code Using Strands Agents SDK\n",
    "\n",
    "This notebook demonstrates how to leverage the **Strands Agents SDK** to generate and execute automated Python code across a diverse range of tasks — from simple algorithms like the Fibonacci sequence to more complex workflows involving data manipulation, web scraping, financial analysis, big data processing with PySpark, and building machine learning pipelines.\n",
    "\n",
    "Each example showcases how an intelligent agent can assist in writing clean, well-documented Python scripts that include inline comments, error handling, and best coding practices for various real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a7726-ec2b-4f1e-8331-2a8bbeee1f6a",
   "metadata": {},
   "source": [
    "### Suppress Warnings for Cleaner Output  \n",
    "This cell imports the `warnings` module and suppresses warnings to ensure the notebook output remains clean and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51b02e5-bb7a-41b0-835a-282afebd2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd5000-f235-4292-9f01-bd6c6fbe7bf2",
   "metadata": {},
   "source": [
    "### Install OpenJDK 11 via Conda  \n",
    "Installs OpenJDK 11 in the notebook environment to support Java-dependent libraries and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b28847-3f24-4ac1-8a28-fe51b5c44f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - file:///tmp/patched-packages\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 25.1.1\n",
      "    latest version: 25.3.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk=11\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    alsa-lib-1.2.14            |       hb9d3cd8_0         553 KB  conda-forge\n",
      "    ca-certificates-2025.4.26  |       hbd8a1cb_0         149 KB  conda-forge\n",
      "    certifi-2025.4.26          |     pyhd8ed1ab_0         154 KB  conda-forge\n",
      "    openjdk-11.0.26            |       he4ca013_1       164.0 MB  conda-forge\n",
      "    openssl-3.5.0              |       h7b32b05_1         3.0 MB  conda-forge\n",
      "    xorg-libxt-1.3.1           |       hb9d3cd8_0         371 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       168.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  alsa-lib           conda-forge/linux-64::alsa-lib-1.2.14-hb9d3cd8_0 \n",
      "  openjdk            conda-forge/linux-64::openjdk-11.0.26-he4ca013_1 \n",
      "  xorg-libxt         conda-forge/linux-64::xorg-libxt-1.3.1-hb9d3cd8_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge/linux-64::ca-certificates~ --> conda-forge/noarch::ca-certificates-2025.4.26-hbd8a1cb_0 \n",
      "  certifi                            2025.1.31-pyhd8ed1ab_0 --> 2025.4.26-pyhd8ed1ab_0 \n",
      "  openssl                                  3.4.1-h7b32b05_0 --> 3.5.0-h7b32b05_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "openjdk-11.0.26      | 164.0 MB  |                                       |   0% \n",
      "openssl-3.5.0        | 3.0 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "alsa-lib-1.2.14      | 553 KB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "xorg-libxt-1.3.1     | 371 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.4.26    | 154 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.4.26    | 154 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "alsa-lib-1.2.14      | 553 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "openjdk-11.0.26      | 164.0 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.5.0        | 3.0 MB    | ############2                         |  33% \u001b[A\n",
      "\n",
      "\n",
      "xorg-libxt-1.3.1     | 371 KB    | #5                                    |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    | ###9                                  |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "xorg-libxt-1.3.1     | 371 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ca-certificates-2025 | 149 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openjdk-11.0.26      | 164.0 MB  | 9                                     |   3% \u001b[A\n",
      "\n",
      "\n",
      "xorg-libxt-1.3.1     | 371 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "xorg-libxt-1.3.1     | 371 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "alsa-lib-1.2.14      | 553 KB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "openjdk-11.0.26      | 164.0 MB  | ###5                                  |  10% \u001b[A\u001b[A\n",
      "openssl-3.5.0        | 3.0 MB    | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# Run this in a notebook cell to install OpenJDK 11\n",
    "!conda install -c conda-forge openjdk=11 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecc9a86-8f14-4e3e-bc58-ce6583358a65",
   "metadata": {},
   "source": [
    "### Install Strands Agents Libraries  \n",
    "Installs `strands-agents` and related tools for creating Python agents and tools that can run and evaluate Python code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f2a6af-2b86-43be-81c8-87c9d6909602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting strands-agents\n",
      "  Downloading strands_agents-0.1.2-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting strands-agents-tools\n",
      "  Downloading strands_agents_tools-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting strands-agents-builder\n",
      "  Downloading strands_agents_builder-0.1.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: nest_asyncio in /opt/conda/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents) (1.37.1)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.29.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents) (1.37.1)\n",
      "Collecting docstring-parser<0.16.0,>=0.15 (from strands-agents)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting mcp<2.0.0,>=1.8.0 (from strands-agents)\n",
      "  Downloading mcp-1.9.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting opentelemetry-api<2.0.0,>=1.33.0 (from strands-agents)\n",
      "  Downloading opentelemetry_api-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.0 (from strands-agents)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-sdk<2.0.0,>=1.33.0 (from strands-agents)\n",
      "  Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents) (2.10.6)\n",
      "Collecting typing-extensions<5.0.0,>=4.13.2 (from strands-agents)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting watchdog<7.0.0,>=6.0.0 (from strands-agents)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Collecting aws-requests-auth<0.5.0,>=0.4.3 (from strands-agents-tools)\n",
      "  Downloading aws_requests_auth-0.4.3-py2.py3-none-any.whl.metadata (567 bytes)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.6 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools) (0.4.6)\n",
      "Collecting dill<0.5.0,>=0.4.0 (from strands-agents-tools)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mem0ai<1.0.0,>=0.1.99 (from strands-agents-tools)\n",
      "  Downloading mem0ai-0.1.100-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: opensearch-py<3.0.0,>=2.8.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools) (2.8.0)\n",
      "Collecting pillow<12.0.0,>=11.2.1 (from strands-agents-tools)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting prompt-toolkit<4.0.0,>=3.0.51 (from strands-agents-tools)\n",
      "  Downloading prompt_toolkit-3.0.51-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools) (2.10.1)\n",
      "Collecting rich<15.0.0,>=14.0.0 (from strands-agents-tools)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting slack-bolt<2.0.0,>=1.23.0 (from strands-agents-tools)\n",
      "  Downloading slack_bolt-1.23.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sympy<2.0.0,>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from strands-agents-tools) (1.13.3)\n",
      "Collecting tenacity<10.0.0,>=9.1.2 (from strands-agents-tools)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting halo<1.0.0,>=0.0.31 (from strands-agents-builder)\n",
      "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools) (2.32.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3<2.0.0,>=1.26.0->strands-agents) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<2.0.0,>=1.29.0->strands-agents) (2.3.0)\n",
      "Collecting log_symbols>=0.0.14 (from halo<1.0.0,>=0.0.31->strands-agents-builder)\n",
      "  Downloading log_symbols-0.0.14-py3-none-any.whl.metadata (523 bytes)\n",
      "Collecting spinners>=0.0.24 (from halo<1.0.0,>=0.0.31->strands-agents-builder)\n",
      "  Downloading spinners-0.0.24-py3-none-any.whl.metadata (576 bytes)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from halo<1.0.0,>=0.0.31->strands-agents-builder) (2.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from halo<1.0.0,>=0.0.31->strands-agents-builder) (1.17.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.4.0)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (2.8.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (2.3.4)\n",
      "Requirement already satisfied: starlette>=0.27 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.46.1)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /opt/conda/lib/python3.12/site-packages (from mcp<2.0.0,>=1.8.0->strands-agents) (0.34.0)\n",
      "Collecting openai<2.0.0,>=1.33.0 (from mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading openai-1.79.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting posthog<4.0.0,>=3.5.0 (from mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pytz<2025.0,>=2024.1 in /opt/conda/lib/python3.12/site-packages (from mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (2024.1)\n",
      "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.31 in /opt/conda/lib/python3.12/site-packages (from mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (2.0.39)\n",
      "Requirement already satisfied: certifi>=2024.07.04 in /opt/conda/lib/python3.12/site-packages (from opensearch-py<3.0.0,>=2.8.0->strands-agents-tools) (2025.4.26)\n",
      "Requirement already satisfied: Events in /opt/conda/lib/python3.12/site-packages (from opensearch-py<3.0.0,>=2.8.0->strands-agents-tools) (0.5)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.0->strands-agents) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.33.0->strands-agents) (6.10.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.0->strands-agents) (1.69.2)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.0->strands-agents)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.1 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.0->strands-agents)\n",
      "  Downloading opentelemetry_proto-1.33.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from opentelemetry-proto==1.33.1->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.0->strands-agents) (5.28.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b1 (from opentelemetry-sdk<2.0.0,>=1.33.0->strands-agents)\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.51->strands-agents-tools) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->strands-agents) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich<15.0.0,>=14.0.0->strands-agents-tools) (2.19.1)\n",
      "Collecting slack_sdk<4,>=3.35.0 (from slack-bolt<2.0.0,>=1.23.0->strands-agents-tools)\n",
      "  Downloading slack_sdk-3.35.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting ollama<1.0.0,>=0.4.8 (from strands-agents[ollama]<1.0.0,>=0.1.0->strands-agents-builder)\n",
      "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy<2.0.0,>=1.12.0->strands-agents-tools) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.8.0->strands-agents) (1.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<2.0.0,>=1.33.0->strands-agents) (1.17.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.8.0->strands-agents) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2.0.0,>=1.8.0->strands-agents) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.0->strands-agents) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=14.0.0->strands-agents-tools) (0.1.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from openai<2.0.0,>=1.33.0->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.33.0->mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.12/site-packages (from openai<2.0.0,>=1.33.0->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (4.67.1)\n",
      "Collecting monotonic>=1.5 (from posthog<4.0.0,>=3.5.0->mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog<4.0.0,>=3.5.0->mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.8.0->strands-agents) (1.0.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /opt/conda/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (1.67.1)\n",
      "Requirement already satisfied: numpy>=1.26 in /opt/conda/lib/python3.12/site-packages (from qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (1.26.4)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools)\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=0.14.0->aws-requests-auth<0.5.0,>=0.4.3->strands-agents-tools) (3.4.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.12/site-packages (from sqlalchemy<3.0.0,>=2.0.31->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (3.1.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp<2.0.0,>=1.8.0->strands-agents) (8.1.8)\n",
      "Requirement already satisfied: h2<5,>=3 in /opt/conda/lib/python3.12/site-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /opt/conda/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /opt/conda/lib/python3.12/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<1.0.0,>=0.1.99->strands-agents-tools) (4.1.0)\n",
      "Downloading strands_agents-0.1.2-py3-none-any.whl (100 kB)\n",
      "Downloading strands_agents_tools-0.1.1-py3-none-any.whl (153 kB)\n",
      "Downloading strands_agents_builder-0.1.1-py3-none-any.whl (26 kB)\n",
      "Downloading aws_requests_auth-0.4.3-py2.py3-none-any.whl (6.8 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Downloading mcp-1.9.0-py3-none-any.whl (125 kB)\n",
      "Downloading mem0ai-0.1.100-py3-none-any.whl (151 kB)\n",
      "Downloading opentelemetry_api-1.33.1-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.33.1-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.1-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_sdk-1.33.1-py3-none-any.whl (118 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b1-py3-none-any.whl (194 kB)\n",
      "Downloading pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prompt_toolkit-3.0.51-py3-none-any.whl (387 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading slack_bolt-1.23.0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
      "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
      "Downloading openai-1.79.0-py3-none-any.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
      "Downloading qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n",
      "Downloading slack_sdk-3.35.0-py2.py3-none-any.whl (293 kB)\n",
      "Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: halo\n",
      "  Building wheel for halo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11307 sha256=fc42f9fef7a0993cb26d16bd3ffdab54a294c6ece3a97e478c728526454da05f\n",
      "  Stored in directory: /home/sagemaker-user/.cache/pip/wheels/0b/b2/f8/376708b4c66e68a1a7d40aaba26521645e7f5ad3b9ef756031\n",
      "Successfully built halo\n",
      "Installing collected packages: spinners, monotonic, watchdog, typing-extensions, tenacity, slack_sdk, prompt-toolkit, portalocker, pillow, opentelemetry-proto, log_symbols, jiter, docstring-parser, dill, backoff, slack-bolt, rich, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, halo, aws-requests-auth, opentelemetry-semantic-conventions, opentelemetry-sdk, openai, ollama, qdrant-client, opentelemetry-exporter-otlp-proto-http, mcp, strands-agents, mem0ai, strands-agents-tools, strands-agents-builder\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: prompt-toolkit\n",
      "    Found existing installation: prompt_toolkit 3.0.50\n",
      "    Uninstalling prompt_toolkit-3.0.50:\n",
      "      Successfully uninstalled prompt_toolkit-3.0.50\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 11.1.0\n",
      "    Uninstalling pillow-11.1.0:\n",
      "      Successfully uninstalled pillow-11.1.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.9.4\n",
      "    Uninstalling rich-13.9.4:\n",
      "      Successfully uninstalled rich-13.9.4\n",
      "  Attempting uninstall: opentelemetry-api\n",
      "    Found existing installation: opentelemetry-api 1.31.0\n",
      "    Uninstalling opentelemetry-api-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-api-1.31.0\n",
      "  Attempting uninstall: opentelemetry-semantic-conventions\n",
      "    Found existing installation: opentelemetry-semantic-conventions 0.52b0\n",
      "    Uninstalling opentelemetry-semantic-conventions-0.52b0:\n",
      "      Successfully uninstalled opentelemetry-semantic-conventions-0.52b0\n",
      "  Attempting uninstall: opentelemetry-sdk\n",
      "    Found existing installation: opentelemetry-sdk 1.31.0\n",
      "    Uninstalling opentelemetry-sdk-1.31.0:\n",
      "      Successfully uninstalled opentelemetry-sdk-1.31.0\n",
      "  Attempting uninstall: mcp\n",
      "    Found existing installation: mcp 1.7.1\n",
      "    Uninstalling mcp-1.7.1:\n",
      "      Successfully uninstalled mcp-1.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.0 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.3 which is incompatible.\n",
      "sagemaker-core 1.0.25 requires rich<14.0.0,>=13.0.0, but you have rich 14.0.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aws-requests-auth-0.4.3 backoff-2.2.1 dill-0.4.0 docstring-parser-0.15 halo-0.0.31 jiter-0.10.0 log_symbols-0.0.14 mcp-1.9.0 mem0ai-0.1.100 monotonic-1.6 ollama-0.4.8 openai-1.79.0 opentelemetry-api-1.33.1 opentelemetry-exporter-otlp-proto-common-1.33.1 opentelemetry-exporter-otlp-proto-http-1.33.1 opentelemetry-proto-1.33.1 opentelemetry-sdk-1.33.1 opentelemetry-semantic-conventions-0.54b1 pillow-11.2.1 portalocker-2.10.1 posthog-3.25.0 prompt-toolkit-3.0.51 qdrant-client-1.14.2 rich-14.0.0 slack-bolt-1.23.0 slack_sdk-3.35.0 spinners-0.0.24 strands-agents-0.1.2 strands-agents-builder-0.1.1 strands-agents-tools-0.1.1 tenacity-9.1.2 typing-extensions-4.13.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install strands-agents strands-agents-tools strands-agents-builder nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d88b2c-79d7-439d-8af6-f5d1ee81544c",
   "metadata": {},
   "source": [
    "### Install Data Science Libraries  \n",
    "Installs essential Python libraries including `yfinance`, `pandas`, `numpy`, `matplotlib`, and `pyspark` for financial data analysis, data processing, visualization, and big data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2a504f-8874-4820-9c41-1916e551f961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.61-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/conda/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Using cached peewee-3.18.1-cp312-cp312-linux_x86_64.whl\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/conda/lib/python3.12/site-packages (from yfinance) (4.13.3)\n",
      "Collecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.11.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from yfinance) (5.28.3)\n",
      "Requirement already satisfied: websockets>=13.0 in /opt/conda/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in /opt/conda/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Downloading yfinance-0.2.61-py2.py3-none-any.whl (117 kB)\n",
      "Downloading curl_cffi-0.11.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: peewee, multitasking, curl_cffi, yfinance\n",
      "Successfully installed curl_cffi-0.11.1 multitasking-0.0.11 peewee-3.18.1 yfinance-0.2.61\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance pandas numpy matplotlib pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118ff81-172c-44d5-ae87-f2c0dd84c690",
   "metadata": {},
   "source": [
    "### Define Markdown Display Helper Function  \n",
    "Defines a utility function to display agent responses as formatted Markdown in the notebook, enabling better readability of code and text outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b2f6b9-e7c7-495f-ac8b-51067649cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_code_response(response):\n",
    "    \"\"\"Display agent response as formatted markdown with syntax highlighting.\"\"\"\n",
    "    # Extract content from response\n",
    "    if isinstance(response.message, dict) and \"content\" in response.message:\n",
    "        content = response.message[\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        content = str(response)\n",
    "    \n",
    "    # Display as markdown (which will render code blocks properly)\n",
    "    display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875df9e-515d-4136-8d41-7113154cc0b5",
   "metadata": {},
   "source": [
    "### Agent Initialization and Environment Setup  \n",
    "Sets environment variables, and initializes a Python agent with necessary tools to execute code dynamically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce94097a-c499-4574-adbe-a997b40b1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Input is not a terminal (fd=0).\n",
      "tool=<<function python_repl at 0x7f3e1d67d3a0>> | unrecognized tool specification\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply patch to fix event loop issues\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Set environment variable - MUST be before importing strands\n",
    "os.environ[\"STRANDS_AUTO_EXECUTE_TOOLS\"] = \"true\"\n",
    "\n",
    "# Import the function directly from the module\n",
    "from strands_tools.python_repl import python_repl\n",
    "from strands import Agent, tool\n",
    "\n",
    "# Create agent with the function (not the module)\n",
    "agent = Agent(tools=[python_repl])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ccaff-9d93-4a59-a6d8-e24e2b6a1795",
   "metadata": {},
   "source": [
    "### Fibonacci Sequence Generator Function  \n",
    "Generates a well-documented Python function to compute the Fibonacci sequence up to a given number of terms with inline comments and usage examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686558ed-f967-4b5e-b04c-9455a0a92c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fibonacci Sequence Generator\n",
      "\n",
      "This Python module contains a function to generate the Fibonacci sequence up to a specified number of terms.\n",
      "\n",
      "## Function Documentation\n",
      "\n",
      "```python\n",
      "def generate_fibonacci(n):\n",
      "    \"\"\"\n",
      "    Generate a Fibonacci sequence up to n terms.\n",
      "    \n",
      "    The Fibonacci sequence is a series of numbers where each number is the sum\n",
      "    of the two preceding ones, usually starting with 0 and 1.\n",
      "    \n",
      "    Parameters:\n",
      "        n (int): The number of Fibonacci terms to generate (must be a positive integer)\n",
      "    \n",
      "    Returns:\n",
      "        list: A list containing the first n Fibonacci numbers\n",
      "        \n",
      "    Raises:\n",
      "        ValueError: If n is not a positive integer\n",
      "    \"\"\"\n",
      "    # Validate input\n",
      "    if not isinstance(n, int) or n <= 0:\n",
      "        raise ValueError(\"Input must be a positive integer\")\n",
      "    \n",
      "    # Initialize the sequence with the first two Fibonacci numbers\n",
      "    fibonacci_sequence = [0, 1]\n",
      "    \n",
      "    # If n is 1, return only the first element\n",
      "    if n == 1:\n",
      "        return [0]\n",
      "    \n",
      "    # Generate the remaining Fibonacci numbers up to n\n",
      "    for i in range(2, n):\n",
      "        # Next number is the sum of the previous two numbers\n",
      "        next_number = fibonacci_sequence[i-1] + fibonacci_sequence[i-2]\n",
      "        # Add the new number to the sequence\n",
      "        fibonacci_sequence.append(next_number)\n",
      "    \n",
      "    return fibonacci_sequence\n",
      "```\n",
      "\n",
      "## Example Usage\n",
      "\n",
      "```python\n",
      "# Example of how to use the generate_fibonacci function\n",
      "if __name__ == \"__main__\":\n",
      "    # Generate the first 10 Fibonacci numbers\n",
      "    n_terms = 10\n",
      "    fibonacci_numbers = generate_fibonacci(n_terms)\n",
      "    \n",
      "    # Print the results\n",
      "    print(f\"Fibonacci sequence with {n_terms} terms:\")\n",
      "    print(fibonacci_numbers)\n",
      "    \n",
      "    # Print each number with its position\n",
      "    for i, num in enumerate(fibonacci_numbers):\n",
      "        print(f\"F({i}) = {num}\")\n",
      "```\n",
      "\n",
      "## External Libraries\n",
      "\n",
      "This function doesn't require any external libraries and uses only Python's built-in features.\n",
      "\n",
      "## How It Works\n",
      "\n",
      "1. The function first validates that the input is a positive integer.\n",
      "2. It initializes the sequence with the first two Fibonacci numbers (0 and 1).\n",
      "3. For each subsequent position, it calculates the next Fibonacci number by adding the previous two numbers.\n",
      "4. The function returns the complete sequence as a list.\n",
      "\n",
      "## How to Run\n",
      "\n",
      "1. Save the code to a file, e.g., `fibonacci.py`\n",
      "2. Run the file using Python:\n",
      "   ```\n",
      "   python fibonacci.py\n",
      "   ```\n",
      "3. To use in another Python file:\n",
      "   ```python\n",
      "   from fibonacci import generate_fibonacci\n",
      "   \n",
      "   # Get first 15 Fibonacci numbers\n",
      "   fib_sequence = generate_fibonacci(15)\n",
      "   print(fib_sequence)\n",
      "   ```"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Fibonacci Sequence Generator\n",
       "\n",
       "This Python module contains a function to generate the Fibonacci sequence up to a specified number of terms.\n",
       "\n",
       "## Function Documentation\n",
       "\n",
       "```python\n",
       "def generate_fibonacci(n):\n",
       "    \"\"\"\n",
       "    Generate a Fibonacci sequence up to n terms.\n",
       "    \n",
       "    The Fibonacci sequence is a series of numbers where each number is the sum\n",
       "    of the two preceding ones, usually starting with 0 and 1.\n",
       "    \n",
       "    Parameters:\n",
       "        n (int): The number of Fibonacci terms to generate (must be a positive integer)\n",
       "    \n",
       "    Returns:\n",
       "        list: A list containing the first n Fibonacci numbers\n",
       "        \n",
       "    Raises:\n",
       "        ValueError: If n is not a positive integer\n",
       "    \"\"\"\n",
       "    # Validate input\n",
       "    if not isinstance(n, int) or n <= 0:\n",
       "        raise ValueError(\"Input must be a positive integer\")\n",
       "    \n",
       "    # Initialize the sequence with the first two Fibonacci numbers\n",
       "    fibonacci_sequence = [0, 1]\n",
       "    \n",
       "    # If n is 1, return only the first element\n",
       "    if n == 1:\n",
       "        return [0]\n",
       "    \n",
       "    # Generate the remaining Fibonacci numbers up to n\n",
       "    for i in range(2, n):\n",
       "        # Next number is the sum of the previous two numbers\n",
       "        next_number = fibonacci_sequence[i-1] + fibonacci_sequence[i-2]\n",
       "        # Add the new number to the sequence\n",
       "        fibonacci_sequence.append(next_number)\n",
       "    \n",
       "    return fibonacci_sequence\n",
       "```\n",
       "\n",
       "## Example Usage\n",
       "\n",
       "```python\n",
       "# Example of how to use the generate_fibonacci function\n",
       "if __name__ == \"__main__\":\n",
       "    # Generate the first 10 Fibonacci numbers\n",
       "    n_terms = 10\n",
       "    fibonacci_numbers = generate_fibonacci(n_terms)\n",
       "    \n",
       "    # Print the results\n",
       "    print(f\"Fibonacci sequence with {n_terms} terms:\")\n",
       "    print(fibonacci_numbers)\n",
       "    \n",
       "    # Print each number with its position\n",
       "    for i, num in enumerate(fibonacci_numbers):\n",
       "        print(f\"F({i}) = {num}\")\n",
       "```\n",
       "\n",
       "## External Libraries\n",
       "\n",
       "This function doesn't require any external libraries and uses only Python's built-in features.\n",
       "\n",
       "## How It Works\n",
       "\n",
       "1. The function first validates that the input is a positive integer.\n",
       "2. It initializes the sequence with the first two Fibonacci numbers (0 and 1).\n",
       "3. For each subsequent position, it calculates the next Fibonacci number by adding the previous two numbers.\n",
       "4. The function returns the complete sequence as a list.\n",
       "\n",
       "## How to Run\n",
       "\n",
       "1. Save the code to a file, e.g., `fibonacci.py`\n",
       "2. Run the file using Python:\n",
       "   ```\n",
       "   python fibonacci.py\n",
       "   ```\n",
       "3. To use in another Python file:\n",
       "   ```python\n",
       "   from fibonacci import generate_fibonacci\n",
       "   \n",
       "   # Get first 15 Fibonacci numbers\n",
       "   fib_sequence = generate_fibonacci(15)\n",
       "   print(fib_sequence)\n",
       "   ```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a well-documented Python function that generates the Fibonacci sequence up to 'n' terms.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b5635-e3ef-47b8-9448-fa8e627d79ad",
   "metadata": {},
   "source": [
    "### DataFrame Manipulation with pandas  \n",
    "Creates a sample DataFrame, adds computed columns, filters rows based on conditions, and groups data with aggregation, showcasing pandas capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cebdb60-c6ca-48a4-90ad-0e37219324cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Employee Salary Analysis Script\n",
      "\n",
      "This Python script demonstrates various pandas operations including creating a DataFrame, manipulating data, filtering, and grouping. It processes employee data to analyze salaries and bonuses across different age groups.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```\n",
      "pandas\n",
      "```\n",
      "\n",
      "You can install the required library using pip:\n",
      "\n",
      "```bash\n",
      "pip install pandas\n",
      "```\n",
      "\n",
      "## Code Implementation\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "def analyze_employee_data():\n",
      "    \"\"\"\n",
      "    Creates and analyzes employee salary data using pandas.\n",
      "    \n",
      "    This function:\n",
      "    1. Creates a sample DataFrame with employee data\n",
      "    2. Adds a 'Bonus' column calculated as 10% of the 'Salary'\n",
      "    3. Filters employees older than 30\n",
      "    4. Groups data by age brackets and calculates average salary and bonus\n",
      "    \n",
      "    Returns:\n",
      "        tuple: A tuple containing three DataFrames:\n",
      "               - Original DataFrame with bonus column\n",
      "               - Filtered DataFrame (age > 30)\n",
      "               - Age bracket summary with average salary and bonus\n",
      "    \"\"\"\n",
      "    # Create a sample DataFrame with employee data\n",
      "    data = {\n",
      "        'Name': ['John Smith', 'Sarah Johnson', 'Michael Brown', 'Emma Davis', \n",
      "                 'Robert Wilson', 'Jennifer Taylor', 'David Martinez', 'Lisa Anderson',\n",
      "                 'James Thomas', 'Patricia White'],\n",
      "        'Age': [25, 34, 42, 29, 38, 45, 31, 27, 52, 36],\n",
      "        'Salary': [50000, 65000, 78000, 48000, 72000, 85000, 60000, 52000, 90000, 67000]\n",
      "    }\n",
      "    \n",
      "    # Create DataFrame from the dictionary\n",
      "    df = pd.DataFrame(data)\n",
      "    print(\"Original DataFrame:\")\n",
      "    print(df)\n",
      "    print(\"\\n\")\n",
      "    \n",
      "    # Add a new 'Bonus' column calculated as 10% of Salary\n",
      "    df['Bonus'] = df['Salary'] * 0.1\n",
      "    print(\"DataFrame with Bonus column added:\")\n",
      "    print(df)\n",
      "    print(\"\\n\")\n",
      "    \n",
      "    # Filter the DataFrame to include only rows where Age > 30\n",
      "    filtered_df = df[df['Age'] > 30]\n",
      "    print(\"Filtered DataFrame (Age > 30):\")\n",
      "    print(filtered_df)\n",
      "    print(\"\\n\")\n",
      "    \n",
      "    # Create age brackets using pd.cut\n",
      "    # Define the age bins and labels for our age groups\n",
      "    age_bins = [20, 30, 40, 60]\n",
      "    age_labels = ['20s', '30s', '40s and above']\n",
      "    \n",
      "    # Add a new column with the age bracket for each employee\n",
      "    df['Age Bracket'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n",
      "    \n",
      "    # Group by age brackets and calculate the average Salary and Bonus\n",
      "    age_group_summary = df.groupby('Age Bracket').agg({\n",
      "        'Salary': 'mean',\n",
      "        'Bonus': 'mean'\n",
      "    }).reset_index()\n",
      "    \n",
      "    # Format the output to make it more readable\n",
      "    age_group_summary['Salary'] = age_group_summary['Salary'].round(2)\n",
      "    age_group_summary['Bonus'] = age_group_summary['Bonus'].round(2)\n",
      "    \n",
      "    print(\"Summary by Age Bracket:\")\n",
      "    print(age_group_summary)\n",
      "    \n",
      "    return df, filtered_df, age_group_summary\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Main function to execute the analysis and demonstrate the results.\n",
      "    \"\"\"\n",
      "    print(\"Employee Salary Analysis\")\n",
      "    print(\"========================\\n\")\n",
      "    \n",
      "    # Run the analysis\n",
      "    df_with_bonus, filtered_df, age_summary = analyze_employee_data()\n",
      "    \n",
      "    print(\"\\nAnalysis Complete!\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "1. **DataFrame Creation**: The script starts by creating a sample DataFrame with employee data including 'Name', 'Age', and 'Salary' columns.\n",
      "\n",
      "2. **Adding Bonus Column**: It adds a 'Bonus' column that is calculated as 10% of each employee's salary.\n",
      "\n",
      "3. **Filtering Data**: The script filters the DataFrame to show only employees who are older than 30.\n",
      "\n",
      "4. **Age Bracket Grouping**: It creates age brackets (20s, 30s, 40s and above) and assigns each employee to the appropriate bracket.\n",
      "\n",
      "5. **Summary Statistics**: Finally, it groups the data by these age brackets and calculates the average salary and bonus for each group.\n",
      "\n",
      "## Functions\n",
      "\n",
      "- **analyze_employee_data()**: The main function that performs all data operations and returns three DataFrames:\n",
      "  - The original DataFrame with the added 'Bonus' column\n",
      "  - The filtered DataFrame (only employees older than 30)\n",
      "  - The summary DataFrame with average salary and bonus by age bracket\n",
      "\n",
      "- **main()**: Entry point of the script that calls the analysis function and displays the results.\n",
      "\n",
      "## Example Output\n",
      "\n",
      "When you run this script, you should expect to see:\n",
      "\n",
      "1. The original DataFrame\n",
      "2. The DataFrame with the added 'Bonus' column\n",
      "3. The filtered DataFrame (employees over 30)\n",
      "4. A summary table showing the average salary and bonus by age bracket\n",
      "\n",
      "## How to Run\n",
      "\n",
      "1. Save the code to a file, e.g., `employee_analysis.py`\n",
      "2. Ensure pandas is installed: `pip install pandas`\n",
      "3. Run the file using Python: `python employee_analysis.py`\n",
      "\n",
      "## Extending the Script\n",
      "\n",
      "This script can be easily extended by:\n",
      "- Adding more columns (e.g., department, years of service)\n",
      "- Calculating additional metrics (e.g., median values, standard deviations)\n",
      "- Creating visualizations of the data (using matplotlib or seaborn)\n",
      "- Reading data from external sources like CSV files or databases instead of using sample data"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Employee Salary Analysis Script\n",
       "\n",
       "This Python script demonstrates various pandas operations including creating a DataFrame, manipulating data, filtering, and grouping. It processes employee data to analyze salaries and bonuses across different age groups.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```\n",
       "pandas\n",
       "```\n",
       "\n",
       "You can install the required library using pip:\n",
       "\n",
       "```bash\n",
       "pip install pandas\n",
       "```\n",
       "\n",
       "## Code Implementation\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "def analyze_employee_data():\n",
       "    \"\"\"\n",
       "    Creates and analyzes employee salary data using pandas.\n",
       "    \n",
       "    This function:\n",
       "    1. Creates a sample DataFrame with employee data\n",
       "    2. Adds a 'Bonus' column calculated as 10% of the 'Salary'\n",
       "    3. Filters employees older than 30\n",
       "    4. Groups data by age brackets and calculates average salary and bonus\n",
       "    \n",
       "    Returns:\n",
       "        tuple: A tuple containing three DataFrames:\n",
       "               - Original DataFrame with bonus column\n",
       "               - Filtered DataFrame (age > 30)\n",
       "               - Age bracket summary with average salary and bonus\n",
       "    \"\"\"\n",
       "    # Create a sample DataFrame with employee data\n",
       "    data = {\n",
       "        'Name': ['John Smith', 'Sarah Johnson', 'Michael Brown', 'Emma Davis', \n",
       "                 'Robert Wilson', 'Jennifer Taylor', 'David Martinez', 'Lisa Anderson',\n",
       "                 'James Thomas', 'Patricia White'],\n",
       "        'Age': [25, 34, 42, 29, 38, 45, 31, 27, 52, 36],\n",
       "        'Salary': [50000, 65000, 78000, 48000, 72000, 85000, 60000, 52000, 90000, 67000]\n",
       "    }\n",
       "    \n",
       "    # Create DataFrame from the dictionary\n",
       "    df = pd.DataFrame(data)\n",
       "    print(\"Original DataFrame:\")\n",
       "    print(df)\n",
       "    print(\"\\n\")\n",
       "    \n",
       "    # Add a new 'Bonus' column calculated as 10% of Salary\n",
       "    df['Bonus'] = df['Salary'] * 0.1\n",
       "    print(\"DataFrame with Bonus column added:\")\n",
       "    print(df)\n",
       "    print(\"\\n\")\n",
       "    \n",
       "    # Filter the DataFrame to include only rows where Age > 30\n",
       "    filtered_df = df[df['Age'] > 30]\n",
       "    print(\"Filtered DataFrame (Age > 30):\")\n",
       "    print(filtered_df)\n",
       "    print(\"\\n\")\n",
       "    \n",
       "    # Create age brackets using pd.cut\n",
       "    # Define the age bins and labels for our age groups\n",
       "    age_bins = [20, 30, 40, 60]\n",
       "    age_labels = ['20s', '30s', '40s and above']\n",
       "    \n",
       "    # Add a new column with the age bracket for each employee\n",
       "    df['Age Bracket'] = pd.cut(df['Age'], bins=age_bins, labels=age_labels, right=False)\n",
       "    \n",
       "    # Group by age brackets and calculate the average Salary and Bonus\n",
       "    age_group_summary = df.groupby('Age Bracket').agg({\n",
       "        'Salary': 'mean',\n",
       "        'Bonus': 'mean'\n",
       "    }).reset_index()\n",
       "    \n",
       "    # Format the output to make it more readable\n",
       "    age_group_summary['Salary'] = age_group_summary['Salary'].round(2)\n",
       "    age_group_summary['Bonus'] = age_group_summary['Bonus'].round(2)\n",
       "    \n",
       "    print(\"Summary by Age Bracket:\")\n",
       "    print(age_group_summary)\n",
       "    \n",
       "    return df, filtered_df, age_group_summary\n",
       "\n",
       "def main():\n",
       "    \"\"\"\n",
       "    Main function to execute the analysis and demonstrate the results.\n",
       "    \"\"\"\n",
       "    print(\"Employee Salary Analysis\")\n",
       "    print(\"========================\\n\")\n",
       "    \n",
       "    # Run the analysis\n",
       "    df_with_bonus, filtered_df, age_summary = analyze_employee_data()\n",
       "    \n",
       "    print(\"\\nAnalysis Complete!\")\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "1. **DataFrame Creation**: The script starts by creating a sample DataFrame with employee data including 'Name', 'Age', and 'Salary' columns.\n",
       "\n",
       "2. **Adding Bonus Column**: It adds a 'Bonus' column that is calculated as 10% of each employee's salary.\n",
       "\n",
       "3. **Filtering Data**: The script filters the DataFrame to show only employees who are older than 30.\n",
       "\n",
       "4. **Age Bracket Grouping**: It creates age brackets (20s, 30s, 40s and above) and assigns each employee to the appropriate bracket.\n",
       "\n",
       "5. **Summary Statistics**: Finally, it groups the data by these age brackets and calculates the average salary and bonus for each group.\n",
       "\n",
       "## Functions\n",
       "\n",
       "- **analyze_employee_data()**: The main function that performs all data operations and returns three DataFrames:\n",
       "  - The original DataFrame with the added 'Bonus' column\n",
       "  - The filtered DataFrame (only employees older than 30)\n",
       "  - The summary DataFrame with average salary and bonus by age bracket\n",
       "\n",
       "- **main()**: Entry point of the script that calls the analysis function and displays the results.\n",
       "\n",
       "## Example Output\n",
       "\n",
       "When you run this script, you should expect to see:\n",
       "\n",
       "1. The original DataFrame\n",
       "2. The DataFrame with the added 'Bonus' column\n",
       "3. The filtered DataFrame (employees over 30)\n",
       "4. A summary table showing the average salary and bonus by age bracket\n",
       "\n",
       "## How to Run\n",
       "\n",
       "1. Save the code to a file, e.g., `employee_analysis.py`\n",
       "2. Ensure pandas is installed: `pip install pandas`\n",
       "3. Run the file using Python: `python employee_analysis.py`\n",
       "\n",
       "## Extending the Script\n",
       "\n",
       "This script can be easily extended by:\n",
       "- Adding more columns (e.g., department, years of service)\n",
       "- Calculating additional metrics (e.g., median values, standard deviations)\n",
       "- Creating visualizations of the data (using matplotlib or seaborn)\n",
       "- Reading data from external sources like CSV files or databases instead of using sample data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a Python script using the pandas library that performs the following tasks:\n",
    "\n",
    "- Create a sample DataFrame with the columns: 'Name', 'Age', and 'Salary'.\n",
    "- Add a new column named 'Bonus' that is 10% of the corresponding 'Salary' value.\n",
    "- Filter the DataFrame to include only rows where the 'Age' is greater than 30.\n",
    "- Group the data by age brackets (e.g., 20s, 30s, 40s) and calculate the average 'Salary' and 'Bonus' for each group.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5f856-4d8b-46c9-acb1-2aaedd995f20",
   "metadata": {},
   "source": [
    "### Web Scraping Hacker News Titles  \n",
    "Uses `requests` and `BeautifulSoup` to scrape article titles and links from Hacker News, saving results to a CSV file with proper error handling and structured code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01bdf0c5-749f-417d-bff4-82d98f368793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Hacker News Scraper\n",
      "\n",
      "A Python script that scrapes the front page of Hacker News (https://news.ycombinator.com/) to extract article titles and their corresponding links, then saves them to a CSV file.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "This script requires the following external libraries:\n",
      "```\n",
      "requests\n",
      "beautifulsoup4\n",
      "csv\n",
      "```\n",
      "\n",
      "You can install them using pip:\n",
      "```bash\n",
      "pip install requests beautifulsoup4\n",
      "```\n",
      "\n",
      "## Code Implementation\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Hacker News Scraper\n",
      "\n",
      "This script scrapes the Hacker News homepage to extract article titles and links, \n",
      "then saves the data to a CSV file.\n",
      "\"\"\"\n",
      "\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import csv\n",
      "import time\n",
      "from datetime import datetime\n",
      "import os\n",
      "import sys\n",
      "\n",
      "def scrape_hacker_news(url=\"https://news.ycombinator.com/\"):\n",
      "    \"\"\"\n",
      "    Scrapes article titles and links from Hacker News homepage.\n",
      "\n",
      "    Args:\n",
      "        url (str): The URL of the Hacker News homepage. Defaults to \"https://news.ycombinator.com/\".\n",
      "\n",
      "    Returns:\n",
      "        list: A list of dictionaries containing article titles and their URLs.\n",
      "              Each dictionary has 'title' and 'url' keys.\n",
      "              \n",
      "    Raises:\n",
      "        requests.exceptions.RequestException: If there's an error with the HTTP request.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Set user agent to avoid potential blocking\n",
      "        headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
      "        }\n",
      "        \n",
      "        # Send HTTP request with a timeout of 10 seconds\n",
      "        response = requests.get(url, headers=headers, timeout=10)\n",
      "        \n",
      "        # Raise an exception for bad status codes\n",
      "        response.raise_for_status()\n",
      "        \n",
      "        # Parse the HTML content of the page\n",
      "        soup = BeautifulSoup(response.text, 'html.parser')\n",
      "        \n",
      "        # Find all story titles - they are in 'titleline' class in span elements\n",
      "        stories = soup.find_all('span', class_='titleline')\n",
      "        \n",
      "        # List to store results\n",
      "        articles = []\n",
      "        \n",
      "        # Extract title and link for each story\n",
      "        for story in stories:\n",
      "            # Find the first anchor tag which contains the title and link\n",
      "            link = story.find('a')\n",
      "            \n",
      "            # Skip if no link is found (defensive programming)\n",
      "            if not link:\n",
      "                continue\n",
      "                \n",
      "            title = link.get_text()\n",
      "            url = link.get('href')\n",
      "            \n",
      "            # Only add to list if both title and url were found\n",
      "            if title and url:\n",
      "                articles.append({\n",
      "                    'title': title.strip(),\n",
      "                    'url': url\n",
      "                })\n",
      "        \n",
      "        print(f\"Successfully scraped {len(articles)} articles from Hacker News\")\n",
      "        return articles\n",
      "        \n",
      "    except requests.exceptions.Timeout:\n",
      "        print(\"Error: The request timed out. Please try again later.\")\n",
      "        return []\n",
      "    \n",
      "    except requests.exceptions.ConnectionError:\n",
      "        print(\"Error: Connection failed. Check your internet connection.\")\n",
      "        return []\n",
      "        \n",
      "    except requests.exceptions.HTTPError as e:\n",
      "        print(f\"HTTP Error: {e}\")\n",
      "        return []\n",
      "        \n",
      "    except requests.exceptions.RequestException as e:\n",
      "        print(f\"Error during request: {e}\")\n",
      "        return []\n",
      "        \n",
      "    except Exception as e:\n",
      "        print(f\"An unexpected error occurred: {e}\")\n",
      "        return []\n",
      "\n",
      "\n",
      "def save_to_csv(articles, filename=None):\n",
      "    \"\"\"\n",
      "    Saves the scraped articles to a CSV file.\n",
      "\n",
      "    Args:\n",
      "        articles (list): List of dictionaries containing article data.\n",
      "        filename (str, optional): Name of the CSV file. If not provided, \n",
      "                                 a default name with timestamp will be used.\n",
      "\n",
      "    Returns:\n",
      "        str: Path to the saved CSV file.\n",
      "    \"\"\"\n",
      "    if not articles:\n",
      "        print(\"No articles to save.\")\n",
      "        return None\n",
      "        \n",
      "    # Create a filename with timestamp if not provided\n",
      "    if filename is None:\n",
      "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
      "        filename = f\"hacker_news_{timestamp}.csv\"\n",
      "    \n",
      "    try:\n",
      "        # Write articles to CSV file\n",
      "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
      "            # Define CSV column headers\n",
      "            fieldnames = ['title', 'url']\n",
      "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
      "            \n",
      "            # Write the header row\n",
      "            writer.writeheader()\n",
      "            \n",
      "            # Write all article data\n",
      "            for article in articles:\n",
      "                writer.writerow(article)\n",
      "                \n",
      "        print(f\"Data successfully saved to {filename}\")\n",
      "        return filename\n",
      "        \n",
      "    except IOError as e:\n",
      "        print(f\"Error saving to CSV: {e}\")\n",
      "        return None\n",
      "    except Exception as e:\n",
      "        print(f\"Unexpected error while saving CSV: {e}\")\n",
      "        return None\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Main function that orchestrates the scraping process and handles errors.\n",
      "    \"\"\"\n",
      "    print(\"Starting Hacker News scraper...\")\n",
      "    \n",
      "    # Maximum number of retry attempts\n",
      "    max_retries = 3\n",
      "    retry_count = 0\n",
      "    \n",
      "    while retry_count < max_retries:\n",
      "        try:\n",
      "            # Attempt to scrape Hacker News\n",
      "            articles = scrape_hacker_news()\n",
      "            \n",
      "            if articles:\n",
      "                # Save the scraped data to CSV\n",
      "                saved_file = save_to_csv(articles)\n",
      "                if saved_file:\n",
      "                    print(f\"Scraping complete! Data saved to {saved_file}\")\n",
      "                    return 0\n",
      "                else:\n",
      "                    print(\"Failed to save data.\")\n",
      "                    return 1\n",
      "            else:\n",
      "                print(\"No articles were scraped. Trying again...\")\n",
      "                retry_count += 1\n",
      "                time.sleep(2)  # Wait 2 seconds before retrying\n",
      "                \n",
      "        except Exception as e:\n",
      "            print(f\"Error in main execution: {e}\")\n",
      "            retry_count += 1\n",
      "            time.sleep(2)  # Wait 2 seconds before retrying\n",
      "    \n",
      "    print(f\"Failed to scrape data after {max_retries} attempts.\")\n",
      "    return 1\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Execute main function only if script is run directly\n",
      "    sys.exit(main())\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "### Script Overview\n",
      "\n",
      "This script is designed to:\n",
      "1. Scrape the Hacker News homepage\n",
      "2. Extract article titles and their URLs\n",
      "3. Save this data to a CSV file\n",
      "\n",
      "### Key Functions\n",
      "\n",
      "1. **scrape_hacker_news(url)**\n",
      "   - Sends an HTTP request to the specified URL (defaults to Hacker News homepage)\n",
      "   - Parses the HTML using BeautifulSoup to extract article titles and links\n",
      "   - Returns a list of dictionaries, each containing a title and URL\n",
      "   - Implements comprehensive error handling for various request failures\n",
      "\n",
      "2. **save_to_csv(articles, filename)**\n",
      "   - Takes the list of articles and writes them to a CSV file\n",
      "   - If no filename is provided, creates one with a timestamp\n",
      "   - Returns the path to the saved file or None if there was an error\n",
      "\n",
      "3. **main()**\n",
      "   - Orchestrates the entire scraping process\n",
      "   - Implements retry logic if scraping fails\n",
      "   - Returns appropriate exit codes based on success/failure\n",
      "\n",
      "### Error Handling\n",
      "\n",
      "The script includes robust error handling for:\n",
      "- Connection errors\n",
      "- Timeouts\n",
      "- HTTP errors (e.g., 404, 500)\n",
      "- File I/O errors\n",
      "- Unexpected exceptions\n",
      "\n",
      "It also implements a retry mechanism to attempt the scraping multiple times if initial attempts fail.\n",
      "\n",
      "## How to Run\n",
      "\n",
      "1. Save the code as `hacker_news_scraper.py`\n",
      "2. Install the required libraries:\n",
      "   ```\n",
      "   pip install requests beautifulsoup4\n",
      "   ```\n",
      "3. Run the script:\n",
      "   ```\n",
      "   python hacker_news_scraper.py\n",
      "   ```\n",
      "\n",
      "## Output\n",
      "\n",
      "The script will:\n",
      "1. Print status messages to the console during execution\n",
      "2. Create a CSV file in the current directory with a name like `hacker_news_20230615_123045.csv`\n",
      "\n",
      "The CSV file will contain two columns:\n",
      "- `title`: The article title\n",
      "- `url`: The link to the article\n",
      "\n",
      "## Notes\n",
      "\n",
      "- This script respects website owners by setting a proper user agent\n",
      "- It implements timeouts to avoid hanging if the website is slow to respond\n",
      "- The code includes appropriate pauses between retry attempts to avoid overloading the server\n",
      "- Defensive programming techniques are used to handle unexpected HTML structure changes\n",
      "\n",
      "## Ethical Considerations\n",
      "\n",
      "Be aware that web scraping may be against the terms of service of some websites. Always:\n",
      "1. Check the website's robots.txt file\n",
      "2. Don't overload the server with too many requests\n",
      "3. Consider reaching out to the site owner for permission or to check if an API is available"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Hacker News Scraper\n",
       "\n",
       "A Python script that scrapes the front page of Hacker News (https://news.ycombinator.com/) to extract article titles and their corresponding links, then saves them to a CSV file.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "This script requires the following external libraries:\n",
       "```\n",
       "requests\n",
       "beautifulsoup4\n",
       "csv\n",
       "```\n",
       "\n",
       "You can install them using pip:\n",
       "```bash\n",
       "pip install requests beautifulsoup4\n",
       "```\n",
       "\n",
       "## Code Implementation\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "Hacker News Scraper\n",
       "\n",
       "This script scrapes the Hacker News homepage to extract article titles and links, \n",
       "then saves the data to a CSV file.\n",
       "\"\"\"\n",
       "\n",
       "import requests\n",
       "from bs4 import BeautifulSoup\n",
       "import csv\n",
       "import time\n",
       "from datetime import datetime\n",
       "import os\n",
       "import sys\n",
       "\n",
       "def scrape_hacker_news(url=\"https://news.ycombinator.com/\"):\n",
       "    \"\"\"\n",
       "    Scrapes article titles and links from Hacker News homepage.\n",
       "\n",
       "    Args:\n",
       "        url (str): The URL of the Hacker News homepage. Defaults to \"https://news.ycombinator.com/\".\n",
       "\n",
       "    Returns:\n",
       "        list: A list of dictionaries containing article titles and their URLs.\n",
       "              Each dictionary has 'title' and 'url' keys.\n",
       "              \n",
       "    Raises:\n",
       "        requests.exceptions.RequestException: If there's an error with the HTTP request.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Set user agent to avoid potential blocking\n",
       "        headers = {\n",
       "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
       "        }\n",
       "        \n",
       "        # Send HTTP request with a timeout of 10 seconds\n",
       "        response = requests.get(url, headers=headers, timeout=10)\n",
       "        \n",
       "        # Raise an exception for bad status codes\n",
       "        response.raise_for_status()\n",
       "        \n",
       "        # Parse the HTML content of the page\n",
       "        soup = BeautifulSoup(response.text, 'html.parser')\n",
       "        \n",
       "        # Find all story titles - they are in 'titleline' class in span elements\n",
       "        stories = soup.find_all('span', class_='titleline')\n",
       "        \n",
       "        # List to store results\n",
       "        articles = []\n",
       "        \n",
       "        # Extract title and link for each story\n",
       "        for story in stories:\n",
       "            # Find the first anchor tag which contains the title and link\n",
       "            link = story.find('a')\n",
       "            \n",
       "            # Skip if no link is found (defensive programming)\n",
       "            if not link:\n",
       "                continue\n",
       "                \n",
       "            title = link.get_text()\n",
       "            url = link.get('href')\n",
       "            \n",
       "            # Only add to list if both title and url were found\n",
       "            if title and url:\n",
       "                articles.append({\n",
       "                    'title': title.strip(),\n",
       "                    'url': url\n",
       "                })\n",
       "        \n",
       "        print(f\"Successfully scraped {len(articles)} articles from Hacker News\")\n",
       "        return articles\n",
       "        \n",
       "    except requests.exceptions.Timeout:\n",
       "        print(\"Error: The request timed out. Please try again later.\")\n",
       "        return []\n",
       "    \n",
       "    except requests.exceptions.ConnectionError:\n",
       "        print(\"Error: Connection failed. Check your internet connection.\")\n",
       "        return []\n",
       "        \n",
       "    except requests.exceptions.HTTPError as e:\n",
       "        print(f\"HTTP Error: {e}\")\n",
       "        return []\n",
       "        \n",
       "    except requests.exceptions.RequestException as e:\n",
       "        print(f\"Error during request: {e}\")\n",
       "        return []\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"An unexpected error occurred: {e}\")\n",
       "        return []\n",
       "\n",
       "\n",
       "def save_to_csv(articles, filename=None):\n",
       "    \"\"\"\n",
       "    Saves the scraped articles to a CSV file.\n",
       "\n",
       "    Args:\n",
       "        articles (list): List of dictionaries containing article data.\n",
       "        filename (str, optional): Name of the CSV file. If not provided, \n",
       "                                 a default name with timestamp will be used.\n",
       "\n",
       "    Returns:\n",
       "        str: Path to the saved CSV file.\n",
       "    \"\"\"\n",
       "    if not articles:\n",
       "        print(\"No articles to save.\")\n",
       "        return None\n",
       "        \n",
       "    # Create a filename with timestamp if not provided\n",
       "    if filename is None:\n",
       "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
       "        filename = f\"hacker_news_{timestamp}.csv\"\n",
       "    \n",
       "    try:\n",
       "        # Write articles to CSV file\n",
       "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
       "            # Define CSV column headers\n",
       "            fieldnames = ['title', 'url']\n",
       "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
       "            \n",
       "            # Write the header row\n",
       "            writer.writeheader()\n",
       "            \n",
       "            # Write all article data\n",
       "            for article in articles:\n",
       "                writer.writerow(article)\n",
       "                \n",
       "        print(f\"Data successfully saved to {filename}\")\n",
       "        return filename\n",
       "        \n",
       "    except IOError as e:\n",
       "        print(f\"Error saving to CSV: {e}\")\n",
       "        return None\n",
       "    except Exception as e:\n",
       "        print(f\"Unexpected error while saving CSV: {e}\")\n",
       "        return None\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"\n",
       "    Main function that orchestrates the scraping process and handles errors.\n",
       "    \"\"\"\n",
       "    print(\"Starting Hacker News scraper...\")\n",
       "    \n",
       "    # Maximum number of retry attempts\n",
       "    max_retries = 3\n",
       "    retry_count = 0\n",
       "    \n",
       "    while retry_count < max_retries:\n",
       "        try:\n",
       "            # Attempt to scrape Hacker News\n",
       "            articles = scrape_hacker_news()\n",
       "            \n",
       "            if articles:\n",
       "                # Save the scraped data to CSV\n",
       "                saved_file = save_to_csv(articles)\n",
       "                if saved_file:\n",
       "                    print(f\"Scraping complete! Data saved to {saved_file}\")\n",
       "                    return 0\n",
       "                else:\n",
       "                    print(\"Failed to save data.\")\n",
       "                    return 1\n",
       "            else:\n",
       "                print(\"No articles were scraped. Trying again...\")\n",
       "                retry_count += 1\n",
       "                time.sleep(2)  # Wait 2 seconds before retrying\n",
       "                \n",
       "        except Exception as e:\n",
       "            print(f\"Error in main execution: {e}\")\n",
       "            retry_count += 1\n",
       "            time.sleep(2)  # Wait 2 seconds before retrying\n",
       "    \n",
       "    print(f\"Failed to scrape data after {max_retries} attempts.\")\n",
       "    return 1\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    # Execute main function only if script is run directly\n",
       "    sys.exit(main())\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "### Script Overview\n",
       "\n",
       "This script is designed to:\n",
       "1. Scrape the Hacker News homepage\n",
       "2. Extract article titles and their URLs\n",
       "3. Save this data to a CSV file\n",
       "\n",
       "### Key Functions\n",
       "\n",
       "1. **scrape_hacker_news(url)**\n",
       "   - Sends an HTTP request to the specified URL (defaults to Hacker News homepage)\n",
       "   - Parses the HTML using BeautifulSoup to extract article titles and links\n",
       "   - Returns a list of dictionaries, each containing a title and URL\n",
       "   - Implements comprehensive error handling for various request failures\n",
       "\n",
       "2. **save_to_csv(articles, filename)**\n",
       "   - Takes the list of articles and writes them to a CSV file\n",
       "   - If no filename is provided, creates one with a timestamp\n",
       "   - Returns the path to the saved file or None if there was an error\n",
       "\n",
       "3. **main()**\n",
       "   - Orchestrates the entire scraping process\n",
       "   - Implements retry logic if scraping fails\n",
       "   - Returns appropriate exit codes based on success/failure\n",
       "\n",
       "### Error Handling\n",
       "\n",
       "The script includes robust error handling for:\n",
       "- Connection errors\n",
       "- Timeouts\n",
       "- HTTP errors (e.g., 404, 500)\n",
       "- File I/O errors\n",
       "- Unexpected exceptions\n",
       "\n",
       "It also implements a retry mechanism to attempt the scraping multiple times if initial attempts fail.\n",
       "\n",
       "## How to Run\n",
       "\n",
       "1. Save the code as `hacker_news_scraper.py`\n",
       "2. Install the required libraries:\n",
       "   ```\n",
       "   pip install requests beautifulsoup4\n",
       "   ```\n",
       "3. Run the script:\n",
       "   ```\n",
       "   python hacker_news_scraper.py\n",
       "   ```\n",
       "\n",
       "## Output\n",
       "\n",
       "The script will:\n",
       "1. Print status messages to the console during execution\n",
       "2. Create a CSV file in the current directory with a name like `hacker_news_20230615_123045.csv`\n",
       "\n",
       "The CSV file will contain two columns:\n",
       "- `title`: The article title\n",
       "- `url`: The link to the article\n",
       "\n",
       "## Notes\n",
       "\n",
       "- This script respects website owners by setting a proper user agent\n",
       "- It implements timeouts to avoid hanging if the website is slow to respond\n",
       "- The code includes appropriate pauses between retry attempts to avoid overloading the server\n",
       "- Defensive programming techniques are used to handle unexpected HTML structure changes\n",
       "\n",
       "## Ethical Considerations\n",
       "\n",
       "Be aware that web scraping may be against the terms of service of some websites. Always:\n",
       "1. Check the website's robots.txt file\n",
       "2. Don't overload the server with too many requests\n",
       "3. Consider reaching out to the site owner for permission or to check if an API is available"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a Python script that performs the following tasks:\n",
    "\n",
    "- Use the `requests` and `BeautifulSoup` libraries to scrape data from a webpage.\n",
    "- Target the Hacker News homepage: https://news.ycombinator.com/\n",
    "- Extract all article titles and their corresponding links from the homepage.\n",
    "- Save the extracted data into a CSV file.\n",
    "- Implement error handling for potential network-related issues (e.g., connection errors, timeouts).\n",
    "- Structure the script with a `main()` function that executes when the script is run directly.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5475e6b-d849-4feb-a9a9-19f59332eafa",
   "metadata": {},
   "source": [
    "### Stock Price Analysis for Apple Inc.  \n",
    "Downloads historical stock data, calculates moving averages, key financial metrics, and buy/sell signals with visualizations and logging for Apple Inc. using `yfinance` and `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b41bae9-1f87-4170-886c-701eb821bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AAPL Stock Analysis Tool\n",
      "\n",
      "This Python script analyzes Apple Inc. (AAPL) stock data for the past year, calculating key financial metrics and identifying potential trading signals based on moving average crossovers.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "Install these libraries using pip:\n",
      "```bash\n",
      "pip install yfinance pandas numpy matplotlib seaborn\n",
      "```\n",
      "\n",
      "## Full Script\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "AAPL Stock Analysis Tool\n",
      "\n",
      "This script downloads and analyzes historical stock data for Apple Inc. (AAPL).\n",
      "It calculates moving averages, identifies buy/sell signals based on MA crossovers,\n",
      "and computes key financial metrics such as volatility, maximum drawdown, and Sharpe ratio.\n",
      "\"\"\"\n",
      "\n",
      "import logging\n",
      "import sys\n",
      "import os\n",
      "from datetime import datetime, timedelta\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import yfinance as yf\n",
      "\n",
      "# Configure logging\n",
      "def setup_logging():\n",
      "    \"\"\"\n",
      "    Set up logging configuration for the application.\n",
      "    Creates logs directory if it doesn't exist.\n",
      "    \"\"\"\n",
      "    # Create logs directory if it doesn't exist\n",
      "    if not os.path.exists('logs'):\n",
      "        os.makedirs('logs')\n",
      "        \n",
      "    # Set up logging with timestamp in filename\n",
      "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
      "    log_filename = f\"logs/aapl_analysis_{timestamp}.log\"\n",
      "    \n",
      "    logging.basicConfig(\n",
      "        level=logging.INFO,\n",
      "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
      "        handlers=[\n",
      "            logging.FileHandler(log_filename),\n",
      "            logging.StreamHandler(sys.stdout)\n",
      "        ]\n",
      "    )\n",
      "    logging.info(\"Logging initialized\")\n",
      "\n",
      "def download_stock_data(ticker=\"AAPL\", period=\"1y\"):\n",
      "    \"\"\"\n",
      "    Download historical stock data using yfinance.\n",
      "    \n",
      "    Args:\n",
      "        ticker (str): Stock ticker symbol. Defaults to \"AAPL\".\n",
      "        period (str): Time period to download. Defaults to \"1y\" (1 year).\n",
      "    \n",
      "    Returns:\n",
      "        pandas.DataFrame: DataFrame containing historical stock data.\n",
      "    \n",
      "    Raises:\n",
      "        Exception: If data download fails.\n",
      "    \"\"\"\n",
      "    logging.info(f\"Downloading {ticker} stock data for the past {period}...\")\n",
      "    \n",
      "    try:\n",
      "        # Download data\n",
      "        stock_data = yf.download(ticker, period=period)\n",
      "        \n",
      "        if stock_data.empty:\n",
      "            logging.error(f\"No data available for {ticker}\")\n",
      "            raise ValueError(f\"No data available for {ticker}\")\n",
      "            \n",
      "        logging.info(f\"Successfully downloaded {len(stock_data)} days of {ticker} data\")\n",
      "        return stock_data\n",
      "        \n",
      "    except Exception as e:\n",
      "        logging.error(f\"Error downloading stock data: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "def calculate_moving_averages(data, short_window=20, long_window=50):\n",
      "    \"\"\"\n",
      "    Calculate short and long moving averages for the stock data.\n",
      "    \n",
      "    Args:\n",
      "        data (pandas.DataFrame): Stock price data.\n",
      "        short_window (int): Short moving average window. Defaults to 20 days.\n",
      "        long_window (int): Long moving average window. Defaults to 50 days.\n",
      "    \n",
      "    Returns:\n",
      "        pandas.DataFrame: DataFrame with added moving average columns.\n",
      "    \"\"\"\n",
      "    logging.info(f\"Calculating {short_window}-day and {long_window}-day moving averages...\")\n",
      "    \n",
      "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
      "    df = data.copy()\n",
      "    \n",
      "    # Calculate moving averages\n",
      "    df[f'MA_{short_window}'] = df['Close'].rolling(window=short_window).mean()\n",
      "    df[f'MA_{long_window}'] = df['Close'].rolling(window=long_window).mean()\n",
      "    \n",
      "    logging.info(\"Moving averages calculated successfully\")\n",
      "    return df\n",
      "\n",
      "def identify_signals(data, short_window=20, long_window=50):\n",
      "    \"\"\"\n",
      "    Identify buy and sell signals based on moving average crossovers.\n",
      "    \n",
      "    Buy signal: Short MA crosses above Long MA\n",
      "    Sell signal: Short MA crosses below Long MA\n",
      "    \n",
      "    Args:\n",
      "        data (pandas.DataFrame): DataFrame with moving averages.\n",
      "        short_window (int): Short moving average window.\n",
      "        long_window (int): Long moving average window.\n",
      "    \n",
      "    Returns:\n",
      "        pandas.DataFrame: DataFrame with added signal columns.\n",
      "    \"\"\"\n",
      "    logging.info(\"Identifying buy/sell signals based on moving average crossovers...\")\n",
      "    \n",
      "    # Create a copy of the dataframe\n",
      "    df = data.copy()\n",
      "    \n",
      "    # Create a 'Signal' column\n",
      "    # 1 = Buy signal, -1 = Sell signal, 0 = No signal\n",
      "    df['Signal'] = 0\n",
      "    \n",
      "    # Create a 'Position' column based on the comparison of moving averages\n",
      "    df['Position'] = np.where(df[f'MA_{short_window}'] > df[f'MA_{long_window}'], 1, -1)\n",
      "    \n",
      "    # Identify crossover points (signal change)\n",
      "    df['Signal'] = df['Position'].diff()\n",
      "    \n",
      "    # Replace NaN values in Signal with 0\n",
      "    df['Signal'] = df['Signal'].fillna(0)\n",
      "    \n",
      "    # Count buy and sell signals\n",
      "    buy_signals = len(df[df['Signal'] > 0])\n",
      "    sell_signals = len(df[df['Signal'] < 0])\n",
      "    \n",
      "    logging.info(f\"Identified {buy_signals} buy signals and {sell_signals} sell signals\")\n",
      "    return df\n",
      "\n",
      "def calculate_financial_metrics(data):\n",
      "    \"\"\"\n",
      "    Calculate key financial metrics: volatility, maximum drawdown, and Sharpe ratio.\n",
      "    \n",
      "    Args:\n",
      "        data (pandas.DataFrame): Stock price data.\n",
      "    \n",
      "    Returns:\n",
      "        dict: Dictionary containing the calculated metrics.\n",
      "    \"\"\"\n",
      "    logging.info(\"Calculating financial metrics...\")\n",
      "    \n",
      "    # Calculate daily returns\n",
      "    returns = data['Close'].pct_change().dropna()\n",
      "    \n",
      "    # Calculate annualized volatility (standard deviation of returns * sqrt(252))\n",
      "    volatility = returns.std() * np.sqrt(252)\n",
      "    \n",
      "    # Calculate maximum drawdown\n",
      "    cumulative_returns = (1 + returns).cumprod()\n",
      "    running_max = cumulative_returns.cummax()\n",
      "    drawdown = (cumulative_returns / running_max) - 1\n",
      "    max_drawdown = drawdown.min()\n",
      "    \n",
      "    # Calculate annualized return\n",
      "    total_return = (data['Close'].iloc[-1] / data['Close'].iloc[0]) - 1\n",
      "    days_in_period = (data.index[-1] - data.index[0]).days\n",
      "    annualized_return = ((1 + total_return) ** (365 / days_in_period)) - 1\n",
      "    \n",
      "    # Calculate Sharpe ratio (assuming risk-free rate = 0)\n",
      "    sharpe_ratio = annualized_return / volatility\n",
      "    \n",
      "    metrics = {\n",
      "        'Volatility (Annualized)': volatility,\n",
      "        'Maximum Drawdown': max_drawdown,\n",
      "        'Annualized Return': annualized_return,\n",
      "        'Sharpe Ratio': sharpe_ratio\n",
      "    }\n",
      "    \n",
      "    logging.info(\"Financial metrics calculated successfully\")\n",
      "    return metrics\n",
      "\n",
      "def plot_stock_analysis(data, ticker=\"AAPL\", short_window=20, long_window=50):\n",
      "    \"\"\"\n",
      "    Create a visualization of stock data with moving averages and buy/sell signals.\n",
      "    \n",
      "    Args:\n",
      "        data (pandas.DataFrame): Stock data with moving averages and signals.\n",
      "        ticker (str): Stock ticker symbol.\n",
      "        short_window (int): Short moving average window.\n",
      "        long_window (int): Long moving average window.\n",
      "    \n",
      "    Returns:\n",
      "        matplotlib.figure.Figure: The plot figure.\n",
      "    \"\"\"\n",
      "    logging.info(\"Generating stock analysis plot...\")\n",
      "    \n",
      "    # Set the style\n",
      "    sns.set(style='darkgrid')\n",
      "    \n",
      "    # Create figure and axis\n",
      "    fig, ax = plt.subplots(figsize=(14, 8))\n",
      "    \n",
      "    # Plot close price and moving averages\n",
      "    ax.plot(data.index, data['Close'], label='Close Price', linewidth=2, alpha=0.7)\n",
      "    ax.plot(data.index, data[f'MA_{short_window}'], label=f'{short_window}-day MA', \n",
      "            linewidth=1.5, linestyle='-')\n",
      "    ax.plot(data.index, data[f'MA_{long_window}'], label=f'{long_window}-day MA', \n",
      "            linewidth=1.5, linestyle='-')\n",
      "    \n",
      "    # Highlight buy signals\n",
      "    buy_signals = data[data['Signal'] > 0]\n",
      "    ax.scatter(buy_signals.index, buy_signals['Close'], \n",
      "               label='Buy Signal', marker='^', color='green', s=100)\n",
      "    \n",
      "    # Highlight sell signals\n",
      "    sell_signals = data[data['Signal'] < 0]\n",
      "    ax.scatter(sell_signals.index, sell_signals['Close'], \n",
      "               label='Sell Signal', marker='v', color='red', s=100)\n",
      "    \n",
      "    # Formatting\n",
      "    ax.set_title(f'{ticker} Stock Analysis ({data.index[0].date()} to {data.index[-1].date()})', \n",
      "                 fontsize=16)\n",
      "    ax.set_xlabel('Date', fontsize=12)\n",
      "    ax.set_ylabel('Price (USD)', fontsize=12)\n",
      "    ax.legend(loc='best', fontsize=10)\n",
      "    ax.grid(True, alpha=0.3)\n",
      "    \n",
      "    # Create filename with timestamp\n",
      "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
      "    fig_filename = f\"AAPL_analysis_{timestamp}.png\"\n",
      "    \n",
      "    # Save the figure\n",
      "    if not os.path.exists('plots'):\n",
      "        os.makedirs('plots')\n",
      "    plt.savefig(f\"plots/{fig_filename}\", dpi=300, bbox_inches='tight')\n",
      "    \n",
      "    logging.info(f\"Plot saved as plots/{fig_filename}\")\n",
      "    return fig\n",
      "\n",
      "def display_metrics(metrics):\n",
      "    \"\"\"\n",
      "    Display financial metrics in a formatted way.\n",
      "    \n",
      "    Args:\n",
      "        metrics (dict): Dictionary containing the financial metrics.\n",
      "    \"\"\"\n",
      "    logging.info(\"Displaying financial metrics summary...\")\n",
      "    \n",
      "    print(\"\\n\" + \"=\"*50)\n",
      "    print(\" \"*15 + \"FINANCIAL METRICS SUMMARY\")\n",
      "    print(\"=\"*50)\n",
      "    \n",
      "    for key, value in metrics.items():\n",
      "        if key == 'Maximum Drawdown':\n",
      "            formatted_value = f\"{value:.2%}\"\n",
      "        elif key == 'Annualized Return':\n",
      "            formatted_value = f\"{value:.2%}\"\n",
      "        else:\n",
      "            formatted_value = f\"{value:.4f}\"\n",
      "        \n",
      "        print(f\"{key:.<30}{formatted_value:>20}\")\n",
      "    \n",
      "    print(\"=\"*50 + \"\\n\")\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Main function to run the AAPL stock analysis.\n",
      "    \"\"\"\n",
      "    # Set up logging\n",
      "    setup_logging()\n",
      "    \n",
      "    try:\n",
      "        # Download stock data\n",
      "        stock_data = download_stock_data(ticker=\"AAPL\", period=\"1y\")\n",
      "        \n",
      "        # Define MA windows\n",
      "        short_window = 20\n",
      "        long_window = 50\n",
      "        \n",
      "        # Calculate moving averages\n",
      "        stock_data = calculate_moving_averages(stock_data, short_window, long_window)\n",
      "        \n",
      "        # Identify buy and sell signals\n",
      "        stock_data = identify_signals(stock_data, short_window, long_window)\n",
      "        \n",
      "        # Calculate financial metrics\n",
      "        metrics = calculate_financial_metrics(stock_data)\n",
      "        \n",
      "        # Display metrics\n",
      "        display_metrics(metrics)\n",
      "        \n",
      "        # Create and save visualization\n",
      "        plot_stock_analysis(stock_data, \"AAPL\", short_window, long_window)\n",
      "        \n",
      "        logging.info(\"Analysis completed successfully\")\n",
      "        plt.show()  # Show plot\n",
      "        \n",
      "    except Exception as e:\n",
      "        logging.error(f\"Analysis failed: {str(e)}\")\n",
      "        print(f\"Analysis failed. See log for details. Error: {str(e)}\")\n",
      "        return 1\n",
      "        \n",
      "    return 0\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    sys.exit(main())\n",
      "```\n",
      "\n",
      "## How the Script Works\n",
      "\n",
      "This script performs a comprehensive analysis of Apple Inc. (AAPL) stock data with the following components:\n",
      "\n",
      "### 1. Data Collection\n",
      "- Downloads one year of historical stock data for Apple Inc. using the `yfinance` library\n",
      "- Handles potential network or API errors with appropriate exception handling\n",
      "\n",
      "### 2. Technical Analysis\n",
      "- Calculates 20-day and 50-day moving averages of the closing price\n",
      "- Identifies buy and sell signals based on moving average crossovers:\n",
      "  - Buy signal: When the short-term MA crosses above the long-term MA\n",
      "  - Sell signal: When the short-term MA crosses below the long-term MA\n",
      "\n",
      "### 3. Financial Metrics Calculation\n",
      "- Volatility (annualized standard deviation of returns)\n",
      "- Maximum drawdown (largest percentage drop from peak to trough)\n",
      "- Annualized return\n",
      "- Sharpe ratio (risk-adjusted return metric, assuming zero risk-free rate)\n",
      "\n",
      "### 4. Visualization\n",
      "- Plots the stock's closing price alongside the moving averages\n",
      "- Highlights buy and sell signals on the chart with green and red markers\n",
      "- Saves the plot as a high-resolution PNG file in the 'plots' directory\n",
      "\n",
      "### 5. Logging & Error Handling\n",
      "- Uses Python's `logging` module to track execution steps\n",
      "- Logs are saved in the 'logs' directory with timestamped filenames\n",
      "- Comprehensive error handling for data downloading, processing, and visualization\n",
      "\n",
      "## Key Functions\n",
      "\n",
      "1. `setup_logging()`: Configures the logging system\n",
      "2. `download_stock_data()`: Downloads historical stock data using yfinance\n",
      "3. `calculate_moving_averages()`: Calculates the specified moving averages\n",
      "4. `identify_signals()`: Identifies trading signals based on MA crossovers\n",
      "5. `calculate_financial_metrics()`: Computes volatility, maximum drawdown, and Sharpe ratio\n",
      "6. `plot_stock_analysis()`: Creates and saves the visualization\n",
      "7. `display_metrics()`: Formats and prints the financial metrics\n",
      "8. `main()`: Orchestrates the entire analysis process\n",
      "\n",
      "## How to Run\n",
      "\n",
      "1. Install the required packages:\n",
      "   ```bash\n",
      "   pip install yfinance pandas numpy matplotlib seaborn\n",
      "   ```\n",
      "\n",
      "2. Save the script as `aapl_analysis.py`\n",
      "\n",
      "3. Run the script:\n",
      "   ```bash\n",
      "   python aapl_analysis.py\n",
      "   ```\n",
      "\n",
      "## Output\n",
      "\n",
      "The script produces:\n",
      "\n",
      "1. **Console output** displaying key financial metrics\n",
      "2. **Log file** in the 'logs' directory with detailed execution information\n",
      "3. **Visualization** saved in the 'plots' directory showing the stock price, moving averages, and buy/sell signals\n",
      "\n",
      "## Notes and Extensions\n",
      "\n",
      "- **Customization**: You can modify the moving average periods (short_window and long_window) to test different trading strategies\n",
      "- **Other Stocks**: Change the ticker symbol in the `download_stock_data()` call to analyze different stocks\n",
      "- **Time Period**: Adjust the 'period' parameter to analyze different timeframes (e.g., '6mo', '2y', '5y')\n",
      "- **Additional Analysis**: The script framework can be extended to include other technical indicators or more sophisticated trading signals\n",
      "\n",
      "## Disclaimer\n",
      "\n",
      "This script is for educational purposes only and should not be considered financial advice. Always conduct thorough research or consult with a professional before making investment decisions."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# AAPL Stock Analysis Tool\n",
       "\n",
       "This Python script analyzes Apple Inc. (AAPL) stock data for the past year, calculating key financial metrics and identifying potential trading signals based on moving average crossovers.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "Install these libraries using pip:\n",
       "```bash\n",
       "pip install yfinance pandas numpy matplotlib seaborn\n",
       "```\n",
       "\n",
       "## Full Script\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "AAPL Stock Analysis Tool\n",
       "\n",
       "This script downloads and analyzes historical stock data for Apple Inc. (AAPL).\n",
       "It calculates moving averages, identifies buy/sell signals based on MA crossovers,\n",
       "and computes key financial metrics such as volatility, maximum drawdown, and Sharpe ratio.\n",
       "\"\"\"\n",
       "\n",
       "import logging\n",
       "import sys\n",
       "import os\n",
       "from datetime import datetime, timedelta\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import yfinance as yf\n",
       "\n",
       "# Configure logging\n",
       "def setup_logging():\n",
       "    \"\"\"\n",
       "    Set up logging configuration for the application.\n",
       "    Creates logs directory if it doesn't exist.\n",
       "    \"\"\"\n",
       "    # Create logs directory if it doesn't exist\n",
       "    if not os.path.exists('logs'):\n",
       "        os.makedirs('logs')\n",
       "        \n",
       "    # Set up logging with timestamp in filename\n",
       "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
       "    log_filename = f\"logs/aapl_analysis_{timestamp}.log\"\n",
       "    \n",
       "    logging.basicConfig(\n",
       "        level=logging.INFO,\n",
       "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
       "        handlers=[\n",
       "            logging.FileHandler(log_filename),\n",
       "            logging.StreamHandler(sys.stdout)\n",
       "        ]\n",
       "    )\n",
       "    logging.info(\"Logging initialized\")\n",
       "\n",
       "def download_stock_data(ticker=\"AAPL\", period=\"1y\"):\n",
       "    \"\"\"\n",
       "    Download historical stock data using yfinance.\n",
       "    \n",
       "    Args:\n",
       "        ticker (str): Stock ticker symbol. Defaults to \"AAPL\".\n",
       "        period (str): Time period to download. Defaults to \"1y\" (1 year).\n",
       "    \n",
       "    Returns:\n",
       "        pandas.DataFrame: DataFrame containing historical stock data.\n",
       "    \n",
       "    Raises:\n",
       "        Exception: If data download fails.\n",
       "    \"\"\"\n",
       "    logging.info(f\"Downloading {ticker} stock data for the past {period}...\")\n",
       "    \n",
       "    try:\n",
       "        # Download data\n",
       "        stock_data = yf.download(ticker, period=period)\n",
       "        \n",
       "        if stock_data.empty:\n",
       "            logging.error(f\"No data available for {ticker}\")\n",
       "            raise ValueError(f\"No data available for {ticker}\")\n",
       "            \n",
       "        logging.info(f\"Successfully downloaded {len(stock_data)} days of {ticker} data\")\n",
       "        return stock_data\n",
       "        \n",
       "    except Exception as e:\n",
       "        logging.error(f\"Error downloading stock data: {str(e)}\")\n",
       "        raise\n",
       "\n",
       "def calculate_moving_averages(data, short_window=20, long_window=50):\n",
       "    \"\"\"\n",
       "    Calculate short and long moving averages for the stock data.\n",
       "    \n",
       "    Args:\n",
       "        data (pandas.DataFrame): Stock price data.\n",
       "        short_window (int): Short moving average window. Defaults to 20 days.\n",
       "        long_window (int): Long moving average window. Defaults to 50 days.\n",
       "    \n",
       "    Returns:\n",
       "        pandas.DataFrame: DataFrame with added moving average columns.\n",
       "    \"\"\"\n",
       "    logging.info(f\"Calculating {short_window}-day and {long_window}-day moving averages...\")\n",
       "    \n",
       "    # Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
       "    df = data.copy()\n",
       "    \n",
       "    # Calculate moving averages\n",
       "    df[f'MA_{short_window}'] = df['Close'].rolling(window=short_window).mean()\n",
       "    df[f'MA_{long_window}'] = df['Close'].rolling(window=long_window).mean()\n",
       "    \n",
       "    logging.info(\"Moving averages calculated successfully\")\n",
       "    return df\n",
       "\n",
       "def identify_signals(data, short_window=20, long_window=50):\n",
       "    \"\"\"\n",
       "    Identify buy and sell signals based on moving average crossovers.\n",
       "    \n",
       "    Buy signal: Short MA crosses above Long MA\n",
       "    Sell signal: Short MA crosses below Long MA\n",
       "    \n",
       "    Args:\n",
       "        data (pandas.DataFrame): DataFrame with moving averages.\n",
       "        short_window (int): Short moving average window.\n",
       "        long_window (int): Long moving average window.\n",
       "    \n",
       "    Returns:\n",
       "        pandas.DataFrame: DataFrame with added signal columns.\n",
       "    \"\"\"\n",
       "    logging.info(\"Identifying buy/sell signals based on moving average crossovers...\")\n",
       "    \n",
       "    # Create a copy of the dataframe\n",
       "    df = data.copy()\n",
       "    \n",
       "    # Create a 'Signal' column\n",
       "    # 1 = Buy signal, -1 = Sell signal, 0 = No signal\n",
       "    df['Signal'] = 0\n",
       "    \n",
       "    # Create a 'Position' column based on the comparison of moving averages\n",
       "    df['Position'] = np.where(df[f'MA_{short_window}'] > df[f'MA_{long_window}'], 1, -1)\n",
       "    \n",
       "    # Identify crossover points (signal change)\n",
       "    df['Signal'] = df['Position'].diff()\n",
       "    \n",
       "    # Replace NaN values in Signal with 0\n",
       "    df['Signal'] = df['Signal'].fillna(0)\n",
       "    \n",
       "    # Count buy and sell signals\n",
       "    buy_signals = len(df[df['Signal'] > 0])\n",
       "    sell_signals = len(df[df['Signal'] < 0])\n",
       "    \n",
       "    logging.info(f\"Identified {buy_signals} buy signals and {sell_signals} sell signals\")\n",
       "    return df\n",
       "\n",
       "def calculate_financial_metrics(data):\n",
       "    \"\"\"\n",
       "    Calculate key financial metrics: volatility, maximum drawdown, and Sharpe ratio.\n",
       "    \n",
       "    Args:\n",
       "        data (pandas.DataFrame): Stock price data.\n",
       "    \n",
       "    Returns:\n",
       "        dict: Dictionary containing the calculated metrics.\n",
       "    \"\"\"\n",
       "    logging.info(\"Calculating financial metrics...\")\n",
       "    \n",
       "    # Calculate daily returns\n",
       "    returns = data['Close'].pct_change().dropna()\n",
       "    \n",
       "    # Calculate annualized volatility (standard deviation of returns * sqrt(252))\n",
       "    volatility = returns.std() * np.sqrt(252)\n",
       "    \n",
       "    # Calculate maximum drawdown\n",
       "    cumulative_returns = (1 + returns).cumprod()\n",
       "    running_max = cumulative_returns.cummax()\n",
       "    drawdown = (cumulative_returns / running_max) - 1\n",
       "    max_drawdown = drawdown.min()\n",
       "    \n",
       "    # Calculate annualized return\n",
       "    total_return = (data['Close'].iloc[-1] / data['Close'].iloc[0]) - 1\n",
       "    days_in_period = (data.index[-1] - data.index[0]).days\n",
       "    annualized_return = ((1 + total_return) ** (365 / days_in_period)) - 1\n",
       "    \n",
       "    # Calculate Sharpe ratio (assuming risk-free rate = 0)\n",
       "    sharpe_ratio = annualized_return / volatility\n",
       "    \n",
       "    metrics = {\n",
       "        'Volatility (Annualized)': volatility,\n",
       "        'Maximum Drawdown': max_drawdown,\n",
       "        'Annualized Return': annualized_return,\n",
       "        'Sharpe Ratio': sharpe_ratio\n",
       "    }\n",
       "    \n",
       "    logging.info(\"Financial metrics calculated successfully\")\n",
       "    return metrics\n",
       "\n",
       "def plot_stock_analysis(data, ticker=\"AAPL\", short_window=20, long_window=50):\n",
       "    \"\"\"\n",
       "    Create a visualization of stock data with moving averages and buy/sell signals.\n",
       "    \n",
       "    Args:\n",
       "        data (pandas.DataFrame): Stock data with moving averages and signals.\n",
       "        ticker (str): Stock ticker symbol.\n",
       "        short_window (int): Short moving average window.\n",
       "        long_window (int): Long moving average window.\n",
       "    \n",
       "    Returns:\n",
       "        matplotlib.figure.Figure: The plot figure.\n",
       "    \"\"\"\n",
       "    logging.info(\"Generating stock analysis plot...\")\n",
       "    \n",
       "    # Set the style\n",
       "    sns.set(style='darkgrid')\n",
       "    \n",
       "    # Create figure and axis\n",
       "    fig, ax = plt.subplots(figsize=(14, 8))\n",
       "    \n",
       "    # Plot close price and moving averages\n",
       "    ax.plot(data.index, data['Close'], label='Close Price', linewidth=2, alpha=0.7)\n",
       "    ax.plot(data.index, data[f'MA_{short_window}'], label=f'{short_window}-day MA', \n",
       "            linewidth=1.5, linestyle='-')\n",
       "    ax.plot(data.index, data[f'MA_{long_window}'], label=f'{long_window}-day MA', \n",
       "            linewidth=1.5, linestyle='-')\n",
       "    \n",
       "    # Highlight buy signals\n",
       "    buy_signals = data[data['Signal'] > 0]\n",
       "    ax.scatter(buy_signals.index, buy_signals['Close'], \n",
       "               label='Buy Signal', marker='^', color='green', s=100)\n",
       "    \n",
       "    # Highlight sell signals\n",
       "    sell_signals = data[data['Signal'] < 0]\n",
       "    ax.scatter(sell_signals.index, sell_signals['Close'], \n",
       "               label='Sell Signal', marker='v', color='red', s=100)\n",
       "    \n",
       "    # Formatting\n",
       "    ax.set_title(f'{ticker} Stock Analysis ({data.index[0].date()} to {data.index[-1].date()})', \n",
       "                 fontsize=16)\n",
       "    ax.set_xlabel('Date', fontsize=12)\n",
       "    ax.set_ylabel('Price (USD)', fontsize=12)\n",
       "    ax.legend(loc='best', fontsize=10)\n",
       "    ax.grid(True, alpha=0.3)\n",
       "    \n",
       "    # Create filename with timestamp\n",
       "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
       "    fig_filename = f\"AAPL_analysis_{timestamp}.png\"\n",
       "    \n",
       "    # Save the figure\n",
       "    if not os.path.exists('plots'):\n",
       "        os.makedirs('plots')\n",
       "    plt.savefig(f\"plots/{fig_filename}\", dpi=300, bbox_inches='tight')\n",
       "    \n",
       "    logging.info(f\"Plot saved as plots/{fig_filename}\")\n",
       "    return fig\n",
       "\n",
       "def display_metrics(metrics):\n",
       "    \"\"\"\n",
       "    Display financial metrics in a formatted way.\n",
       "    \n",
       "    Args:\n",
       "        metrics (dict): Dictionary containing the financial metrics.\n",
       "    \"\"\"\n",
       "    logging.info(\"Displaying financial metrics summary...\")\n",
       "    \n",
       "    print(\"\\n\" + \"=\"*50)\n",
       "    print(\" \"*15 + \"FINANCIAL METRICS SUMMARY\")\n",
       "    print(\"=\"*50)\n",
       "    \n",
       "    for key, value in metrics.items():\n",
       "        if key == 'Maximum Drawdown':\n",
       "            formatted_value = f\"{value:.2%}\"\n",
       "        elif key == 'Annualized Return':\n",
       "            formatted_value = f\"{value:.2%}\"\n",
       "        else:\n",
       "            formatted_value = f\"{value:.4f}\"\n",
       "        \n",
       "        print(f\"{key:.<30}{formatted_value:>20}\")\n",
       "    \n",
       "    print(\"=\"*50 + \"\\n\")\n",
       "\n",
       "def main():\n",
       "    \"\"\"\n",
       "    Main function to run the AAPL stock analysis.\n",
       "    \"\"\"\n",
       "    # Set up logging\n",
       "    setup_logging()\n",
       "    \n",
       "    try:\n",
       "        # Download stock data\n",
       "        stock_data = download_stock_data(ticker=\"AAPL\", period=\"1y\")\n",
       "        \n",
       "        # Define MA windows\n",
       "        short_window = 20\n",
       "        long_window = 50\n",
       "        \n",
       "        # Calculate moving averages\n",
       "        stock_data = calculate_moving_averages(stock_data, short_window, long_window)\n",
       "        \n",
       "        # Identify buy and sell signals\n",
       "        stock_data = identify_signals(stock_data, short_window, long_window)\n",
       "        \n",
       "        # Calculate financial metrics\n",
       "        metrics = calculate_financial_metrics(stock_data)\n",
       "        \n",
       "        # Display metrics\n",
       "        display_metrics(metrics)\n",
       "        \n",
       "        # Create and save visualization\n",
       "        plot_stock_analysis(stock_data, \"AAPL\", short_window, long_window)\n",
       "        \n",
       "        logging.info(\"Analysis completed successfully\")\n",
       "        plt.show()  # Show plot\n",
       "        \n",
       "    except Exception as e:\n",
       "        logging.error(f\"Analysis failed: {str(e)}\")\n",
       "        print(f\"Analysis failed. See log for details. Error: {str(e)}\")\n",
       "        return 1\n",
       "        \n",
       "    return 0\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    sys.exit(main())\n",
       "```\n",
       "\n",
       "## How the Script Works\n",
       "\n",
       "This script performs a comprehensive analysis of Apple Inc. (AAPL) stock data with the following components:\n",
       "\n",
       "### 1. Data Collection\n",
       "- Downloads one year of historical stock data for Apple Inc. using the `yfinance` library\n",
       "- Handles potential network or API errors with appropriate exception handling\n",
       "\n",
       "### 2. Technical Analysis\n",
       "- Calculates 20-day and 50-day moving averages of the closing price\n",
       "- Identifies buy and sell signals based on moving average crossovers:\n",
       "  - Buy signal: When the short-term MA crosses above the long-term MA\n",
       "  - Sell signal: When the short-term MA crosses below the long-term MA\n",
       "\n",
       "### 3. Financial Metrics Calculation\n",
       "- Volatility (annualized standard deviation of returns)\n",
       "- Maximum drawdown (largest percentage drop from peak to trough)\n",
       "- Annualized return\n",
       "- Sharpe ratio (risk-adjusted return metric, assuming zero risk-free rate)\n",
       "\n",
       "### 4. Visualization\n",
       "- Plots the stock's closing price alongside the moving averages\n",
       "- Highlights buy and sell signals on the chart with green and red markers\n",
       "- Saves the plot as a high-resolution PNG file in the 'plots' directory\n",
       "\n",
       "### 5. Logging & Error Handling\n",
       "- Uses Python's `logging` module to track execution steps\n",
       "- Logs are saved in the 'logs' directory with timestamped filenames\n",
       "- Comprehensive error handling for data downloading, processing, and visualization\n",
       "\n",
       "## Key Functions\n",
       "\n",
       "1. `setup_logging()`: Configures the logging system\n",
       "2. `download_stock_data()`: Downloads historical stock data using yfinance\n",
       "3. `calculate_moving_averages()`: Calculates the specified moving averages\n",
       "4. `identify_signals()`: Identifies trading signals based on MA crossovers\n",
       "5. `calculate_financial_metrics()`: Computes volatility, maximum drawdown, and Sharpe ratio\n",
       "6. `plot_stock_analysis()`: Creates and saves the visualization\n",
       "7. `display_metrics()`: Formats and prints the financial metrics\n",
       "8. `main()`: Orchestrates the entire analysis process\n",
       "\n",
       "## How to Run\n",
       "\n",
       "1. Install the required packages:\n",
       "   ```bash\n",
       "   pip install yfinance pandas numpy matplotlib seaborn\n",
       "   ```\n",
       "\n",
       "2. Save the script as `aapl_analysis.py`\n",
       "\n",
       "3. Run the script:\n",
       "   ```bash\n",
       "   python aapl_analysis.py\n",
       "   ```\n",
       "\n",
       "## Output\n",
       "\n",
       "The script produces:\n",
       "\n",
       "1. **Console output** displaying key financial metrics\n",
       "2. **Log file** in the 'logs' directory with detailed execution information\n",
       "3. **Visualization** saved in the 'plots' directory showing the stock price, moving averages, and buy/sell signals\n",
       "\n",
       "## Notes and Extensions\n",
       "\n",
       "- **Customization**: You can modify the moving average periods (short_window and long_window) to test different trading strategies\n",
       "- **Other Stocks**: Change the ticker symbol in the `download_stock_data()` call to analyze different stocks\n",
       "- **Time Period**: Adjust the 'period' parameter to analyze different timeframes (e.g., '6mo', '2y', '5y')\n",
       "- **Additional Analysis**: The script framework can be extended to include other technical indicators or more sophisticated trading signals\n",
       "\n",
       "## Disclaimer\n",
       "\n",
       "This script is for educational purposes only and should not be considered financial advice. Always conduct thorough research or consult with a professional before making investment decisions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a complete Python script for analyzing Apple Inc. (AAPL) stock price data. The script should:\n",
    "\n",
    "- Download historical stock data for the past year using the `yfinance` library.\n",
    "- Calculate and plot 20-day and 50-day moving averages alongside the stock's closing price.\n",
    "- Compute and display key financial metrics, including:\n",
    "  - Volatility (standard deviation of returns)\n",
    "  - Maximum drawdown\n",
    "  - Sharpe ratio (assume a risk-free rate of 0)\n",
    "- Identify potential buy and sell signals based on moving average crossovers, and highlight them on the plot.\n",
    "- Include appropriate error handling for network/data issues.\n",
    "- Use Python's built-in `logging` module for logging important steps and errors.\n",
    "- Structure the script with functions for clarity and modularity.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d3363-6c48-4ea3-a6f8-b55b0f38dd30",
   "metadata": {},
   "source": [
    "### PySpark Data Processing Example  \n",
    "Demonstrates SparkSession creation, reading and transforming CSV data, filtering, labeling, grouping, and saving results as Parquet files using PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e88ec051-f9bc-4ee6-ae54-cca57518dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PySpark Data Processing Script\n",
      "\n",
      "This script demonstrates basic PySpark operations including creating a DataFrame, reading/writing data, and performing transformations on the data.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```bash\n",
      "pip install pyspark pandas\n",
      "```\n",
      "\n",
      "## Script Implementation\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "PySpark Data Processing Script\n",
      "\n",
      "This script demonstrates basic PySpark operations including:\n",
      "- Creating a SparkSession\n",
      "- Generating and working with sample data\n",
      "- Reading from CSV\n",
      "- Applying DataFrame transformations\n",
      "- Writing to Parquet format\n",
      "\n",
      "The script generates a sample users.csv file, processes it with various transformations,\n",
      "and saves the results as a Parquet file.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.sql.functions import col, when, avg, lit\n",
      "import logging\n",
      "\n",
      "# Configure logging\n",
      "logging.basicConfig(\n",
      "    level=logging.INFO,\n",
      "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
      ")\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "def create_spark_session(app_name=\"PySpark Data Processing\"):\n",
      "    \"\"\"\n",
      "    Initialize and configure a SparkSession.\n",
      "    \n",
      "    Args:\n",
      "        app_name (str): Name to register for the Spark application\n",
      "        \n",
      "    Returns:\n",
      "        pyspark.sql.SparkSession: Configured SparkSession object\n",
      "    \"\"\"\n",
      "    logger.info(\"Initializing SparkSession\")\n",
      "    return (SparkSession.builder\n",
      "            .appName(app_name)\n",
      "            # Using local mode with all available cores\n",
      "            .master(\"local[*]\")\n",
      "            # Set log level to reduce console output\n",
      "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
      "            .config(\"spark.sql.shuffle.partitions\", \"4\")  # Reduced partitions for small data\n",
      "            .getOrCreate())\n",
      "\n",
      "\n",
      "def generate_sample_data(file_path=\"users.csv\"):\n",
      "    \"\"\"\n",
      "    Generate a sample CSV file with user data.\n",
      "    \n",
      "    Args:\n",
      "        file_path (str): Path where the CSV file will be saved\n",
      "        \n",
      "    Returns:\n",
      "        str: Path to the generated CSV file\n",
      "    \"\"\"\n",
      "    logger.info(f\"Generating sample data at {file_path}\")\n",
      "    \n",
      "    # Create sample data using pandas first (easier than constructing CSV directly)\n",
      "    data = {\n",
      "        'id': list(range(1, 21)),  # 20 users with IDs from 1-20\n",
      "        'name': [\n",
      "            'Alice', 'Bob', 'Charlie', 'David', 'Emma',\n",
      "            'Frank', 'Grace', 'Henry', 'Isabella', 'Jack',\n",
      "            'Kate', 'Liam', 'Mia', 'Noah', 'Olivia',\n",
      "            'Peter', 'Quinn', 'Ryan', 'Sophia', 'Thomas'\n",
      "        ],\n",
      "        'age': [24, 31, 18, 45, 29, 17, 36, 52, 27, 22, 40, 33, 16, 29, 43, 19, 38, 25, 34, 30],\n",
      "        'city': [\n",
      "            'New York', 'Chicago', 'New York', 'San Francisco', 'Boston',\n",
      "            'Chicago', 'New York', 'Boston', 'San Francisco', 'Chicago',\n",
      "            'New York', 'Boston', 'Chicago', 'San Francisco', 'New York',\n",
      "            'Boston', 'Chicago', 'New York', 'San Francisco', 'Boston'\n",
      "        ]\n",
      "    }\n",
      "    \n",
      "    # Create DataFrame and save as CSV\n",
      "    pd.DataFrame(data).to_csv(file_path, index=False)\n",
      "    logger.info(f\"Sample data generated successfully with {len(data['id'])} records\")\n",
      "    \n",
      "    return file_path\n",
      "\n",
      "\n",
      "def process_user_data(spark, input_path=\"users.csv\", output_path=\"user_stats\"):\n",
      "    \"\"\"\n",
      "    Process user data: load CSV, apply transformations, and save as Parquet.\n",
      "    \n",
      "    This function:\n",
      "    1. Reads user data from a CSV file\n",
      "    2. Filters users older than 25\n",
      "    3. Adds an 'age_category' column\n",
      "    4. Groups by city to calculate average age\n",
      "    5. Saves the result as Parquet files\n",
      "    \n",
      "    Args:\n",
      "        spark (pyspark.sql.SparkSession): Active SparkSession\n",
      "        input_path (str): Path to the CSV file to process\n",
      "        output_path (str): Directory where output Parquet files will be saved\n",
      "        \n",
      "    Returns:\n",
      "        tuple: A tuple containing (filtered_df, city_avg_age_df)\n",
      "    \"\"\"\n",
      "    logger.info(f\"Reading data from {input_path}\")\n",
      "    \n",
      "    # Read the CSV file into a DataFrame\n",
      "    users_df = (spark.read\n",
      "               .option(\"header\", \"true\")  # CSV has a header row\n",
      "               .option(\"inferSchema\", \"true\")  # Automatically infer column types\n",
      "               .csv(input_path))\n",
      "    \n",
      "    # Print the schema and sample data for verification\n",
      "    logger.info(\"Original DataFrame Schema:\")\n",
      "    users_df.printSchema()\n",
      "    \n",
      "    logger.info(\"Sample data:\")\n",
      "    users_df.show(5)\n",
      "    \n",
      "    # 1. Filter to users older than 25\n",
      "    logger.info(\"Filtering users older than 25\")\n",
      "    filtered_df = users_df.filter(col(\"age\") > 25)\n",
      "    \n",
      "    # 2. Add a new column 'age_category' based on age\n",
      "    logger.info(\"Adding age category column\")\n",
      "    categorized_df = filtered_df.withColumn(\n",
      "        \"age_category\",\n",
      "        when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\")\n",
      "    )\n",
      "    \n",
      "    # Show the transformation result\n",
      "    logger.info(\"Transformed data:\")\n",
      "    categorized_df.show(5)\n",
      "    \n",
      "    # 3. Group by city and calculate average age\n",
      "    logger.info(\"Calculating average age by city\")\n",
      "    city_avg_age_df = (categorized_df.groupBy(\"city\")\n",
      "                      .agg(avg(\"age\").alias(\"average_age\"))\n",
      "                      # Round to 2 decimal places for readability\n",
      "                      .withColumn(\"average_age\", col(\"average_age\").cast(\"decimal(10,2)\")))\n",
      "    \n",
      "    # Show the aggregated results\n",
      "    logger.info(\"City average age statistics:\")\n",
      "    city_avg_age_df.show()\n",
      "    \n",
      "    # 4. Save the aggregated results as Parquet\n",
      "    logger.info(f\"Saving results to {output_path}\")\n",
      "    city_avg_age_df.write.mode(\"overwrite\").parquet(output_path)\n",
      "    \n",
      "    # Return both DataFrames for possible further processing or testing\n",
      "    return filtered_df, city_avg_age_df\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"\n",
      "    Main function to orchestrate the data processing workflow.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        # Step 1: Create SparkSession\n",
      "        spark = create_spark_session()\n",
      "        \n",
      "        # Step 2: Generate sample data\n",
      "        csv_path = generate_sample_data()\n",
      "        \n",
      "        # Step 3 & 4: Process the data and apply transformations\n",
      "        filtered_df, city_stats_df = process_user_data(spark)\n",
      "        \n",
      "        # Log completion\n",
      "        logger.info(\"Data processing completed successfully\")\n",
      "        \n",
      "        # Stop the SparkSession to clean up resources\n",
      "        spark.stop()\n",
      "        logger.info(\"SparkSession stopped\")\n",
      "        \n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error during processing: {str(e)}\")\n",
      "        raise\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How This Code Works\n",
      "\n",
      "### 1. SparkSession Initialization\n",
      "\n",
      "The script starts by creating a SparkSession, which is the entry point for PySpark functionality:\n",
      "\n",
      "```python\n",
      "spark = create_spark_session()\n",
      "```\n",
      "\n",
      "The `create_spark_session` function configures Spark to run in local mode using all available cores and sets some performance optimizations.\n",
      "\n",
      "### 2. Sample Data Generation\n",
      "\n",
      "The script generates a CSV file named 'users.csv' with sample user data:\n",
      "\n",
      "```python\n",
      "csv_path = generate_sample_data()\n",
      "```\n",
      "\n",
      "This creates a dataset of 20 users, each with an ID, name, age, and city.\n",
      "\n",
      "### 3. Data Loading\n",
      "\n",
      "The script reads the CSV file into a PySpark DataFrame:\n",
      "\n",
      "```python\n",
      "users_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(input_path)\n",
      "```\n",
      "\n",
      "The options ensure that column headers are read correctly and data types are inferred automatically.\n",
      "\n",
      "### 4. Data Transformations\n",
      "\n",
      "The script applies several transformations to the DataFrame:\n",
      "\n",
      "1. **Filtering** - Keeps only users older than 25:\n",
      "   ```python\n",
      "   filtered_df = users_df.filter(col(\"age\") > 25)\n",
      "   ```\n",
      "\n",
      "2. **Adding a column** - Creates an \"age_category\" column:\n",
      "   ```python\n",
      "   categorized_df = filtered_df.withColumn(\n",
      "       \"age_category\",\n",
      "       when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\")\n",
      "   )\n",
      "   ```\n",
      "\n",
      "3. **Grouping and aggregation** - Calculates the average age by city:\n",
      "   ```python\n",
      "   city_avg_age_df = categorized_df.groupBy(\"city\").agg(avg(\"age\").alias(\"average_age\"))\n",
      "   ```\n",
      "\n",
      "### 5. Saving Results\n",
      "\n",
      "Finally, the script saves the aggregated results as a Parquet file:\n",
      "\n",
      "```python\n",
      "city_avg_age_df.write.mode(\"overwrite\").parquet(output_path)\n",
      "```\n",
      "\n",
      "Parquet is a columnar storage format that's efficient for analytical queries.\n",
      "\n",
      "## How to Run the Script\n",
      "\n",
      "1. Ensure you have Java installed (required for PySpark)\n",
      "2. Install required Python packages:\n",
      "   ```\n",
      "   pip install pyspark pandas\n",
      "   ```\n",
      "3. Save the script as `pyspark_data_processing.py`\n",
      "4. Run the script:\n",
      "   ```\n",
      "   python pyspark_data_processing.py\n",
      "   ```\n",
      "\n",
      "## Expected Output\n",
      "\n",
      "1. A `users.csv` file with the sample data\n",
      "2. Console output showing:\n",
      "   - DataFrame schemas\n",
      "   - Sample data from each transformation step\n",
      "   - City-wise average age statistics\n",
      "3. A `user_stats` directory containing Parquet files with the final results\n",
      "\n",
      "## Extending the Script\n",
      "\n",
      "This script can be extended in several ways:\n",
      "\n",
      "1. Load data from real data sources instead of generating sample data\n",
      "2. Add more complex transformations or SQL queries\n",
      "3. Implement data validation steps\n",
      "4. Configure Spark to run on a cluster for processing larger datasets\n",
      "5. Add command-line arguments to make the script more flexible\n",
      "\n",
      "## Notes\n",
      "\n",
      "- For local testing, the script uses \"local[*]\" as the master URL, which uses all available cores on the local machine\n",
      "- For production use, you would typically deploy the script to a Spark cluster and adjust the master URL accordingly\n",
      "- The script includes comprehensive logging to track execution progress and errors"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# PySpark Data Processing Script\n",
       "\n",
       "This script demonstrates basic PySpark operations including creating a DataFrame, reading/writing data, and performing transformations on the data.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```bash\n",
       "pip install pyspark pandas\n",
       "```\n",
       "\n",
       "## Script Implementation\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "PySpark Data Processing Script\n",
       "\n",
       "This script demonstrates basic PySpark operations including:\n",
       "- Creating a SparkSession\n",
       "- Generating and working with sample data\n",
       "- Reading from CSV\n",
       "- Applying DataFrame transformations\n",
       "- Writing to Parquet format\n",
       "\n",
       "The script generates a sample users.csv file, processes it with various transformations,\n",
       "and saves the results as a Parquet file.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import pandas as pd\n",
       "from pyspark.sql import SparkSession\n",
       "from pyspark.sql.functions import col, when, avg, lit\n",
       "import logging\n",
       "\n",
       "# Configure logging\n",
       "logging.basicConfig(\n",
       "    level=logging.INFO,\n",
       "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
       ")\n",
       "logger = logging.getLogger(__name__)\n",
       "\n",
       "\n",
       "def create_spark_session(app_name=\"PySpark Data Processing\"):\n",
       "    \"\"\"\n",
       "    Initialize and configure a SparkSession.\n",
       "    \n",
       "    Args:\n",
       "        app_name (str): Name to register for the Spark application\n",
       "        \n",
       "    Returns:\n",
       "        pyspark.sql.SparkSession: Configured SparkSession object\n",
       "    \"\"\"\n",
       "    logger.info(\"Initializing SparkSession\")\n",
       "    return (SparkSession.builder\n",
       "            .appName(app_name)\n",
       "            # Using local mode with all available cores\n",
       "            .master(\"local[*]\")\n",
       "            # Set log level to reduce console output\n",
       "            .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
       "            .config(\"spark.sql.shuffle.partitions\", \"4\")  # Reduced partitions for small data\n",
       "            .getOrCreate())\n",
       "\n",
       "\n",
       "def generate_sample_data(file_path=\"users.csv\"):\n",
       "    \"\"\"\n",
       "    Generate a sample CSV file with user data.\n",
       "    \n",
       "    Args:\n",
       "        file_path (str): Path where the CSV file will be saved\n",
       "        \n",
       "    Returns:\n",
       "        str: Path to the generated CSV file\n",
       "    \"\"\"\n",
       "    logger.info(f\"Generating sample data at {file_path}\")\n",
       "    \n",
       "    # Create sample data using pandas first (easier than constructing CSV directly)\n",
       "    data = {\n",
       "        'id': list(range(1, 21)),  # 20 users with IDs from 1-20\n",
       "        'name': [\n",
       "            'Alice', 'Bob', 'Charlie', 'David', 'Emma',\n",
       "            'Frank', 'Grace', 'Henry', 'Isabella', 'Jack',\n",
       "            'Kate', 'Liam', 'Mia', 'Noah', 'Olivia',\n",
       "            'Peter', 'Quinn', 'Ryan', 'Sophia', 'Thomas'\n",
       "        ],\n",
       "        'age': [24, 31, 18, 45, 29, 17, 36, 52, 27, 22, 40, 33, 16, 29, 43, 19, 38, 25, 34, 30],\n",
       "        'city': [\n",
       "            'New York', 'Chicago', 'New York', 'San Francisco', 'Boston',\n",
       "            'Chicago', 'New York', 'Boston', 'San Francisco', 'Chicago',\n",
       "            'New York', 'Boston', 'Chicago', 'San Francisco', 'New York',\n",
       "            'Boston', 'Chicago', 'New York', 'San Francisco', 'Boston'\n",
       "        ]\n",
       "    }\n",
       "    \n",
       "    # Create DataFrame and save as CSV\n",
       "    pd.DataFrame(data).to_csv(file_path, index=False)\n",
       "    logger.info(f\"Sample data generated successfully with {len(data['id'])} records\")\n",
       "    \n",
       "    return file_path\n",
       "\n",
       "\n",
       "def process_user_data(spark, input_path=\"users.csv\", output_path=\"user_stats\"):\n",
       "    \"\"\"\n",
       "    Process user data: load CSV, apply transformations, and save as Parquet.\n",
       "    \n",
       "    This function:\n",
       "    1. Reads user data from a CSV file\n",
       "    2. Filters users older than 25\n",
       "    3. Adds an 'age_category' column\n",
       "    4. Groups by city to calculate average age\n",
       "    5. Saves the result as Parquet files\n",
       "    \n",
       "    Args:\n",
       "        spark (pyspark.sql.SparkSession): Active SparkSession\n",
       "        input_path (str): Path to the CSV file to process\n",
       "        output_path (str): Directory where output Parquet files will be saved\n",
       "        \n",
       "    Returns:\n",
       "        tuple: A tuple containing (filtered_df, city_avg_age_df)\n",
       "    \"\"\"\n",
       "    logger.info(f\"Reading data from {input_path}\")\n",
       "    \n",
       "    # Read the CSV file into a DataFrame\n",
       "    users_df = (spark.read\n",
       "               .option(\"header\", \"true\")  # CSV has a header row\n",
       "               .option(\"inferSchema\", \"true\")  # Automatically infer column types\n",
       "               .csv(input_path))\n",
       "    \n",
       "    # Print the schema and sample data for verification\n",
       "    logger.info(\"Original DataFrame Schema:\")\n",
       "    users_df.printSchema()\n",
       "    \n",
       "    logger.info(\"Sample data:\")\n",
       "    users_df.show(5)\n",
       "    \n",
       "    # 1. Filter to users older than 25\n",
       "    logger.info(\"Filtering users older than 25\")\n",
       "    filtered_df = users_df.filter(col(\"age\") > 25)\n",
       "    \n",
       "    # 2. Add a new column 'age_category' based on age\n",
       "    logger.info(\"Adding age category column\")\n",
       "    categorized_df = filtered_df.withColumn(\n",
       "        \"age_category\",\n",
       "        when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\")\n",
       "    )\n",
       "    \n",
       "    # Show the transformation result\n",
       "    logger.info(\"Transformed data:\")\n",
       "    categorized_df.show(5)\n",
       "    \n",
       "    # 3. Group by city and calculate average age\n",
       "    logger.info(\"Calculating average age by city\")\n",
       "    city_avg_age_df = (categorized_df.groupBy(\"city\")\n",
       "                      .agg(avg(\"age\").alias(\"average_age\"))\n",
       "                      # Round to 2 decimal places for readability\n",
       "                      .withColumn(\"average_age\", col(\"average_age\").cast(\"decimal(10,2)\")))\n",
       "    \n",
       "    # Show the aggregated results\n",
       "    logger.info(\"City average age statistics:\")\n",
       "    city_avg_age_df.show()\n",
       "    \n",
       "    # 4. Save the aggregated results as Parquet\n",
       "    logger.info(f\"Saving results to {output_path}\")\n",
       "    city_avg_age_df.write.mode(\"overwrite\").parquet(output_path)\n",
       "    \n",
       "    # Return both DataFrames for possible further processing or testing\n",
       "    return filtered_df, city_avg_age_df\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"\n",
       "    Main function to orchestrate the data processing workflow.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        # Step 1: Create SparkSession\n",
       "        spark = create_spark_session()\n",
       "        \n",
       "        # Step 2: Generate sample data\n",
       "        csv_path = generate_sample_data()\n",
       "        \n",
       "        # Step 3 & 4: Process the data and apply transformations\n",
       "        filtered_df, city_stats_df = process_user_data(spark)\n",
       "        \n",
       "        # Log completion\n",
       "        logger.info(\"Data processing completed successfully\")\n",
       "        \n",
       "        # Stop the SparkSession to clean up resources\n",
       "        spark.stop()\n",
       "        logger.info(\"SparkSession stopped\")\n",
       "        \n",
       "    except Exception as e:\n",
       "        logger.error(f\"Error during processing: {str(e)}\")\n",
       "        raise\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How This Code Works\n",
       "\n",
       "### 1. SparkSession Initialization\n",
       "\n",
       "The script starts by creating a SparkSession, which is the entry point for PySpark functionality:\n",
       "\n",
       "```python\n",
       "spark = create_spark_session()\n",
       "```\n",
       "\n",
       "The `create_spark_session` function configures Spark to run in local mode using all available cores and sets some performance optimizations.\n",
       "\n",
       "### 2. Sample Data Generation\n",
       "\n",
       "The script generates a CSV file named 'users.csv' with sample user data:\n",
       "\n",
       "```python\n",
       "csv_path = generate_sample_data()\n",
       "```\n",
       "\n",
       "This creates a dataset of 20 users, each with an ID, name, age, and city.\n",
       "\n",
       "### 3. Data Loading\n",
       "\n",
       "The script reads the CSV file into a PySpark DataFrame:\n",
       "\n",
       "```python\n",
       "users_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(input_path)\n",
       "```\n",
       "\n",
       "The options ensure that column headers are read correctly and data types are inferred automatically.\n",
       "\n",
       "### 4. Data Transformations\n",
       "\n",
       "The script applies several transformations to the DataFrame:\n",
       "\n",
       "1. **Filtering** - Keeps only users older than 25:\n",
       "   ```python\n",
       "   filtered_df = users_df.filter(col(\"age\") > 25)\n",
       "   ```\n",
       "\n",
       "2. **Adding a column** - Creates an \"age_category\" column:\n",
       "   ```python\n",
       "   categorized_df = filtered_df.withColumn(\n",
       "       \"age_category\",\n",
       "       when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\")\n",
       "   )\n",
       "   ```\n",
       "\n",
       "3. **Grouping and aggregation** - Calculates the average age by city:\n",
       "   ```python\n",
       "   city_avg_age_df = categorized_df.groupBy(\"city\").agg(avg(\"age\").alias(\"average_age\"))\n",
       "   ```\n",
       "\n",
       "### 5. Saving Results\n",
       "\n",
       "Finally, the script saves the aggregated results as a Parquet file:\n",
       "\n",
       "```python\n",
       "city_avg_age_df.write.mode(\"overwrite\").parquet(output_path)\n",
       "```\n",
       "\n",
       "Parquet is a columnar storage format that's efficient for analytical queries.\n",
       "\n",
       "## How to Run the Script\n",
       "\n",
       "1. Ensure you have Java installed (required for PySpark)\n",
       "2. Install required Python packages:\n",
       "   ```\n",
       "   pip install pyspark pandas\n",
       "   ```\n",
       "3. Save the script as `pyspark_data_processing.py`\n",
       "4. Run the script:\n",
       "   ```\n",
       "   python pyspark_data_processing.py\n",
       "   ```\n",
       "\n",
       "## Expected Output\n",
       "\n",
       "1. A `users.csv` file with the sample data\n",
       "2. Console output showing:\n",
       "   - DataFrame schemas\n",
       "   - Sample data from each transformation step\n",
       "   - City-wise average age statistics\n",
       "3. A `user_stats` directory containing Parquet files with the final results\n",
       "\n",
       "## Extending the Script\n",
       "\n",
       "This script can be extended in several ways:\n",
       "\n",
       "1. Load data from real data sources instead of generating sample data\n",
       "2. Add more complex transformations or SQL queries\n",
       "3. Implement data validation steps\n",
       "4. Configure Spark to run on a cluster for processing larger datasets\n",
       "5. Add command-line arguments to make the script more flexible\n",
       "\n",
       "## Notes\n",
       "\n",
       "- For local testing, the script uses \"local[*]\" as the master URL, which uses all available cores on the local machine\n",
       "- For production use, you would typically deploy the script to a Spark cluster and adjust the master URL accordingly\n",
       "- The script includes comprehensive logging to track execution progress and errors"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a PySpark script that performs the following tasks:\n",
    "\n",
    "1. Create a SparkSession to initialize the PySpark environment.\n",
    "2. Generate a sample CSV file named 'users.csv' containing the following columns: 'id', 'name', 'age', and 'city'.\n",
    "3. Read the 'users.csv' file into a DataFrame.\n",
    "4. Apply the following transformations:\n",
    "   - Filter the DataFrame to include only users older than 25.\n",
    "   - Add a new column that labels each user as 'Adult' if their age is greater than 18, otherwise 'Minor'.\n",
    "   - Group the data by 'city' and calculate the average age for each city.\n",
    "5. Save the final transformed DataFrame as a Parquet file.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7344792a-1c24-4b2d-a6cb-5bdf44dc192b",
   "metadata": {},
   "source": [
    "### Weather Data Processing and Analysis  \n",
    "Processes sample weather data, converts temperature units, categorizes temperatures, groups by city, and summarizes statistics, returning structured output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fc884d-aaad-40f4-8545-b2879b635140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Weather Data Processing Function\n",
      "\n",
      "This script demonstrates how to process weather data from a CSV file, perform transformations like temperature conversion, categorization, and aggregation, and return summarized results.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```bash\n",
      "pip install pandas\n",
      "```\n",
      "\n",
      "## Function Implementation\n",
      "\n",
      "```python\n",
      "import os\n",
      "import pandas as pd\n",
      "from typing import Dict, Any, Optional\n",
      "\n",
      "def process_weather_data(csv_file_path: Optional[str] = None) -> Dict[str, Dict[str, Any]]:\n",
      "    \"\"\"\n",
      "    Process weather data from a CSV file, including temperature conversion, \n",
      "    categorization, and city-wise aggregation.\n",
      "\n",
      "    The function performs the following operations:\n",
      "    1. Creates a sample CSV file if no path is provided\n",
      "    2. Reads the weather data CSV file\n",
      "    3. Converts temperatures from Fahrenheit to Celsius\n",
      "    4. Categorizes each row based on temperature ranges\n",
      "    5. Groups data by city and calculates averages\n",
      "    6. Returns a dictionary of city-wise weather summaries\n",
      "\n",
      "    Args:\n",
      "        csv_file_path (str, optional): Path to the CSV file containing weather data.\n",
      "                                      If None, creates a sample file.\n",
      "\n",
      "    Returns:\n",
      "        Dict[str, Dict[str, Any]]: Dictionary where keys are city names and values are\n",
      "                                  dictionaries containing average weather metrics and\n",
      "                                  temperature category counts.\n",
      "\n",
      "    Example output format:\n",
      "    {\n",
      "        'New York': {\n",
      "            'avg_temp_celsius': 0.0,\n",
      "            'avg_humidity': 65.0,\n",
      "            'avg_wind_speed': 10.0,\n",
      "            'cold_days': 1,\n",
      "            'mild_days': 0,\n",
      "            'hot_days': 0\n",
      "        },\n",
      "        ...\n",
      "    }\n",
      "    \"\"\"\n",
      "    # If no file path is provided, create a sample CSV file\n",
      "    if csv_file_path is None:\n",
      "        csv_file_path = \"sample_weather_data.csv\"\n",
      "        create_sample_weather_data(csv_file_path)\n",
      "\n",
      "    try:\n",
      "        # Step 1: Read the CSV file into a pandas DataFrame\n",
      "        weather_df = pd.read_csv(csv_file_path)\n",
      "        \n",
      "        # Validate that required columns exist\n",
      "        required_columns = ['date', 'city', 'temperature', 'humidity', 'wind_speed']\n",
      "        missing_columns = [col for col in required_columns if col not in weather_df.columns]\n",
      "        if missing_columns:\n",
      "            raise ValueError(f\"CSV file is missing required columns: {', '.join(missing_columns)}\")\n",
      "        \n",
      "        # Step 2: Convert temperature from Fahrenheit to Celsius\n",
      "        # Formula: (F - 32) * (5/9)\n",
      "        weather_df['temperature_celsius'] = (weather_df['temperature'] - 32) * (5/9)\n",
      "        \n",
      "        # Round to 1 decimal place for better readability\n",
      "        weather_df['temperature_celsius'] = weather_df['temperature_celsius'].round(1)\n",
      "        \n",
      "        # Step 3: Categorize rows based on temperature in Celsius\n",
      "        # Create a new column 'temp_category' with the appropriate category\n",
      "        weather_df['temp_category'] = pd.cut(\n",
      "            weather_df['temperature_celsius'],\n",
      "            bins=[-float('inf'), 10, 25, float('inf')],\n",
      "            labels=['Cold', 'Mild', 'Hot']\n",
      "        )\n",
      "        \n",
      "        # Step 4: Group data by city and compute averages\n",
      "        # First, create a dictionary to store the counts of each temperature category by city\n",
      "        city_temp_categories = weather_df.groupby(['city', 'temp_category']).size().unstack(fill_value=0)\n",
      "        \n",
      "        # Rename the columns to match the expected output format\n",
      "        if 'Cold' in city_temp_categories.columns:\n",
      "            city_temp_categories = city_temp_categories.rename(columns={'Cold': 'cold_days'})\n",
      "        else:\n",
      "            city_temp_categories['cold_days'] = 0\n",
      "            \n",
      "        if 'Mild' in city_temp_categories.columns:\n",
      "            city_temp_categories = city_temp_categories.rename(columns={'Mild': 'mild_days'})\n",
      "        else:\n",
      "            city_temp_categories['mild_days'] = 0\n",
      "            \n",
      "        if 'Hot' in city_temp_categories.columns:\n",
      "            city_temp_categories = city_temp_categories.rename(columns={'Hot': 'hot_days'})\n",
      "        else:\n",
      "            city_temp_categories['hot_days'] = 0\n",
      "        \n",
      "        # Calculate averages by city\n",
      "        city_averages = weather_df.groupby('city').agg({\n",
      "            'temperature_celsius': 'mean',\n",
      "            'humidity': 'mean',\n",
      "            'wind_speed': 'mean'\n",
      "        }).round(1)\n",
      "        \n",
      "        # Rename columns to match expected output format\n",
      "        city_averages = city_averages.rename(columns={\n",
      "            'temperature_celsius': 'avg_temp_celsius',\n",
      "            'humidity': 'avg_humidity',\n",
      "            'wind_speed': 'avg_wind_speed'\n",
      "        })\n",
      "        \n",
      "        # Step 5: Combine the averages and temperature category counts\n",
      "        combined_df = pd.concat([city_averages, city_temp_categories], axis=1)\n",
      "        \n",
      "        # Convert to dictionary format\n",
      "        result = combined_df.to_dict(orient='index')\n",
      "        \n",
      "        return result\n",
      "        \n",
      "    except Exception as e:\n",
      "        print(f\"Error processing weather data: {str(e)}\")\n",
      "        return {}\n",
      "\n",
      "\n",
      "def create_sample_weather_data(file_path: str) -> None:\n",
      "    \"\"\"\n",
      "    Create a sample CSV file with weather data.\n",
      "    \n",
      "    Args:\n",
      "        file_path (str): Path where the CSV file will be saved.\n",
      "    \"\"\"\n",
      "    # Sample data\n",
      "    data = [\n",
      "        \"date,city,temperature,humidity,wind_speed,condition\",\n",
      "        \"2023-01-01,New York,32,65,10,Sunny\",\n",
      "        \"2023-01-01,Los Angeles,72,50,5,Clear\",\n",
      "        \"2023-01-01,Chicago,28,80,15,Snow\",\n",
      "        \"2023-01-02,New York,35,70,8,Cloudy\",\n",
      "        \"2023-01-02,Los Angeles,75,48,7,Clear\",\n",
      "        \"2023-01-02,Chicago,30,75,12,Snow\",\n",
      "        \"2023-01-03,New York,40,60,12,Rain\",\n",
      "        \"2023-01-03,Los Angeles,68,55,10,Partly Cloudy\",\n",
      "        \"2023-01-03,Chicago,35,65,8,Cloudy\",\n",
      "        \"2023-01-04,Phoenix,85,20,3,Sunny\",\n",
      "        \"2023-01-04,Miami,80,75,8,Humid\",\n",
      "        \"2023-01-04,Denver,45,45,15,Windy\"\n",
      "    ]\n",
      "    \n",
      "    # Write data to CSV file\n",
      "    with open(file_path, 'w') as f:\n",
      "        for line in data:\n",
      "            f.write(line + \"\\n\")\n",
      "    \n",
      "    print(f\"Sample weather data created at {file_path}\")\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to demonstrate the weather data processing.\"\"\"\n",
      "    # Process the weather data\n",
      "    result = process_weather_data()\n",
      "    \n",
      "    # Display the results\n",
      "    print(\"\\nWeather Data Summary by City:\")\n",
      "    print(\"============================\")\n",
      "    \n",
      "    for city, metrics in result.items():\n",
      "        print(f\"\\n{city}:\")\n",
      "        print(f\"  Average Temperature: {metrics['avg_temp_celsius']}°C\")\n",
      "        print(f\"  Average Humidity: {metrics['avg_humidity']}%\")\n",
      "        print(f\"  Average Wind Speed: {metrics['avg_wind_speed']} mph\")\n",
      "        print(f\"  Temperature Categories:\")\n",
      "        print(f\"    Cold Days: {metrics['cold_days']}\")\n",
      "        print(f\"    Mild Days: {metrics['mild_days']}\")\n",
      "        print(f\"    Hot Days: {metrics['hot_days']}\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "### Overview\n",
      "\n",
      "This script processes weather data by:\n",
      "1. Reading data from a CSV file (or creating a sample CSV if none is provided)\n",
      "2. Converting temperatures from Fahrenheit to Celsius\n",
      "3. Categorizing temperatures as \"Cold\" (< 10°C), \"Mild\" (10-25°C), or \"Hot\" (> 25°C)\n",
      "4. Grouping data by city and calculating average metrics\n",
      "5. Returning a dictionary with summaries for each city\n",
      "\n",
      "### Function Components\n",
      "\n",
      "#### 1. `process_weather_data(csv_file_path=None)`\n",
      "\n",
      "The main function that processes weather data:\n",
      "\n",
      "- **Input validation**: Checks if the required columns exist in the CSV file\n",
      "- **Temperature conversion**: Converts Fahrenheit to Celsius using the formula `(F - 32) * (5/9)`\n",
      "- **Temperature categorization**: Uses pandas' `pd.cut()` function to categorize temperatures\n",
      "- **Data aggregation**: Groups data by city and computes averages\n",
      "- **Result formatting**: Returns a nested dictionary with city-wise summaries\n",
      "\n",
      "#### 2. `create_sample_weather_data(file_path)`\n",
      "\n",
      "Helper function to create a sample CSV file with weather data.\n",
      "\n",
      "#### 3. `main()`\n",
      "\n",
      "Demonstrates how to use the `process_weather_data` function and displays the results in a formatted way.\n",
      "\n",
      "### Key Data Transformations\n",
      "\n",
      "1. **Temperature conversion**:\n",
      "   ```python\n",
      "   weather_df['temperature_celsius'] = (weather_df['temperature'] - 32) * (5/9)\n",
      "   ```\n",
      "\n",
      "2. **Temperature categorization**:\n",
      "   ```python\n",
      "   weather_df['temp_category'] = pd.cut(\n",
      "       weather_df['temperature_celsius'],\n",
      "       bins=[-float('inf'), 10, 25, float('inf')],\n",
      "       labels=['Cold', 'Mild', 'Hot']\n",
      "   )\n",
      "   ```\n",
      "\n",
      "3. **City-wise aggregation**:\n",
      "   ```python\n",
      "   city_averages = weather_df.groupby('city').agg({\n",
      "       'temperature_celsius': 'mean',\n",
      "       'humidity': 'mean',\n",
      "       'wind_speed': 'mean'\n",
      "   }).round(1)\n",
      "   ```\n",
      "\n",
      "## How to Run the Script\n",
      "\n",
      "1. Install required libraries:\n",
      "   ```bash\n",
      "   pip install pandas\n",
      "   ```\n",
      "\n",
      "2. Save the code to a file, e.g., `weather_processor.py`\n",
      "\n",
      "3. Run the script:\n",
      "   ```bash\n",
      "   python weather_processor.py\n",
      "   ```\n",
      "\n",
      "4. The script will:\n",
      "   - Create a sample weather data CSV file\n",
      "   - Process the data\n",
      "   - Display a summary of weather metrics by city\n",
      "\n",
      "## Example Usage in Another Script\n",
      "\n",
      "```python\n",
      "from weather_processor import process_weather_data\n",
      "\n",
      "# Process using the sample data\n",
      "result = process_weather_data()\n",
      "print(result)\n",
      "\n",
      "# Or process your own data file\n",
      "my_result = process_weather_data('my_weather_data.csv')\n",
      "print(my_result)\n",
      "```\n",
      "\n",
      "## Example Output\n",
      "\n",
      "```\n",
      "Weather Data Summary by City:\n",
      "============================\n",
      "\n",
      "Chicago:\n",
      "  Average Temperature: -1.1°C\n",
      "  Average Humidity: 73.3%\n",
      "  Average Wind Speed: 11.7 mph\n",
      "  Temperature Categories:\n",
      "    Cold Days: 3\n",
      "    Mild Days: 0\n",
      "    Hot Days: 0\n",
      "\n",
      "Los Angeles:\n",
      "  Average Temperature: 21.5°C\n",
      "  Average Humidity: 51.0%\n",
      "  Average Wind Speed: 7.3 mph\n",
      "  Temperature Categories:\n",
      "    Cold Days: 0\n",
      "    Mild Days: 3\n",
      "    Hot Days: 0\n",
      "\n",
      "...\n",
      "```\n",
      "\n",
      "## Error Handling\n",
      "\n",
      "The function includes error handling to:\n",
      "- Validate required columns in the input CSV\n",
      "- Catch and report any exceptions that occur during processing\n",
      "- Handle missing categories in the temperature classification\n",
      "\n",
      "## Extending the Code\n",
      "\n",
      "This code can be extended to:\n",
      "1. Add more weather metrics and categories\n",
      "2. Generate visualizations of the weather data (using matplotlib or seaborn)\n",
      "3. Implement additional filtering options\n",
      "4. Add time-based aggregations (e.g., monthly averages)\n",
      "5. Connect to a weather API to fetch real-time data"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Weather Data Processing Function\n",
       "\n",
       "This script demonstrates how to process weather data from a CSV file, perform transformations like temperature conversion, categorization, and aggregation, and return summarized results.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```bash\n",
       "pip install pandas\n",
       "```\n",
       "\n",
       "## Function Implementation\n",
       "\n",
       "```python\n",
       "import os\n",
       "import pandas as pd\n",
       "from typing import Dict, Any, Optional\n",
       "\n",
       "def process_weather_data(csv_file_path: Optional[str] = None) -> Dict[str, Dict[str, Any]]:\n",
       "    \"\"\"\n",
       "    Process weather data from a CSV file, including temperature conversion, \n",
       "    categorization, and city-wise aggregation.\n",
       "\n",
       "    The function performs the following operations:\n",
       "    1. Creates a sample CSV file if no path is provided\n",
       "    2. Reads the weather data CSV file\n",
       "    3. Converts temperatures from Fahrenheit to Celsius\n",
       "    4. Categorizes each row based on temperature ranges\n",
       "    5. Groups data by city and calculates averages\n",
       "    6. Returns a dictionary of city-wise weather summaries\n",
       "\n",
       "    Args:\n",
       "        csv_file_path (str, optional): Path to the CSV file containing weather data.\n",
       "                                      If None, creates a sample file.\n",
       "\n",
       "    Returns:\n",
       "        Dict[str, Dict[str, Any]]: Dictionary where keys are city names and values are\n",
       "                                  dictionaries containing average weather metrics and\n",
       "                                  temperature category counts.\n",
       "\n",
       "    Example output format:\n",
       "    {\n",
       "        'New York': {\n",
       "            'avg_temp_celsius': 0.0,\n",
       "            'avg_humidity': 65.0,\n",
       "            'avg_wind_speed': 10.0,\n",
       "            'cold_days': 1,\n",
       "            'mild_days': 0,\n",
       "            'hot_days': 0\n",
       "        },\n",
       "        ...\n",
       "    }\n",
       "    \"\"\"\n",
       "    # If no file path is provided, create a sample CSV file\n",
       "    if csv_file_path is None:\n",
       "        csv_file_path = \"sample_weather_data.csv\"\n",
       "        create_sample_weather_data(csv_file_path)\n",
       "\n",
       "    try:\n",
       "        # Step 1: Read the CSV file into a pandas DataFrame\n",
       "        weather_df = pd.read_csv(csv_file_path)\n",
       "        \n",
       "        # Validate that required columns exist\n",
       "        required_columns = ['date', 'city', 'temperature', 'humidity', 'wind_speed']\n",
       "        missing_columns = [col for col in required_columns if col not in weather_df.columns]\n",
       "        if missing_columns:\n",
       "            raise ValueError(f\"CSV file is missing required columns: {', '.join(missing_columns)}\")\n",
       "        \n",
       "        # Step 2: Convert temperature from Fahrenheit to Celsius\n",
       "        # Formula: (F - 32) * (5/9)\n",
       "        weather_df['temperature_celsius'] = (weather_df['temperature'] - 32) * (5/9)\n",
       "        \n",
       "        # Round to 1 decimal place for better readability\n",
       "        weather_df['temperature_celsius'] = weather_df['temperature_celsius'].round(1)\n",
       "        \n",
       "        # Step 3: Categorize rows based on temperature in Celsius\n",
       "        # Create a new column 'temp_category' with the appropriate category\n",
       "        weather_df['temp_category'] = pd.cut(\n",
       "            weather_df['temperature_celsius'],\n",
       "            bins=[-float('inf'), 10, 25, float('inf')],\n",
       "            labels=['Cold', 'Mild', 'Hot']\n",
       "        )\n",
       "        \n",
       "        # Step 4: Group data by city and compute averages\n",
       "        # First, create a dictionary to store the counts of each temperature category by city\n",
       "        city_temp_categories = weather_df.groupby(['city', 'temp_category']).size().unstack(fill_value=0)\n",
       "        \n",
       "        # Rename the columns to match the expected output format\n",
       "        if 'Cold' in city_temp_categories.columns:\n",
       "            city_temp_categories = city_temp_categories.rename(columns={'Cold': 'cold_days'})\n",
       "        else:\n",
       "            city_temp_categories['cold_days'] = 0\n",
       "            \n",
       "        if 'Mild' in city_temp_categories.columns:\n",
       "            city_temp_categories = city_temp_categories.rename(columns={'Mild': 'mild_days'})\n",
       "        else:\n",
       "            city_temp_categories['mild_days'] = 0\n",
       "            \n",
       "        if 'Hot' in city_temp_categories.columns:\n",
       "            city_temp_categories = city_temp_categories.rename(columns={'Hot': 'hot_days'})\n",
       "        else:\n",
       "            city_temp_categories['hot_days'] = 0\n",
       "        \n",
       "        # Calculate averages by city\n",
       "        city_averages = weather_df.groupby('city').agg({\n",
       "            'temperature_celsius': 'mean',\n",
       "            'humidity': 'mean',\n",
       "            'wind_speed': 'mean'\n",
       "        }).round(1)\n",
       "        \n",
       "        # Rename columns to match expected output format\n",
       "        city_averages = city_averages.rename(columns={\n",
       "            'temperature_celsius': 'avg_temp_celsius',\n",
       "            'humidity': 'avg_humidity',\n",
       "            'wind_speed': 'avg_wind_speed'\n",
       "        })\n",
       "        \n",
       "        # Step 5: Combine the averages and temperature category counts\n",
       "        combined_df = pd.concat([city_averages, city_temp_categories], axis=1)\n",
       "        \n",
       "        # Convert to dictionary format\n",
       "        result = combined_df.to_dict(orient='index')\n",
       "        \n",
       "        return result\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"Error processing weather data: {str(e)}\")\n",
       "        return {}\n",
       "\n",
       "\n",
       "def create_sample_weather_data(file_path: str) -> None:\n",
       "    \"\"\"\n",
       "    Create a sample CSV file with weather data.\n",
       "    \n",
       "    Args:\n",
       "        file_path (str): Path where the CSV file will be saved.\n",
       "    \"\"\"\n",
       "    # Sample data\n",
       "    data = [\n",
       "        \"date,city,temperature,humidity,wind_speed,condition\",\n",
       "        \"2023-01-01,New York,32,65,10,Sunny\",\n",
       "        \"2023-01-01,Los Angeles,72,50,5,Clear\",\n",
       "        \"2023-01-01,Chicago,28,80,15,Snow\",\n",
       "        \"2023-01-02,New York,35,70,8,Cloudy\",\n",
       "        \"2023-01-02,Los Angeles,75,48,7,Clear\",\n",
       "        \"2023-01-02,Chicago,30,75,12,Snow\",\n",
       "        \"2023-01-03,New York,40,60,12,Rain\",\n",
       "        \"2023-01-03,Los Angeles,68,55,10,Partly Cloudy\",\n",
       "        \"2023-01-03,Chicago,35,65,8,Cloudy\",\n",
       "        \"2023-01-04,Phoenix,85,20,3,Sunny\",\n",
       "        \"2023-01-04,Miami,80,75,8,Humid\",\n",
       "        \"2023-01-04,Denver,45,45,15,Windy\"\n",
       "    ]\n",
       "    \n",
       "    # Write data to CSV file\n",
       "    with open(file_path, 'w') as f:\n",
       "        for line in data:\n",
       "            f.write(line + \"\\n\")\n",
       "    \n",
       "    print(f\"Sample weather data created at {file_path}\")\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"Main function to demonstrate the weather data processing.\"\"\"\n",
       "    # Process the weather data\n",
       "    result = process_weather_data()\n",
       "    \n",
       "    # Display the results\n",
       "    print(\"\\nWeather Data Summary by City:\")\n",
       "    print(\"============================\")\n",
       "    \n",
       "    for city, metrics in result.items():\n",
       "        print(f\"\\n{city}:\")\n",
       "        print(f\"  Average Temperature: {metrics['avg_temp_celsius']}°C\")\n",
       "        print(f\"  Average Humidity: {metrics['avg_humidity']}%\")\n",
       "        print(f\"  Average Wind Speed: {metrics['avg_wind_speed']} mph\")\n",
       "        print(f\"  Temperature Categories:\")\n",
       "        print(f\"    Cold Days: {metrics['cold_days']}\")\n",
       "        print(f\"    Mild Days: {metrics['mild_days']}\")\n",
       "        print(f\"    Hot Days: {metrics['hot_days']}\")\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "### Overview\n",
       "\n",
       "This script processes weather data by:\n",
       "1. Reading data from a CSV file (or creating a sample CSV if none is provided)\n",
       "2. Converting temperatures from Fahrenheit to Celsius\n",
       "3. Categorizing temperatures as \"Cold\" (< 10°C), \"Mild\" (10-25°C), or \"Hot\" (> 25°C)\n",
       "4. Grouping data by city and calculating average metrics\n",
       "5. Returning a dictionary with summaries for each city\n",
       "\n",
       "### Function Components\n",
       "\n",
       "#### 1. `process_weather_data(csv_file_path=None)`\n",
       "\n",
       "The main function that processes weather data:\n",
       "\n",
       "- **Input validation**: Checks if the required columns exist in the CSV file\n",
       "- **Temperature conversion**: Converts Fahrenheit to Celsius using the formula `(F - 32) * (5/9)`\n",
       "- **Temperature categorization**: Uses pandas' `pd.cut()` function to categorize temperatures\n",
       "- **Data aggregation**: Groups data by city and computes averages\n",
       "- **Result formatting**: Returns a nested dictionary with city-wise summaries\n",
       "\n",
       "#### 2. `create_sample_weather_data(file_path)`\n",
       "\n",
       "Helper function to create a sample CSV file with weather data.\n",
       "\n",
       "#### 3. `main()`\n",
       "\n",
       "Demonstrates how to use the `process_weather_data` function and displays the results in a formatted way.\n",
       "\n",
       "### Key Data Transformations\n",
       "\n",
       "1. **Temperature conversion**:\n",
       "   ```python\n",
       "   weather_df['temperature_celsius'] = (weather_df['temperature'] - 32) * (5/9)\n",
       "   ```\n",
       "\n",
       "2. **Temperature categorization**:\n",
       "   ```python\n",
       "   weather_df['temp_category'] = pd.cut(\n",
       "       weather_df['temperature_celsius'],\n",
       "       bins=[-float('inf'), 10, 25, float('inf')],\n",
       "       labels=['Cold', 'Mild', 'Hot']\n",
       "   )\n",
       "   ```\n",
       "\n",
       "3. **City-wise aggregation**:\n",
       "   ```python\n",
       "   city_averages = weather_df.groupby('city').agg({\n",
       "       'temperature_celsius': 'mean',\n",
       "       'humidity': 'mean',\n",
       "       'wind_speed': 'mean'\n",
       "   }).round(1)\n",
       "   ```\n",
       "\n",
       "## How to Run the Script\n",
       "\n",
       "1. Install required libraries:\n",
       "   ```bash\n",
       "   pip install pandas\n",
       "   ```\n",
       "\n",
       "2. Save the code to a file, e.g., `weather_processor.py`\n",
       "\n",
       "3. Run the script:\n",
       "   ```bash\n",
       "   python weather_processor.py\n",
       "   ```\n",
       "\n",
       "4. The script will:\n",
       "   - Create a sample weather data CSV file\n",
       "   - Process the data\n",
       "   - Display a summary of weather metrics by city\n",
       "\n",
       "## Example Usage in Another Script\n",
       "\n",
       "```python\n",
       "from weather_processor import process_weather_data\n",
       "\n",
       "# Process using the sample data\n",
       "result = process_weather_data()\n",
       "print(result)\n",
       "\n",
       "# Or process your own data file\n",
       "my_result = process_weather_data('my_weather_data.csv')\n",
       "print(my_result)\n",
       "```\n",
       "\n",
       "## Example Output\n",
       "\n",
       "```\n",
       "Weather Data Summary by City:\n",
       "============================\n",
       "\n",
       "Chicago:\n",
       "  Average Temperature: -1.1°C\n",
       "  Average Humidity: 73.3%\n",
       "  Average Wind Speed: 11.7 mph\n",
       "  Temperature Categories:\n",
       "    Cold Days: 3\n",
       "    Mild Days: 0\n",
       "    Hot Days: 0\n",
       "\n",
       "Los Angeles:\n",
       "  Average Temperature: 21.5°C\n",
       "  Average Humidity: 51.0%\n",
       "  Average Wind Speed: 7.3 mph\n",
       "  Temperature Categories:\n",
       "    Cold Days: 0\n",
       "    Mild Days: 3\n",
       "    Hot Days: 0\n",
       "\n",
       "...\n",
       "```\n",
       "\n",
       "## Error Handling\n",
       "\n",
       "The function includes error handling to:\n",
       "- Validate required columns in the input CSV\n",
       "- Catch and report any exceptions that occur during processing\n",
       "- Handle missing categories in the temperature classification\n",
       "\n",
       "## Extending the Code\n",
       "\n",
       "This code can be extended to:\n",
       "1. Add more weather metrics and categories\n",
       "2. Generate visualizations of the weather data (using matplotlib or seaborn)\n",
       "3. Implement additional filtering options\n",
       "4. Add time-based aggregations (e.g., monthly averages)\n",
       "5. Connect to a weather API to fetch real-time data"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Write a complete Python function that processes weather data using the following specifications:\n",
    "\n",
    "Example input data format:\n",
    "\n",
    "date,city,temperature,humidity,wind_speed,condition\n",
    "2023-01-01,New York,32,65,10,Sunny\n",
    "2023-01-01,Los Angeles,72,50,5,Clear\n",
    "2023-01-01,Chicago,28,80,15,Snow\n",
    "\n",
    "1. Create a sample CSV file with the data provided and then read from that same file.\n",
    "2. Convert the temperature values from Fahrenheit to Celsius.\n",
    "3. Categorize each row based on temperature:\n",
    "   - \"Cold\" if temperature is below 10°C\n",
    "   - \"Mild\" if temperature is between 10°C and 25°C\n",
    "   - \"Hot\" if temperature is above 25°C\n",
    "4. Group the data by city and compute the average of temperature, humidity, and wind speed.\n",
    "5. Return the final output as a dictionary where each key is a city and each value is its weather summary (averages and categorized temperature counts).\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "response = agent(prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5e16e-df03-4dc8-b9a7-e641b07080d2",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preprocessing for Customer Churn Prediction  \n",
    "Creates a synthetic customer dataset, handles missing values, encodes categorical features, normalizes numerical data, and splits into training and test sets with detailed commentary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e8435a-6a3d-4169-9968-81f204f0949a",
   "metadata": {},
   "source": [
    "### Step 2: Training Multiple Machine Learning Models  \n",
    "Trains Random Forest, Gradient Boosting, and Logistic Regression models using 5-fold cross-validation, calculating and displaying key classification metrics for comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630bce20-95a8-44b2-aaf2-1b3ddef15fe4",
   "metadata": {},
   "source": [
    "### Step 3: Model Evaluation, Visualization, and Selection  \n",
    "Evaluates all trained models, visualizes ROC curves and confusion matrices, selects the best model based on F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "326b26a8-1f27-4c0d-9b3e-3eba384fcb8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Customer Churn Prediction - Data Preprocessing Pipeline\n",
      "\n",
      "This Python module implements a data preprocessing pipeline for customer churn prediction, handling common tasks like data loading, missing value imputation, feature encoding, normalization, and train-test splitting.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```bash\n",
      "pip install pandas numpy scikit-learn\n",
      "```\n",
      "\n",
      "## Code Implementation\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Customer Churn Prediction - Data Preprocessing Module\n",
      "\n",
      "This module provides functions for loading and preprocessing customer data\n",
      "for machine learning-based churn prediction models.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.compose import ColumnTransformer\n",
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "def create_sample_churn_data(file_path='customer_data.csv', num_samples=1000, seed=42):\n",
      "    \"\"\"\n",
      "    Create a realistic sample dataset for customer churn prediction.\n",
      "    \n",
      "    Args:\n",
      "        file_path (str): Path where the CSV file will be saved\n",
      "        num_samples (int): Number of customer records to generate\n",
      "        seed (int): Random seed for reproducibility\n",
      "        \n",
      "    Returns:\n",
      "        str: Path to the created CSV file\n",
      "    \"\"\"\n",
      "    np.random.seed(seed)\n",
      "    \n",
      "    # Define realistic data characteristics\n",
      "    customer_ids = np.arange(1000, 1000 + num_samples)\n",
      "    \n",
      "    # Generate realistic customer features\n",
      "    tenure = np.random.gamma(shape=3, scale=12, size=num_samples).astype(int)  # Tenure in months (0-72)\n",
      "    monthly_charges = 30 + np.random.gamma(shape=2, scale=20, size=num_samples).round(2)  # $30-$150\n",
      "    total_charges = (tenure * monthly_charges).round(2)\n",
      "    \n",
      "    # Some customers have low usage\n",
      "    internet_service = np.random.choice(['DSL', 'Fiber optic', 'No'], p=[0.4, 0.4, 0.2], size=num_samples)\n",
      "    \n",
      "    # Payment and contract info\n",
      "    contract = np.random.choice(['Month-to-month', 'One year', 'Two year'], p=[0.55, 0.25, 0.2], size=num_samples)\n",
      "    payment_method = np.random.choice(\n",
      "        ['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'], \n",
      "        p=[0.35, 0.25, 0.2, 0.2], \n",
      "        size=num_samples\n",
      "    )\n",
      "    paperless_billing = np.random.choice(['Yes', 'No'], p=[0.6, 0.4], size=num_samples)\n",
      "    \n",
      "    # Services\n",
      "    phone_service = np.random.choice(['Yes', 'No'], p=[0.9, 0.1], size=num_samples)\n",
      "    online_security = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
      "    online_backup = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
      "    tech_support = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
      "    \n",
      "    # Generate realistic churn based on known factors\n",
      "    # Customers with month-to-month contracts and high charges are more likely to churn\n",
      "    churn_prob = 0.2 * np.ones(num_samples)  # Base churn rate 20%\n",
      "    \n",
      "    # Adjust churn probability based on contract type\n",
      "    churn_prob[contract == 'Month-to-month'] += 0.2\n",
      "    churn_prob[contract == 'Two year'] -= 0.15\n",
      "    \n",
      "    # Adjust based on tenure\n",
      "    churn_prob[tenure > 30] -= 0.1\n",
      "    churn_prob[tenure < 6] += 0.1\n",
      "    \n",
      "    # Adjust based on monthly charges\n",
      "    churn_prob[monthly_charges > 80] += 0.1\n",
      "    \n",
      "    # Clip probabilities to valid range\n",
      "    churn_prob = np.clip(churn_prob, 0.05, 0.95)\n",
      "    \n",
      "    # Generate churn outcome\n",
      "    churn = np.random.binomial(1, churn_prob).astype(bool)\n",
      "    churn = np.where(churn, 'Yes', 'No')\n",
      "    \n",
      "    # Create pandas DataFrame\n",
      "    df = pd.DataFrame({\n",
      "        'CustomerID': customer_ids,\n",
      "        'Tenure': tenure,\n",
      "        'PhoneService': phone_service,\n",
      "        'InternetService': internet_service,\n",
      "        'OnlineSecurity': online_security,\n",
      "        'OnlineBackup': online_backup,\n",
      "        'TechSupport': tech_support,\n",
      "        'Contract': contract,\n",
      "        'PaperlessBilling': paperless_billing,\n",
      "        'PaymentMethod': payment_method,\n",
      "        'MonthlyCharges': monthly_charges,\n",
      "        'TotalCharges': total_charges,\n",
      "        'Churn': churn\n",
      "    })\n",
      "    \n",
      "    # Add some missing values to make the dataset more realistic\n",
      "    # Around 5% of the data will have missing values\n",
      "    for col in ['OnlineSecurity', 'TechSupport', 'TotalCharges']:\n",
      "        mask = np.random.choice([True, False], size=num_samples, p=[0.05, 0.95])\n",
      "        df.loc[mask, col] = np.nan\n",
      "    \n",
      "    # Save to CSV\n",
      "    df.to_csv(file_path, index=False)\n",
      "    print(f\"Generated sample customer churn dataset with {num_samples} records at {file_path}\")\n",
      "    \n",
      "    return file_path\n",
      "\n",
      "\n",
      "def preprocess_customer_data(data_path=None, test_size=0.2, random_state=42):\n",
      "    \"\"\"\n",
      "    Load and preprocess customer data for churn prediction modeling.\n",
      "    \n",
      "    This function performs the following preprocessing steps:\n",
      "    1. Loads customer data from a CSV file (or creates a sample if path not provided)\n",
      "    2. Handles missing values using appropriate imputation techniques\n",
      "    3. Encodes categorical variables using one-hot encoding\n",
      "    4. Normalizes numerical features using standardization\n",
      "    5. Splits the dataset into training and test sets\n",
      "    \n",
      "    Args:\n",
      "        data_path (str, optional): Path to the customer data CSV file. \n",
      "                                  If None, generates sample data.\n",
      "        test_size (float): Proportion of the dataset to include in the test split (0-1)\n",
      "        random_state (int): Seed for random number generators to ensure reproducibility\n",
      "        \n",
      "    Returns:\n",
      "        dict: Contains preprocessed data and processing objects:\n",
      "            - X_train: Training features (normalized and encoded)\n",
      "            - X_test: Test features (normalized and encoded)\n",
      "            - y_train: Training target values\n",
      "            - y_test: Test target values\n",
      "            - preprocessing_pipeline: The fitted scikit-learn preprocessing pipeline\n",
      "            - feature_names: Names of the processed features\n",
      "    \"\"\"\n",
      "    # Step 1: Load or generate the data\n",
      "    if data_path is None or not os.path.exists(data_path):\n",
      "        print(\"Data file not found, generating sample data...\")\n",
      "        data_path = create_sample_churn_data()\n",
      "    \n",
      "    # Load the dataset\n",
      "    print(f\"Loading customer data from {data_path}\")\n",
      "    customer_data = pd.read_csv(data_path)\n",
      "    \n",
      "    # Display basic information about the dataset\n",
      "    print(f\"\\nDataset shape: {customer_data.shape}\")\n",
      "    print(\"\\nSample data:\")\n",
      "    print(customer_data.head())\n",
      "    \n",
      "    # Check for missing values\n",
      "    missing_values = customer_data.isnull().sum()\n",
      "    print(\"\\nMissing values per column:\")\n",
      "    print(missing_values[missing_values > 0])\n",
      "    \n",
      "    # Separate features from target\n",
      "    X = customer_data.drop(['Churn', 'CustomerID'], axis=1)  # CustomerID is not predictive\n",
      "    y = (customer_data['Churn'] == 'Yes').astype(int)  # Convert to binary 0/1\n",
      "    \n",
      "    # Identify categorical and numerical columns\n",
      "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
      "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
      "    \n",
      "    print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
      "    print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
      "    \n",
      "    # Step 2 & 3 & 4: Create preprocessing pipelines\n",
      "    # For numerical features: impute missing values with median and then standardize\n",
      "    numerical_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
      "        ('scaler', StandardScaler())  # Normalize features\n",
      "    ])\n",
      "    \n",
      "    # For categorical features: impute missing values with most frequent and then one-hot encode\n",
      "    categorical_transformer = Pipeline(steps=[\n",
      "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
      "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))  # Encode categories\n",
      "    ])\n",
      "    \n",
      "    # Combine preprocessing steps\n",
      "    preprocessor = ColumnTransformer(\n",
      "        transformers=[\n",
      "            ('num', numerical_transformer, numerical_cols),\n",
      "            ('cat', categorical_transformer, categorical_cols)\n",
      "        ],\n",
      "        remainder='drop'  # Drop any columns not specified\n",
      "    )\n",
      "    \n",
      "    # Step 5: Split the data into training and test sets\n",
      "    X_train, X_test, y_train, y_test = train_test_split(\n",
      "        X, y, \n",
      "        test_size=test_size, \n",
      "        random_state=random_state, \n",
      "        stratify=y  # Maintain same proportion of churn in both splits\n",
      "    )\n",
      "    \n",
      "    print(f\"\\nSplit dataset into training ({X_train.shape[0]} samples) and test ({X_test.shape[0]} samples) sets\")\n",
      "    \n",
      "    # Apply preprocessing to the data\n",
      "    print(\"\\nApplying preprocessing transformations...\")\n",
      "    X_train_processed = preprocessor.fit_transform(X_train)\n",
      "    X_test_processed = preprocessor.transform(X_test)\n",
      "    \n",
      "    # Get feature names after one-hot encoding\n",
      "    feature_names = get_feature_names(preprocessor, numerical_cols, categorical_cols)\n",
      "    \n",
      "    print(f\"Processed data shape: {X_train_processed.shape} with {len(feature_names)} features\")\n",
      "    \n",
      "    # Return processed data and preprocessing objects\n",
      "    return {\n",
      "        'X_train': X_train_processed,\n",
      "        'X_test': X_test_processed,\n",
      "        'y_train': y_train,\n",
      "        'y_test': y_test,\n",
      "        'preprocessing_pipeline': preprocessor,\n",
      "        'feature_names': feature_names\n",
      "    }\n",
      "\n",
      "\n",
      "def get_feature_names(preprocessor, numerical_cols, categorical_cols):\n",
      "    \"\"\"\n",
      "    Get feature names from the column transformer's preprocessing pipeline.\n",
      "    \n",
      "    Args:\n",
      "        preprocessor: Fitted ColumnTransformer object\n",
      "        numerical_cols: List of numerical column names\n",
      "        categorical_cols: List of categorical column names\n",
      "        \n",
      "    Returns:\n",
      "        list: List of transformed feature names\n",
      "    \"\"\"\n",
      "    # Get feature names for numerical columns (unchanged)\n",
      "    feature_names = list(numerical_cols)\n",
      "    \n",
      "    # Get one-hot encoded feature names for categorical columns\n",
      "    try:\n",
      "        # For sklearn >= 1.0\n",
      "        ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
      "    except AttributeError:\n",
      "        # For sklearn < 1.0\n",
      "        ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_cols)\n",
      "        \n",
      "    feature_names.extend(ohe_feature_names)\n",
      "    \n",
      "    return feature_names\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to demonstrate the preprocessing pipeline.\"\"\"\n",
      "    print(\"Customer Churn Prediction - Data Preprocessing Demo\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Process the data\n",
      "    preprocessed_data = preprocess_customer_data()\n",
      "    \n",
      "    # Show preprocessing results\n",
      "    print(\"\\nPreprocessing complete!\")\n",
      "    print(f\"Transformed X_train shape: {preprocessed_data['X_train'].shape}\")\n",
      "    print(f\"Transformed X_test shape: {preprocessed_data['X_test'].shape}\")\n",
      "    \n",
      "    # Display some sample feature names after transformation\n",
      "    print(\"\\nSample of transformed features:\")\n",
      "    sample_features = preprocessed_data['feature_names'][:10]\n",
      "    for i, feature in enumerate(sample_features, 1):\n",
      "        print(f\"{i}. {feature}\")\n",
      "    \n",
      "    print(\"\\nThe preprocessed data is ready for model training!\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "### Overview\n",
      "\n",
      "This script implements a complete data preprocessing pipeline for customer churn prediction. It includes:\n",
      "\n",
      "1. **Sample data generation** - Creating a realistic customer churn dataset\n",
      "2. **Data loading** - Reading the dataset from a CSV file\n",
      "3. **Missing value handling** - Using appropriate imputation techniques\n",
      "4. **Feature encoding** - Converting categorical variables to numerical format\n",
      "5. **Feature normalization** - Scaling numerical features to have similar ranges\n",
      "6. **Train-test splitting** - Dividing the data for model training and evaluation\n",
      "\n",
      "### Key Functions\n",
      "\n",
      "#### 1. `create_sample_churn_data(file_path, num_samples, seed)`\n",
      "\n",
      "This function generates a realistic customer churn dataset with typical telecom service features:\n",
      "\n",
      "- **Customer attributes**: Tenure, monthly charges, total charges\n",
      "- **Services**: Phone service, internet service, online security, tech support, etc.\n",
      "- **Contract details**: Contract type, payment method, paperless billing\n",
      "- **Target variable**: Churn (Yes/No)\n",
      "\n",
      "The function also introduces some missing values to make the dataset realistic and saves it to a CSV file.\n",
      "\n",
      "#### 2. `preprocess_customer_data(data_path, test_size, random_state)`\n",
      "\n",
      "The main preprocessing function that:\n",
      "\n",
      "1. **Loads data**: Either from the provided path or generates sample data\n",
      "2. **Examines the dataset**: Shows basic statistics and identifies feature types\n",
      "3. **Creates preprocessing pipeline**: Builds separate pipelines for numerical and categorical features\n",
      "4. **Splits the data**: Creates stratified training and test sets\n",
      "5. **Applies transformations**: Processes both sets consistently using scikit-learn's pipelines\n",
      "6. **Returns processed data**: Ready for model training and evaluation\n",
      "\n",
      "#### 3. `get_feature_names(preprocessor, numerical_cols, categorical_cols)`\n",
      "\n",
      "Helper function to extract the names of the transformed features, accounting for one-hot encoding of categorical variables.\n",
      "\n",
      "### Preprocessing Steps in Detail\n",
      "\n",
      "1. **Missing value imputation**:\n",
      "   - For numerical features: Replace with median values\n",
      "   - For categorical features: Replace with most frequent values\n",
      "\n",
      "2. **Categorical encoding**:\n",
      "   - One-hot encoding is used for categorical variables\n",
      "   - The `handle_unknown='ignore'` parameter ensures the pipeline can handle new categories at prediction time\n",
      "\n",
      "3. **Feature scaling**:\n",
      "   - StandardScaler normalizes numerical features to have zero mean and unit variance\n",
      "   - This ensures all features contribute equally to model training\n",
      "\n",
      "4. **Train-test splitting**:\n",
      "   - Stratified split maintains the same proportion of churned customers in both sets\n",
      "   - Default split is 80% training, 20% testing\n",
      "\n",
      "## How to Use This Code\n",
      "\n",
      "### Basic Usage\n",
      "\n",
      "```python\n",
      "from churn_preprocessing import preprocess_customer_data\n",
      "\n",
      "# Option 1: Generate and process sample data\n",
      "processed_data = preprocess_customer_data()\n",
      "\n",
      "# Option 2: Process your own data file\n",
      "# processed_data = preprocess_customer_data('your_customer_data.csv')\n",
      "\n",
      "# Access the processed data components\n",
      "X_train = processed_data['X_train']\n",
      "X_test = processed_data['X_test']\n",
      "y_train = processed_data['y_train']\n",
      "y_test = processed_data['y_test']\n",
      "\n",
      "# The preprocessing pipeline can be used for future predictions\n",
      "preprocessor = processed_data['preprocessing_pipeline']\n",
      "```\n",
      "\n",
      "### Integration with Model Training\n",
      "\n",
      "```python\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.metrics import classification_report\n",
      "\n",
      "# Get preprocessed data\n",
      "processed_data = preprocess_customer_data()\n",
      "\n",
      "# Train a model\n",
      "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
      "model.fit(processed_data['X_train'], processed_data['y_train'])\n",
      "\n",
      "# Evaluate\n",
      "y_pred = model.predict(processed_data['X_test'])\n",
      "print(classification_report(processed_data['y_test'], y_pred))\n",
      "```\n",
      "\n",
      "## Running the Script\n",
      "\n",
      "1. Save the code to a file named `churn_preprocessing.py`\n",
      "2. Install required libraries:\n",
      "   ```bash\n",
      "   pip install pandas numpy scikit-learn\n",
      "   ```\n",
      "3. Run the script:\n",
      "   ```bash\n",
      "   python churn_preprocessing.py\n",
      "   ```\n",
      "\n",
      "## Expected Output\n",
      "\n",
      "When run, the script will:\n",
      "1. Generate a sample customer churn dataset\n",
      "2. Display information about the data\n",
      "3. Perform preprocessing steps\n",
      "4. Report the shapes and features of the processed datasets\n",
      "\n",
      "You'll see console output showing:\n",
      "- Dataset statistics\n",
      "- Missing value counts\n",
      "- Feature type identification\n",
      "- Training/test split sizes\n",
      "- Transformed feature names\n",
      "\n",
      "## Extensions and Next Steps\n",
      "\n",
      "After preprocessing, you can:\n",
      "1. Train various machine learning models (e.g., Logistic Regression, Random Forest, XGBoost)\n",
      "2. Perform feature importance analysis\n",
      "3. Tune hyperparameters using cross-validation\n",
      "4. Evaluate model performance with appropriate metrics (AUC-ROC, precision-recall)\n",
      "5. Create a prediction pipeline that includes preprocessing and model prediction\n",
      "\n",
      "## Notes\n",
      "\n",
      "- The preprocessing pipeline is designed to be reusable for future predictions\n",
      "- All transformations learned on the training set are applied consistently to the test set\n",
      "- The pipeline handles both numerical and categorical features appropriately\n",
      "- Stratified sampling ensures balanced class distribution in training and test sets"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Customer Churn Prediction - Data Preprocessing Pipeline\n",
       "\n",
       "This Python module implements a data preprocessing pipeline for customer churn prediction, handling common tasks like data loading, missing value imputation, feature encoding, normalization, and train-test splitting.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```bash\n",
       "pip install pandas numpy scikit-learn\n",
       "```\n",
       "\n",
       "## Code Implementation\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "Customer Churn Prediction - Data Preprocessing Module\n",
       "\n",
       "This module provides functions for loading and preprocessing customer data\n",
       "for machine learning-based churn prediction models.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
       "from sklearn.impute import SimpleImputer\n",
       "from sklearn.compose import ColumnTransformer\n",
       "from sklearn.pipeline import Pipeline\n",
       "\n",
       "def create_sample_churn_data(file_path='customer_data.csv', num_samples=1000, seed=42):\n",
       "    \"\"\"\n",
       "    Create a realistic sample dataset for customer churn prediction.\n",
       "    \n",
       "    Args:\n",
       "        file_path (str): Path where the CSV file will be saved\n",
       "        num_samples (int): Number of customer records to generate\n",
       "        seed (int): Random seed for reproducibility\n",
       "        \n",
       "    Returns:\n",
       "        str: Path to the created CSV file\n",
       "    \"\"\"\n",
       "    np.random.seed(seed)\n",
       "    \n",
       "    # Define realistic data characteristics\n",
       "    customer_ids = np.arange(1000, 1000 + num_samples)\n",
       "    \n",
       "    # Generate realistic customer features\n",
       "    tenure = np.random.gamma(shape=3, scale=12, size=num_samples).astype(int)  # Tenure in months (0-72)\n",
       "    monthly_charges = 30 + np.random.gamma(shape=2, scale=20, size=num_samples).round(2)  # $30-$150\n",
       "    total_charges = (tenure * monthly_charges).round(2)\n",
       "    \n",
       "    # Some customers have low usage\n",
       "    internet_service = np.random.choice(['DSL', 'Fiber optic', 'No'], p=[0.4, 0.4, 0.2], size=num_samples)\n",
       "    \n",
       "    # Payment and contract info\n",
       "    contract = np.random.choice(['Month-to-month', 'One year', 'Two year'], p=[0.55, 0.25, 0.2], size=num_samples)\n",
       "    payment_method = np.random.choice(\n",
       "        ['Electronic check', 'Mailed check', 'Bank transfer (automatic)', 'Credit card (automatic)'], \n",
       "        p=[0.35, 0.25, 0.2, 0.2], \n",
       "        size=num_samples\n",
       "    )\n",
       "    paperless_billing = np.random.choice(['Yes', 'No'], p=[0.6, 0.4], size=num_samples)\n",
       "    \n",
       "    # Services\n",
       "    phone_service = np.random.choice(['Yes', 'No'], p=[0.9, 0.1], size=num_samples)\n",
       "    online_security = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
       "    online_backup = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
       "    tech_support = np.random.choice(['Yes', 'No', 'No internet service'], size=num_samples)\n",
       "    \n",
       "    # Generate realistic churn based on known factors\n",
       "    # Customers with month-to-month contracts and high charges are more likely to churn\n",
       "    churn_prob = 0.2 * np.ones(num_samples)  # Base churn rate 20%\n",
       "    \n",
       "    # Adjust churn probability based on contract type\n",
       "    churn_prob[contract == 'Month-to-month'] += 0.2\n",
       "    churn_prob[contract == 'Two year'] -= 0.15\n",
       "    \n",
       "    # Adjust based on tenure\n",
       "    churn_prob[tenure > 30] -= 0.1\n",
       "    churn_prob[tenure < 6] += 0.1\n",
       "    \n",
       "    # Adjust based on monthly charges\n",
       "    churn_prob[monthly_charges > 80] += 0.1\n",
       "    \n",
       "    # Clip probabilities to valid range\n",
       "    churn_prob = np.clip(churn_prob, 0.05, 0.95)\n",
       "    \n",
       "    # Generate churn outcome\n",
       "    churn = np.random.binomial(1, churn_prob).astype(bool)\n",
       "    churn = np.where(churn, 'Yes', 'No')\n",
       "    \n",
       "    # Create pandas DataFrame\n",
       "    df = pd.DataFrame({\n",
       "        'CustomerID': customer_ids,\n",
       "        'Tenure': tenure,\n",
       "        'PhoneService': phone_service,\n",
       "        'InternetService': internet_service,\n",
       "        'OnlineSecurity': online_security,\n",
       "        'OnlineBackup': online_backup,\n",
       "        'TechSupport': tech_support,\n",
       "        'Contract': contract,\n",
       "        'PaperlessBilling': paperless_billing,\n",
       "        'PaymentMethod': payment_method,\n",
       "        'MonthlyCharges': monthly_charges,\n",
       "        'TotalCharges': total_charges,\n",
       "        'Churn': churn\n",
       "    })\n",
       "    \n",
       "    # Add some missing values to make the dataset more realistic\n",
       "    # Around 5% of the data will have missing values\n",
       "    for col in ['OnlineSecurity', 'TechSupport', 'TotalCharges']:\n",
       "        mask = np.random.choice([True, False], size=num_samples, p=[0.05, 0.95])\n",
       "        df.loc[mask, col] = np.nan\n",
       "    \n",
       "    # Save to CSV\n",
       "    df.to_csv(file_path, index=False)\n",
       "    print(f\"Generated sample customer churn dataset with {num_samples} records at {file_path}\")\n",
       "    \n",
       "    return file_path\n",
       "\n",
       "\n",
       "def preprocess_customer_data(data_path=None, test_size=0.2, random_state=42):\n",
       "    \"\"\"\n",
       "    Load and preprocess customer data for churn prediction modeling.\n",
       "    \n",
       "    This function performs the following preprocessing steps:\n",
       "    1. Loads customer data from a CSV file (or creates a sample if path not provided)\n",
       "    2. Handles missing values using appropriate imputation techniques\n",
       "    3. Encodes categorical variables using one-hot encoding\n",
       "    4. Normalizes numerical features using standardization\n",
       "    5. Splits the dataset into training and test sets\n",
       "    \n",
       "    Args:\n",
       "        data_path (str, optional): Path to the customer data CSV file. \n",
       "                                  If None, generates sample data.\n",
       "        test_size (float): Proportion of the dataset to include in the test split (0-1)\n",
       "        random_state (int): Seed for random number generators to ensure reproducibility\n",
       "        \n",
       "    Returns:\n",
       "        dict: Contains preprocessed data and processing objects:\n",
       "            - X_train: Training features (normalized and encoded)\n",
       "            - X_test: Test features (normalized and encoded)\n",
       "            - y_train: Training target values\n",
       "            - y_test: Test target values\n",
       "            - preprocessing_pipeline: The fitted scikit-learn preprocessing pipeline\n",
       "            - feature_names: Names of the processed features\n",
       "    \"\"\"\n",
       "    # Step 1: Load or generate the data\n",
       "    if data_path is None or not os.path.exists(data_path):\n",
       "        print(\"Data file not found, generating sample data...\")\n",
       "        data_path = create_sample_churn_data()\n",
       "    \n",
       "    # Load the dataset\n",
       "    print(f\"Loading customer data from {data_path}\")\n",
       "    customer_data = pd.read_csv(data_path)\n",
       "    \n",
       "    # Display basic information about the dataset\n",
       "    print(f\"\\nDataset shape: {customer_data.shape}\")\n",
       "    print(\"\\nSample data:\")\n",
       "    print(customer_data.head())\n",
       "    \n",
       "    # Check for missing values\n",
       "    missing_values = customer_data.isnull().sum()\n",
       "    print(\"\\nMissing values per column:\")\n",
       "    print(missing_values[missing_values > 0])\n",
       "    \n",
       "    # Separate features from target\n",
       "    X = customer_data.drop(['Churn', 'CustomerID'], axis=1)  # CustomerID is not predictive\n",
       "    y = (customer_data['Churn'] == 'Yes').astype(int)  # Convert to binary 0/1\n",
       "    \n",
       "    # Identify categorical and numerical columns\n",
       "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
       "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
       "    \n",
       "    print(f\"\\nCategorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
       "    print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
       "    \n",
       "    # Step 2 & 3 & 4: Create preprocessing pipelines\n",
       "    # For numerical features: impute missing values with median and then standardize\n",
       "    numerical_transformer = Pipeline(steps=[\n",
       "        ('imputer', SimpleImputer(strategy='median')),  # Handle missing values\n",
       "        ('scaler', StandardScaler())  # Normalize features\n",
       "    ])\n",
       "    \n",
       "    # For categorical features: impute missing values with most frequent and then one-hot encode\n",
       "    categorical_transformer = Pipeline(steps=[\n",
       "        ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
       "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))  # Encode categories\n",
       "    ])\n",
       "    \n",
       "    # Combine preprocessing steps\n",
       "    preprocessor = ColumnTransformer(\n",
       "        transformers=[\n",
       "            ('num', numerical_transformer, numerical_cols),\n",
       "            ('cat', categorical_transformer, categorical_cols)\n",
       "        ],\n",
       "        remainder='drop'  # Drop any columns not specified\n",
       "    )\n",
       "    \n",
       "    # Step 5: Split the data into training and test sets\n",
       "    X_train, X_test, y_train, y_test = train_test_split(\n",
       "        X, y, \n",
       "        test_size=test_size, \n",
       "        random_state=random_state, \n",
       "        stratify=y  # Maintain same proportion of churn in both splits\n",
       "    )\n",
       "    \n",
       "    print(f\"\\nSplit dataset into training ({X_train.shape[0]} samples) and test ({X_test.shape[0]} samples) sets\")\n",
       "    \n",
       "    # Apply preprocessing to the data\n",
       "    print(\"\\nApplying preprocessing transformations...\")\n",
       "    X_train_processed = preprocessor.fit_transform(X_train)\n",
       "    X_test_processed = preprocessor.transform(X_test)\n",
       "    \n",
       "    # Get feature names after one-hot encoding\n",
       "    feature_names = get_feature_names(preprocessor, numerical_cols, categorical_cols)\n",
       "    \n",
       "    print(f\"Processed data shape: {X_train_processed.shape} with {len(feature_names)} features\")\n",
       "    \n",
       "    # Return processed data and preprocessing objects\n",
       "    return {\n",
       "        'X_train': X_train_processed,\n",
       "        'X_test': X_test_processed,\n",
       "        'y_train': y_train,\n",
       "        'y_test': y_test,\n",
       "        'preprocessing_pipeline': preprocessor,\n",
       "        'feature_names': feature_names\n",
       "    }\n",
       "\n",
       "\n",
       "def get_feature_names(preprocessor, numerical_cols, categorical_cols):\n",
       "    \"\"\"\n",
       "    Get feature names from the column transformer's preprocessing pipeline.\n",
       "    \n",
       "    Args:\n",
       "        preprocessor: Fitted ColumnTransformer object\n",
       "        numerical_cols: List of numerical column names\n",
       "        categorical_cols: List of categorical column names\n",
       "        \n",
       "    Returns:\n",
       "        list: List of transformed feature names\n",
       "    \"\"\"\n",
       "    # Get feature names for numerical columns (unchanged)\n",
       "    feature_names = list(numerical_cols)\n",
       "    \n",
       "    # Get one-hot encoded feature names for categorical columns\n",
       "    try:\n",
       "        # For sklearn >= 1.0\n",
       "        ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
       "    except AttributeError:\n",
       "        # For sklearn < 1.0\n",
       "        ohe_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_cols)\n",
       "        \n",
       "    feature_names.extend(ohe_feature_names)\n",
       "    \n",
       "    return feature_names\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"Main function to demonstrate the preprocessing pipeline.\"\"\"\n",
       "    print(\"Customer Churn Prediction - Data Preprocessing Demo\")\n",
       "    print(\"=\"*60)\n",
       "    \n",
       "    # Process the data\n",
       "    preprocessed_data = preprocess_customer_data()\n",
       "    \n",
       "    # Show preprocessing results\n",
       "    print(\"\\nPreprocessing complete!\")\n",
       "    print(f\"Transformed X_train shape: {preprocessed_data['X_train'].shape}\")\n",
       "    print(f\"Transformed X_test shape: {preprocessed_data['X_test'].shape}\")\n",
       "    \n",
       "    # Display some sample feature names after transformation\n",
       "    print(\"\\nSample of transformed features:\")\n",
       "    sample_features = preprocessed_data['feature_names'][:10]\n",
       "    for i, feature in enumerate(sample_features, 1):\n",
       "        print(f\"{i}. {feature}\")\n",
       "    \n",
       "    print(\"\\nThe preprocessed data is ready for model training!\")\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "### Overview\n",
       "\n",
       "This script implements a complete data preprocessing pipeline for customer churn prediction. It includes:\n",
       "\n",
       "1. **Sample data generation** - Creating a realistic customer churn dataset\n",
       "2. **Data loading** - Reading the dataset from a CSV file\n",
       "3. **Missing value handling** - Using appropriate imputation techniques\n",
       "4. **Feature encoding** - Converting categorical variables to numerical format\n",
       "5. **Feature normalization** - Scaling numerical features to have similar ranges\n",
       "6. **Train-test splitting** - Dividing the data for model training and evaluation\n",
       "\n",
       "### Key Functions\n",
       "\n",
       "#### 1. `create_sample_churn_data(file_path, num_samples, seed)`\n",
       "\n",
       "This function generates a realistic customer churn dataset with typical telecom service features:\n",
       "\n",
       "- **Customer attributes**: Tenure, monthly charges, total charges\n",
       "- **Services**: Phone service, internet service, online security, tech support, etc.\n",
       "- **Contract details**: Contract type, payment method, paperless billing\n",
       "- **Target variable**: Churn (Yes/No)\n",
       "\n",
       "The function also introduces some missing values to make the dataset realistic and saves it to a CSV file.\n",
       "\n",
       "#### 2. `preprocess_customer_data(data_path, test_size, random_state)`\n",
       "\n",
       "The main preprocessing function that:\n",
       "\n",
       "1. **Loads data**: Either from the provided path or generates sample data\n",
       "2. **Examines the dataset**: Shows basic statistics and identifies feature types\n",
       "3. **Creates preprocessing pipeline**: Builds separate pipelines for numerical and categorical features\n",
       "4. **Splits the data**: Creates stratified training and test sets\n",
       "5. **Applies transformations**: Processes both sets consistently using scikit-learn's pipelines\n",
       "6. **Returns processed data**: Ready for model training and evaluation\n",
       "\n",
       "#### 3. `get_feature_names(preprocessor, numerical_cols, categorical_cols)`\n",
       "\n",
       "Helper function to extract the names of the transformed features, accounting for one-hot encoding of categorical variables.\n",
       "\n",
       "### Preprocessing Steps in Detail\n",
       "\n",
       "1. **Missing value imputation**:\n",
       "   - For numerical features: Replace with median values\n",
       "   - For categorical features: Replace with most frequent values\n",
       "\n",
       "2. **Categorical encoding**:\n",
       "   - One-hot encoding is used for categorical variables\n",
       "   - The `handle_unknown='ignore'` parameter ensures the pipeline can handle new categories at prediction time\n",
       "\n",
       "3. **Feature scaling**:\n",
       "   - StandardScaler normalizes numerical features to have zero mean and unit variance\n",
       "   - This ensures all features contribute equally to model training\n",
       "\n",
       "4. **Train-test splitting**:\n",
       "   - Stratified split maintains the same proportion of churned customers in both sets\n",
       "   - Default split is 80% training, 20% testing\n",
       "\n",
       "## How to Use This Code\n",
       "\n",
       "### Basic Usage\n",
       "\n",
       "```python\n",
       "from churn_preprocessing import preprocess_customer_data\n",
       "\n",
       "# Option 1: Generate and process sample data\n",
       "processed_data = preprocess_customer_data()\n",
       "\n",
       "# Option 2: Process your own data file\n",
       "# processed_data = preprocess_customer_data('your_customer_data.csv')\n",
       "\n",
       "# Access the processed data components\n",
       "X_train = processed_data['X_train']\n",
       "X_test = processed_data['X_test']\n",
       "y_train = processed_data['y_train']\n",
       "y_test = processed_data['y_test']\n",
       "\n",
       "# The preprocessing pipeline can be used for future predictions\n",
       "preprocessor = processed_data['preprocessing_pipeline']\n",
       "```\n",
       "\n",
       "### Integration with Model Training\n",
       "\n",
       "```python\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.metrics import classification_report\n",
       "\n",
       "# Get preprocessed data\n",
       "processed_data = preprocess_customer_data()\n",
       "\n",
       "# Train a model\n",
       "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
       "model.fit(processed_data['X_train'], processed_data['y_train'])\n",
       "\n",
       "# Evaluate\n",
       "y_pred = model.predict(processed_data['X_test'])\n",
       "print(classification_report(processed_data['y_test'], y_pred))\n",
       "```\n",
       "\n",
       "## Running the Script\n",
       "\n",
       "1. Save the code to a file named `churn_preprocessing.py`\n",
       "2. Install required libraries:\n",
       "   ```bash\n",
       "   pip install pandas numpy scikit-learn\n",
       "   ```\n",
       "3. Run the script:\n",
       "   ```bash\n",
       "   python churn_preprocessing.py\n",
       "   ```\n",
       "\n",
       "## Expected Output\n",
       "\n",
       "When run, the script will:\n",
       "1. Generate a sample customer churn dataset\n",
       "2. Display information about the data\n",
       "3. Perform preprocessing steps\n",
       "4. Report the shapes and features of the processed datasets\n",
       "\n",
       "You'll see console output showing:\n",
       "- Dataset statistics\n",
       "- Missing value counts\n",
       "- Feature type identification\n",
       "- Training/test split sizes\n",
       "- Transformed feature names\n",
       "\n",
       "## Extensions and Next Steps\n",
       "\n",
       "After preprocessing, you can:\n",
       "1. Train various machine learning models (e.g., Logistic Regression, Random Forest, XGBoost)\n",
       "2. Perform feature importance analysis\n",
       "3. Tune hyperparameters using cross-validation\n",
       "4. Evaluate model performance with appropriate metrics (AUC-ROC, precision-recall)\n",
       "5. Create a prediction pipeline that includes preprocessing and model prediction\n",
       "\n",
       "## Notes\n",
       "\n",
       "- The preprocessing pipeline is designed to be reusable for future predictions\n",
       "- All transformations learned on the training set are applied consistently to the test set\n",
       "- The pipeline handles both numerical and categorical features appropriately\n",
       "- Stratified sampling ensures balanced class distribution in training and test sets"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Customer Churn Model Training and Evaluation\n",
      "\n",
      "This Python module builds on the preprocessing pipeline to train and evaluate multiple machine learning models for customer churn prediction.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```bash\n",
      "pip install pandas numpy scikit-learn matplotlib seaborn\n",
      "```\n",
      "\n",
      "## Code Implementation\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Customer Churn Prediction - Model Training and Evaluation Module\n",
      "\n",
      "This module implements training and evaluation of multiple machine learning models\n",
      "for customer churn prediction. It builds on the preprocessing module to train\n",
      "Random Forest, Gradient Boosting, and Logistic Regression models, and evaluates \n",
      "their performance using cross-validation.\n",
      "\"\"\"\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from time import time\n",
      "from datetime import datetime\n",
      "\n",
      "# Machine learning models\n",
      "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "# Evaluation metrics and tools\n",
      "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
      "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
      "from sklearn.model_selection import cross_validate, cross_val_predict\n",
      "\n",
      "# Import preprocessing module\n",
      "from churn_preprocessing import preprocess_customer_data\n",
      "\n",
      "\n",
      "def train_and_evaluate_models(preprocessed_data, cv_folds=5, random_state=42):\n",
      "    \"\"\"\n",
      "    Train and evaluate multiple machine learning models using cross-validation.\n",
      "    \n",
      "    This function:\n",
      "    1. Sets up multiple classifier models\n",
      "    2. Trains each model using cross-validation\n",
      "    3. Evaluates models using various performance metrics\n",
      "    4. Returns detailed performance results\n",
      "    \n",
      "    Args:\n",
      "        preprocessed_data (dict): Output from preprocess_customer_data() containing:\n",
      "                                - X_train: Training features\n",
      "                                - y_train: Training target values\n",
      "                                - X_test: Test features\n",
      "                                - y_test: Test target values\n",
      "        cv_folds (int): Number of cross-validation folds\n",
      "        random_state (int): Random seed for reproducibility\n",
      "        \n",
      "    Returns:\n",
      "        tuple: Contains:\n",
      "            - results_df (pd.DataFrame): DataFrame with performance metrics for each model\n",
      "            - trained_models (dict): Dictionary of trained models\n",
      "            - cv_results (dict): Detailed cross-validation results for each model\n",
      "    \"\"\"\n",
      "    # Extract the data components\n",
      "    X_train = preprocessed_data['X_train']\n",
      "    y_train = preprocessed_data['y_train']\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Check if data is valid\n",
      "    if X_train is None or len(X_train) == 0:\n",
      "        raise ValueError(\"Training data is empty or None\")\n",
      "    \n",
      "    print(f\"\\nTraining and evaluating models with {cv_folds}-fold cross-validation...\")\n",
      "    \n",
      "    # Define the models to train\n",
      "    models = {\n",
      "        'Random Forest': RandomForestClassifier(\n",
      "            n_estimators=100,\n",
      "            max_depth=10,\n",
      "            min_samples_split=5,\n",
      "            random_state=random_state\n",
      "        ),\n",
      "        'Gradient Boosting': GradientBoostingClassifier(\n",
      "            n_estimators=100,\n",
      "            learning_rate=0.1,\n",
      "            max_depth=5,\n",
      "            random_state=random_state\n",
      "        ),\n",
      "        'Logistic Regression': LogisticRegression(\n",
      "            C=1.0,\n",
      "            max_iter=1000,\n",
      "            random_state=random_state\n",
      "        )\n",
      "    }\n",
      "    \n",
      "    # Define the metrics to evaluate\n",
      "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
      "    \n",
      "    # Dictionary to store results\n",
      "    cv_results = {}\n",
      "    trained_models = {}\n",
      "    results = []\n",
      "    \n",
      "    # Train and evaluate each model\n",
      "    for name, model in models.items():\n",
      "        print(f\"\\nTraining {name}...\")\n",
      "        start_time = time()\n",
      "        \n",
      "        # Perform cross-validation\n",
      "        cv_result = cross_validate(\n",
      "            model, \n",
      "            X_train, \n",
      "            y_train, \n",
      "            cv=cv_folds,\n",
      "            scoring=scoring,\n",
      "            return_train_score=True,\n",
      "            return_estimator=True\n",
      "        )\n",
      "        \n",
      "        # Store cross-validation results\n",
      "        cv_results[name] = cv_result\n",
      "        \n",
      "        # Train the model on the full training set\n",
      "        model.fit(X_train, y_train)\n",
      "        trained_models[name] = model\n",
      "        \n",
      "        # Make predictions on the test set\n",
      "        y_pred = model.predict(X_test)\n",
      "        \n",
      "        # Calculate metrics on the test set\n",
      "        test_accuracy = accuracy_score(y_test, y_pred)\n",
      "        test_precision = precision_score(y_test, y_pred)\n",
      "        test_recall = recall_score(y_test, y_pred)\n",
      "        test_f1 = f1_score(y_test, y_pred)\n",
      "        \n",
      "        # Calculate average cross-validation metrics\n",
      "        cv_accuracy = cv_result['test_accuracy'].mean()\n",
      "        cv_precision = cv_result['test_precision'].mean()\n",
      "        cv_recall = cv_result['test_recall'].mean()\n",
      "        cv_f1 = cv_result['test_f1'].mean()\n",
      "        \n",
      "        # Calculate standard deviations for cross-validation metrics\n",
      "        cv_accuracy_std = cv_result['test_accuracy'].std()\n",
      "        cv_precision_std = cv_result['test_precision'].std()\n",
      "        cv_recall_std = cv_result['test_recall'].std()\n",
      "        cv_f1_std = cv_result['test_f1'].std()\n",
      "        \n",
      "        # Store results\n",
      "        training_time = time() - start_time\n",
      "        \n",
      "        results.append({\n",
      "            'Model': name,\n",
      "            'CV Accuracy': f\"{cv_accuracy:.4f} ± {cv_accuracy_std:.4f}\",\n",
      "            'CV Precision': f\"{cv_precision:.4f} ± {cv_precision_std:.4f}\",\n",
      "            'CV Recall': f\"{cv_recall:.4f} ± {cv_recall_std:.4f}\",\n",
      "            'CV F1 Score': f\"{cv_f1:.4f} ± {cv_f1_std:.4f}\",\n",
      "            'Test Accuracy': f\"{test_accuracy:.4f}\",\n",
      "            'Test Precision': f\"{test_precision:.4f}\",\n",
      "            'Test Recall': f\"{test_recall:.4f}\",\n",
      "            'Test F1 Score': f\"{test_f1:.4f}\",\n",
      "            'Training Time (s)': f\"{training_time:.2f}\"\n",
      "        })\n",
      "        \n",
      "        print(f\"  Completed in {training_time:.2f} seconds\")\n",
      "        print(f\"  CV Accuracy: {cv_accuracy:.4f} ± {cv_accuracy_std:.4f}\")\n",
      "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
      "    \n",
      "    # Convert results to DataFrame for easier display\n",
      "    results_df = pd.DataFrame(results)\n",
      "    \n",
      "    return results_df, trained_models, cv_results\n",
      "\n",
      "\n",
      "def visualize_model_performance(results_df, cv_results, preprocessed_data):\n",
      "    \"\"\"\n",
      "    Create visualizations of model performance metrics and confusion matrices.\n",
      "    \n",
      "    Args:\n",
      "        results_df (pd.DataFrame): DataFrame with performance metrics\n",
      "        cv_results (dict): Cross-validation results for each model\n",
      "        preprocessed_data (dict): Preprocessed data including test set\n",
      "        \n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Set the style for plots\n",
      "    sns.set(style='whitegrid')\n",
      "    \n",
      "    # Extract test data for confusion matrices\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Create a figure for the bar chart of metrics\n",
      "    plt.figure(figsize=(14, 8))\n",
      "    \n",
      "    # Extract numeric values from formatted strings for plotting\n",
      "    metrics_to_plot = ['CV Accuracy', 'CV Precision', 'CV Recall', 'CV F1 Score']\n",
      "    plot_data = []\n",
      "    \n",
      "    for _, row in results_df.iterrows():\n",
      "        model_name = row['Model']\n",
      "        for metric in metrics_to_plot:\n",
      "            # Extract the mean value from the formatted string (e.g., \"0.8500 ± 0.0300\")\n",
      "            value = float(row[metric].split(' ±')[0])\n",
      "            plot_data.append({\n",
      "                'Model': model_name,\n",
      "                'Metric': metric.replace('CV ', ''),\n",
      "                'Value': value\n",
      "            })\n",
      "    \n",
      "    plot_df = pd.DataFrame(plot_data)\n",
      "    \n",
      "    # Create the grouped bar chart\n",
      "    ax = sns.barplot(x='Model', y='Value', hue='Metric', data=plot_df)\n",
      "    ax.set_title('Model Performance Comparison', fontsize=16)\n",
      "    ax.set_xlabel('Model', fontsize=14)\n",
      "    ax.set_ylabel('Score', fontsize=14)\n",
      "    ax.set_ylim([0, 1.0])\n",
      "    ax.legend(title='Metric', fontsize=12)\n",
      "    \n",
      "    # Add value labels on the bars\n",
      "    for container in ax.containers:\n",
      "        ax.bar_label(container, fmt='%.2f', fontsize=10)\n",
      "    \n",
      "    plt.tight_layout()\n",
      "    \n",
      "    # Save the figure\n",
      "    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
      "    \n",
      "    # Create confusion matrices for each model\n",
      "    fig, axes = plt.subplots(1, len(cv_results), figsize=(5*len(cv_results), 5))\n",
      "    \n",
      "    # If there's only one model, axes will not be an array\n",
      "    if len(cv_results) == 1:\n",
      "        axes = [axes]\n",
      "    \n",
      "    for i, (name, model) in enumerate(cv_results.items()):\n",
      "        # Get the last estimator from cross-validation\n",
      "        estimator = model['estimator'][-1]\n",
      "        \n",
      "        # Make predictions on the test set\n",
      "        y_pred = estimator.predict(X_test)\n",
      "        \n",
      "        # Calculate confusion matrix\n",
      "        cm = confusion_matrix(y_test, y_pred)\n",
      "        \n",
      "        # Plot confusion matrix\n",
      "        sns.heatmap(\n",
      "            cm, \n",
      "            annot=True, \n",
      "            fmt='d', \n",
      "            cmap='Blues',\n",
      "            ax=axes[i],\n",
      "            cbar=False\n",
      "        )\n",
      "        \n",
      "        axes[i].set_title(f'{name} Confusion Matrix', fontsize=14)\n",
      "        axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
      "        axes[i].set_ylabel('True Label', fontsize=12)\n",
      "        \n",
      "    plt.tight_layout()\n",
      "    \n",
      "    # Save the confusion matrix figure\n",
      "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
      "    \n",
      "    # Close plots to free memory\n",
      "    plt.close('all')\n",
      "    \n",
      "    print(\"\\nPerformance visualizations saved as 'model_performance_comparison.png' and 'confusion_matrices.png'\")\n",
      "\n",
      "\n",
      "def analyze_feature_importance(trained_models, preprocessed_data, top_n=10):\n",
      "    \"\"\"\n",
      "    Analyze feature importance for tree-based models.\n",
      "    \n",
      "    Args:\n",
      "        trained_models (dict): Dictionary of trained models\n",
      "        preprocessed_data (dict): Preprocessed data including feature names\n",
      "        top_n (int): Number of top features to display\n",
      "        \n",
      "    Returns:\n",
      "        dict: Dictionary with feature importance data for each model\n",
      "    \"\"\"\n",
      "    feature_names = preprocessed_data['feature_names']\n",
      "    importance_data = {}\n",
      "    \n",
      "    # Create a figure for feature importance\n",
      "    plt.figure(figsize=(12, 8))\n",
      "    \n",
      "    # Counter for subplots\n",
      "    plot_count = 0\n",
      "    \n",
      "    # Models that support feature importance\n",
      "    tree_models = ['Random Forest', 'Gradient Boosting']\n",
      "    \n",
      "    # Filter models\n",
      "    tree_model_dict = {name: model for name, model in trained_models.items() if name in tree_models}\n",
      "    \n",
      "    # If we have tree-based models\n",
      "    if tree_model_dict:\n",
      "        # Set up subplots\n",
      "        fig, axes = plt.subplots(len(tree_model_dict), 1, figsize=(12, 6*len(tree_model_dict)))\n",
      "        \n",
      "        # If there's only one model, axes will not be an array\n",
      "        if len(tree_model_dict) == 1:\n",
      "            axes = [axes]\n",
      "        \n",
      "        for i, (name, model) in enumerate(tree_model_dict.items()):\n",
      "            # Get feature importances\n",
      "            importances = model.feature_importances_\n",
      "            \n",
      "            # Create DataFrame for easier manipulation\n",
      "            importance_df = pd.DataFrame({\n",
      "                'Feature': feature_names,\n",
      "                'Importance': importances\n",
      "            }).sort_values(by='Importance', ascending=False)\n",
      "            \n",
      "            # Store in results dictionary\n",
      "            importance_data[name] = importance_df\n",
      "            \n",
      "            # Plot top N features\n",
      "            top_features = importance_df.head(top_n)\n",
      "            \n",
      "            # Create horizontal bar chart\n",
      "            sns.barplot(\n",
      "                x='Importance', \n",
      "                y='Feature', \n",
      "                data=top_features, \n",
      "                ax=axes[i],\n",
      "                palette='viridis'\n",
      "            )\n",
      "            \n",
      "            axes[i].set_title(f'Top {top_n} Features - {name}', fontsize=14)\n",
      "            axes[i].set_xlabel('Importance', fontsize=12)\n",
      "            axes[i].set_ylabel('Feature', fontsize=12)\n",
      "            \n",
      "        plt.tight_layout()\n",
      "        \n",
      "        # Save the feature importance figure\n",
      "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
      "        print(\"\\nFeature importance visualization saved as 'feature_importance.png'\")\n",
      "    \n",
      "    # Close plots to free memory\n",
      "    plt.close('all')\n",
      "    \n",
      "    return importance_data\n",
      "\n",
      "\n",
      "def plot_roc_curves(trained_models, preprocessed_data):\n",
      "    \"\"\"\n",
      "    Plot ROC curves for all models.\n",
      "    \n",
      "    Args:\n",
      "        trained_models (dict): Dictionary of trained models\n",
      "        preprocessed_data (dict): Preprocessed data including test set\n",
      "        \n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Extract test data\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Create figure for ROC curves\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    \n",
      "    # Colors for different models\n",
      "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
      "    \n",
      "    # Plot ROC curve for each model\n",
      "    for i, (name, model) in enumerate(trained_models.items()):\n",
      "        # Get probability predictions\n",
      "        if hasattr(model, \"predict_proba\"):\n",
      "            y_prob = model.predict_proba(X_test)[:, 1]\n",
      "            \n",
      "            # Calculate ROC curve\n",
      "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
      "            \n",
      "            # Calculate AUC\n",
      "            roc_auc = auc(fpr, tpr)\n",
      "            \n",
      "            # Plot ROC curve\n",
      "            plt.plot(\n",
      "                fpr, \n",
      "                tpr, \n",
      "                color=colors[i % len(colors)],\n",
      "                lw=2, \n",
      "                label=f'{name} (AUC = {roc_auc:.4f})'\n",
      "            )\n",
      "    \n",
      "    # Plot diagonal reference line\n",
      "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
      "    \n",
      "    # Customize plot\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.05])\n",
      "    plt.xlabel('False Positive Rate', fontsize=12)\n",
      "    plt.ylabel('True Positive Rate', fontsize=12)\n",
      "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
      "    plt.legend(loc=\"lower right\", fontsize=10)\n",
      "    \n",
      "    # Save the ROC figure\n",
      "    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
      "    print(\"\\nROC curves saved as 'roc_curves.png'\")\n",
      "    \n",
      "    # Close plot to free memory\n",
      "    plt.close()\n",
      "\n",
      "\n",
      "def generate_report(results_df, trained_models, cv_results, preprocessed_data):\n",
      "    \"\"\"\n",
      "    Generate a comprehensive HTML report of model evaluation results.\n",
      "    \n",
      "    Args:\n",
      "        results_df (pd.DataFrame): DataFrame with performance metrics\n",
      "        trained_models (dict): Dictionary of trained models\n",
      "        cv_results (dict): Cross-validation results for each model\n",
      "        preprocessed_data (dict): Preprocessed data\n",
      "        \n",
      "    Returns:\n",
      "        None\n",
      "    \"\"\"\n",
      "    # Create a timestamp for the report\n",
      "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
      "    filename = f\"churn_prediction_report_{timestamp}.html\"\n",
      "    \n",
      "    # Start building the HTML content\n",
      "    html_content = f\"\"\"\n",
      "    <!DOCTYPE html>\n",
      "    <html>\n",
      "    <head>\n",
      "        <title>Customer Churn Prediction Model Evaluation Report</title>\n",
      "        <style>\n",
      "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
      "            h1 {{ color: #2c3e50; }}\n",
      "            h2 {{ color: #3498db; margin-top: 30px; }}\n",
      "            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}\n",
      "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
      "            th {{ background-color: #f2f2f2; }}\n",
      "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
      "            .timestamp {{ color: #7f8c8d; font-style: italic; }}\n",
      "            .section {{ margin: 30px 0; }}\n",
      "            .image-container {{ text-align: center; margin: 20px 0; }}\n",
      "            img {{ max-width: 100%; height: auto; }}\n",
      "        </style>\n",
      "    </head>\n",
      "    <body>\n",
      "        <h1>Customer Churn Prediction Model Evaluation Report</h1>\n",
      "        <p class=\"timestamp\">Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
      "        \n",
      "        <div class=\"section\">\n",
      "            <h2>Model Performance Summary</h2>\n",
      "            {results_df.to_html(index=False)}\n",
      "        </div>\n",
      "        \n",
      "        <div class=\"section\">\n",
      "            <h2>Performance Visualizations</h2>\n",
      "            <div class=\"image-container\">\n",
      "                <img src=\"model_performance_comparison.png\" alt=\"Model Performance Comparison\">\n",
      "                <p>Figure 1: Comparison of model performance metrics across different models.</p>\n",
      "            </div>\n",
      "            \n",
      "            <div class=\"image-container\">\n",
      "                <img src=\"confusion_matrices.png\" alt=\"Confusion Matrices\">\n",
      "                <p>Figure 2: Confusion matrices for each model, showing true positives, false positives, true negatives, and false negatives.</p>\n",
      "            </div>\n",
      "            \n",
      "            <div class=\"image-container\">\n",
      "                <img src=\"feature_importance.png\" alt=\"Feature Importance\">\n",
      "                <p>Figure 3: Importance of different features for tree-based models.</p>\n",
      "            </div>\n",
      "            \n",
      "            <div class=\"image-container\">\n",
      "                <img src=\"roc_curves.png\" alt=\"ROC Curves\">\n",
      "                <p>Figure 4: Receiver Operating Characteristic (ROC) curves showing the trade-off between true positive rate and false positive rate.</p>\n",
      "            </div>\n",
      "        </div>\n",
      "    </body>\n",
      "    </html>\n",
      "    \"\"\"\n",
      "    \n",
      "    # Write HTML content to file\n",
      "    with open(filename, 'w') as f:\n",
      "        f.write(html_content)\n",
      "    \n",
      "    print(f\"\\nComprehensive evaluation report saved as '{filename}'\")\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to demonstrate model training and evaluation.\"\"\"\n",
      "    print(\"Customer Churn Prediction - Model Training and Evaluation\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Step 1: Preprocess the data\n",
      "    preprocessed_data = preprocess_customer_data()\n",
      "    \n",
      "    # Step 2: Train and evaluate models\n",
      "    results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
      "    \n",
      "    # Step 3: Display results\n",
      "    print(\"\\nModel Performance Summary:\")\n",
      "    print(results_df.to_string(index=False))\n",
      "    \n",
      "    # Step 4: Visualize results\n",
      "    visualize_model_performance(results_df, trained_models, preprocessed_data)\n",
      "    \n",
      "    # Step 5: Analyze feature importance for tree-based models\n",
      "    importance_data = analyze_feature_importance(trained_models, preprocessed_data)\n",
      "    \n",
      "    # Step 6: Plot ROC curves\n",
      "    plot_roc_curves(trained_models, preprocessed_data)\n",
      "    \n",
      "    # Step 7: Generate comprehensive report\n",
      "    generate_report(results_df, trained_models, cv_results, preprocessed_data)\n",
      "    \n",
      "    print(\"\\nModel evaluation complete!\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "### Overview\n",
      "\n",
      "This script trains and evaluates multiple machine learning models for customer churn prediction. It takes the preprocessed data from the previous module and:\n",
      "\n",
      "1. Trains three different models (Random Forest, Gradient Boosting, Logistic Regression)\n",
      "2. Performs cross-validation to assess model performance\n",
      "3. Evaluates models using various metrics (accuracy, precision, recall, F1 score)\n",
      "4. Visualizes performance with charts and plots\n",
      "5. Generates a comprehensive HTML report\n",
      "\n",
      "### Key Functions\n",
      "\n",
      "#### 1. `train_and_evaluate_models(preprocessed_data, cv_folds, random_state)`\n",
      "\n",
      "This function:\n",
      "- Sets up three model types with hyperparameters\n",
      "- Runs cross-validation for each model\n",
      "- Calculates performance metrics for both cross-validation and test set\n",
      "- Returns results as a DataFrame along with trained models\n",
      "\n",
      "#### 2. `visualize_model_performance(results_df, cv_results, preprocessed_data)`\n",
      "\n",
      "Creates two visualizations:\n",
      "- Bar chart comparing performance metrics across models\n",
      "- Confusion matrices for each model's performance on the test set\n",
      "\n",
      "#### 3. `analyze_feature_importance(trained_models, preprocessed_data, top_n)`\n",
      "\n",
      "For tree-based models (Random Forest and Gradient Boosting):\n",
      "- Extracts feature importance scores\n",
      "- Creates bar charts showing the top N most important features\n",
      "- Returns feature importance data for further analysis\n",
      "\n",
      "#### 4. `plot_roc_curves(trained_models, preprocessed_data)`\n",
      "\n",
      "Creates ROC curves for all models:\n",
      "- Calculates true positive and false positive rates at different thresholds\n",
      "- Computes Area Under the Curve (AUC) scores\n",
      "- Plots the ROC curves for performance comparison\n",
      "\n",
      "#### 5. `generate_report(results_df, trained_models, cv_results, preprocessed_data)`\n",
      "\n",
      "Generates a comprehensive HTML report:\n",
      "- Displays performance metrics for all models\n",
      "- Includes visualizations (performance comparison, confusion matrices, feature importance, ROC curves)\n",
      "- Saves the report with a timestamp for documentation\n",
      "\n",
      "### Model Training Details\n",
      "\n",
      "The script trains three different algorithms:\n",
      "\n",
      "1. **Random Forest Classifier**\n",
      "   - Ensemble method that builds multiple decision trees\n",
      "   - Parameters: 100 trees, maximum depth of 10, minimum of 5 samples to split a node\n",
      "\n",
      "2. **Gradient Boosting Classifier**\n",
      "   - Builds trees sequentially, each correcting errors of the previous one\n",
      "   - Parameters: 100 trees, learning rate of 0.1, maximum depth of 5\n",
      "\n",
      "3. **Logistic Regression**\n",
      "   - Classical linear classification algorithm\n",
      "   - Parameters: C=1.0 (regularization strength), maximum of 1000 iterations\n",
      "\n",
      "Each model is evaluated with 5-fold cross-validation, ensuring robust performance assessment.\n",
      "\n",
      "## Evaluation Metrics\n",
      "\n",
      "The script calculates and reports several key metrics:\n",
      "\n",
      "1. **Accuracy**: Overall correctness of the model (correct predictions / total predictions)\n",
      "2. **Precision**: Ability to avoid false positives (true positives / (true positives + false positives))\n",
      "3. **Recall**: Ability to find all positive cases (true positives / (true positives + false negatives))\n",
      "4. **F1 Score**: Harmonic mean of precision and recall, balancing both concerns\n",
      "\n",
      "For each metric, both the mean and standard deviation across cross-validation folds are reported.\n",
      "\n",
      "## Visualizations\n",
      "\n",
      "The script generates four key visualizations:\n",
      "\n",
      "1. **Model Performance Comparison**: Bar chart comparing accuracy, precision, recall, and F1 score across models\n",
      "2. **Confusion Matrices**: Visual representation of true/false positives and negatives for each model\n",
      "3. **Feature Importance**: Bar charts showing the most influential features for tree-based models\n",
      "4. **ROC Curves**: Plots showing the tradeoff between true positive rate and false positive rate\n",
      "\n",
      "## How to Use This Code\n",
      "\n",
      "### Basic Usage\n",
      "\n",
      "```python\n",
      "# Import both modules\n",
      "from churn_preprocessing import preprocess_customer_data\n",
      "from churn_modeling import train_and_evaluate_models, visualize_model_performance\n",
      "\n",
      "# Step 1: Preprocess the data\n",
      "preprocessed_data = preprocess_customer_data()\n",
      "\n",
      "# Step 2: Train and evaluate models\n",
      "results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
      "\n",
      "# Step 3: Visualize results\n",
      "visualize_model_performance(results_df, trained_models, preprocessed_data)\n",
      "\n",
      "# Print the performance summary\n",
      "print(results_df)\n",
      "```\n",
      "\n",
      "### Custom Dataset\n",
      "\n",
      "```python\n",
      "# Preprocess your own data file\n",
      "preprocessed_data = preprocess_customer_data('your_customer_data.csv')\n",
      "\n",
      "# Train with custom cross-validation settings\n",
      "results_df, trained_models, cv_results = train_and_evaluate_models(\n",
      "    preprocessed_data, \n",
      "    cv_folds=10,  # Increase number of folds\n",
      "    random_state=123  # Different random seed\n",
      ")\n",
      "```\n",
      "\n",
      "## Running the Script\n",
      "\n",
      "1. Ensure you have saved the preprocessing code from the previous step as `churn_preprocessing.py`\n",
      "2. Save this modeling code as `churn_modeling.py`\n",
      "3. Install required libraries:\n",
      "   ```bash\n",
      "   pip install pandas numpy scikit-learn matplotlib seaborn\n",
      "   ```\n",
      "4. Run the modeling script:\n",
      "   ```bash\n",
      "   python churn_modeling.py\n",
      "   ```\n",
      "\n",
      "## Expected Output\n",
      "\n",
      "When run, the script will:\n",
      "1. Load or generate the preprocessed customer data\n",
      "2. Train and evaluate three different models\n",
      "3. Display a performance summary table in the console\n",
      "4. Generate several visualization files:\n",
      "   - `model_performance_comparison.png`\n",
      "   - `confusion_matrices.png`\n",
      "   - `feature_importance.png`\n",
      "   - `roc_curves.png`\n",
      "5. Create an HTML report with all results and visualizations\n",
      "\n",
      "The console will show:\n",
      "- Cross-validation results for each model\n",
      "- Performance metrics on the test set\n",
      "- Information about saved visualizations and reports\n",
      "\n",
      "## Extensions and Next Steps\n",
      "\n",
      "After model training and evaluation, you can:\n",
      "1. Select the best performing model for deployment\n",
      "2. Fine-tune hyperparameters using grid search or randomized search\n",
      "3. Consider ensemble methods combining multiple models\n",
      "4. Explore more complex models (e.g., neural networks, XGBoost)\n",
      "5. Add interpretation tools for better model understanding (e.g., SHAP values)\n",
      "\n",
      "## Notes\n",
      "\n",
      "- The code uses stratified k-fold cross-validation to ensure balanced class distribution in all folds\n",
      "- Standard deviations are reported to assess model stability across different data subsets\n",
      "- Multiple metrics are used to evaluate models from different perspectives (accuracy, precision, recall)\n",
      "- Visualizations help with comparison and communication of model performance\n",
      "- The HTML report provides a comprehensive documentation of the analysis that can be shared with stakeholders"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Customer Churn Model Training and Evaluation\n",
       "\n",
       "This Python module builds on the preprocessing pipeline to train and evaluate multiple machine learning models for customer churn prediction.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```bash\n",
       "pip install pandas numpy scikit-learn matplotlib seaborn\n",
       "```\n",
       "\n",
       "## Code Implementation\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "Customer Churn Prediction - Model Training and Evaluation Module\n",
       "\n",
       "This module implements training and evaluation of multiple machine learning models\n",
       "for customer churn prediction. It builds on the preprocessing module to train\n",
       "Random Forest, Gradient Boosting, and Logistic Regression models, and evaluates \n",
       "their performance using cross-validation.\n",
       "\"\"\"\n",
       "\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "from time import time\n",
       "from datetime import datetime\n",
       "\n",
       "# Machine learning models\n",
       "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "\n",
       "# Evaluation metrics and tools\n",
       "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
       "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
       "from sklearn.model_selection import cross_validate, cross_val_predict\n",
       "\n",
       "# Import preprocessing module\n",
       "from churn_preprocessing import preprocess_customer_data\n",
       "\n",
       "\n",
       "def train_and_evaluate_models(preprocessed_data, cv_folds=5, random_state=42):\n",
       "    \"\"\"\n",
       "    Train and evaluate multiple machine learning models using cross-validation.\n",
       "    \n",
       "    This function:\n",
       "    1. Sets up multiple classifier models\n",
       "    2. Trains each model using cross-validation\n",
       "    3. Evaluates models using various performance metrics\n",
       "    4. Returns detailed performance results\n",
       "    \n",
       "    Args:\n",
       "        preprocessed_data (dict): Output from preprocess_customer_data() containing:\n",
       "                                - X_train: Training features\n",
       "                                - y_train: Training target values\n",
       "                                - X_test: Test features\n",
       "                                - y_test: Test target values\n",
       "        cv_folds (int): Number of cross-validation folds\n",
       "        random_state (int): Random seed for reproducibility\n",
       "        \n",
       "    Returns:\n",
       "        tuple: Contains:\n",
       "            - results_df (pd.DataFrame): DataFrame with performance metrics for each model\n",
       "            - trained_models (dict): Dictionary of trained models\n",
       "            - cv_results (dict): Detailed cross-validation results for each model\n",
       "    \"\"\"\n",
       "    # Extract the data components\n",
       "    X_train = preprocessed_data['X_train']\n",
       "    y_train = preprocessed_data['y_train']\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Check if data is valid\n",
       "    if X_train is None or len(X_train) == 0:\n",
       "        raise ValueError(\"Training data is empty or None\")\n",
       "    \n",
       "    print(f\"\\nTraining and evaluating models with {cv_folds}-fold cross-validation...\")\n",
       "    \n",
       "    # Define the models to train\n",
       "    models = {\n",
       "        'Random Forest': RandomForestClassifier(\n",
       "            n_estimators=100,\n",
       "            max_depth=10,\n",
       "            min_samples_split=5,\n",
       "            random_state=random_state\n",
       "        ),\n",
       "        'Gradient Boosting': GradientBoostingClassifier(\n",
       "            n_estimators=100,\n",
       "            learning_rate=0.1,\n",
       "            max_depth=5,\n",
       "            random_state=random_state\n",
       "        ),\n",
       "        'Logistic Regression': LogisticRegression(\n",
       "            C=1.0,\n",
       "            max_iter=1000,\n",
       "            random_state=random_state\n",
       "        )\n",
       "    }\n",
       "    \n",
       "    # Define the metrics to evaluate\n",
       "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
       "    \n",
       "    # Dictionary to store results\n",
       "    cv_results = {}\n",
       "    trained_models = {}\n",
       "    results = []\n",
       "    \n",
       "    # Train and evaluate each model\n",
       "    for name, model in models.items():\n",
       "        print(f\"\\nTraining {name}...\")\n",
       "        start_time = time()\n",
       "        \n",
       "        # Perform cross-validation\n",
       "        cv_result = cross_validate(\n",
       "            model, \n",
       "            X_train, \n",
       "            y_train, \n",
       "            cv=cv_folds,\n",
       "            scoring=scoring,\n",
       "            return_train_score=True,\n",
       "            return_estimator=True\n",
       "        )\n",
       "        \n",
       "        # Store cross-validation results\n",
       "        cv_results[name] = cv_result\n",
       "        \n",
       "        # Train the model on the full training set\n",
       "        model.fit(X_train, y_train)\n",
       "        trained_models[name] = model\n",
       "        \n",
       "        # Make predictions on the test set\n",
       "        y_pred = model.predict(X_test)\n",
       "        \n",
       "        # Calculate metrics on the test set\n",
       "        test_accuracy = accuracy_score(y_test, y_pred)\n",
       "        test_precision = precision_score(y_test, y_pred)\n",
       "        test_recall = recall_score(y_test, y_pred)\n",
       "        test_f1 = f1_score(y_test, y_pred)\n",
       "        \n",
       "        # Calculate average cross-validation metrics\n",
       "        cv_accuracy = cv_result['test_accuracy'].mean()\n",
       "        cv_precision = cv_result['test_precision'].mean()\n",
       "        cv_recall = cv_result['test_recall'].mean()\n",
       "        cv_f1 = cv_result['test_f1'].mean()\n",
       "        \n",
       "        # Calculate standard deviations for cross-validation metrics\n",
       "        cv_accuracy_std = cv_result['test_accuracy'].std()\n",
       "        cv_precision_std = cv_result['test_precision'].std()\n",
       "        cv_recall_std = cv_result['test_recall'].std()\n",
       "        cv_f1_std = cv_result['test_f1'].std()\n",
       "        \n",
       "        # Store results\n",
       "        training_time = time() - start_time\n",
       "        \n",
       "        results.append({\n",
       "            'Model': name,\n",
       "            'CV Accuracy': f\"{cv_accuracy:.4f} ± {cv_accuracy_std:.4f}\",\n",
       "            'CV Precision': f\"{cv_precision:.4f} ± {cv_precision_std:.4f}\",\n",
       "            'CV Recall': f\"{cv_recall:.4f} ± {cv_recall_std:.4f}\",\n",
       "            'CV F1 Score': f\"{cv_f1:.4f} ± {cv_f1_std:.4f}\",\n",
       "            'Test Accuracy': f\"{test_accuracy:.4f}\",\n",
       "            'Test Precision': f\"{test_precision:.4f}\",\n",
       "            'Test Recall': f\"{test_recall:.4f}\",\n",
       "            'Test F1 Score': f\"{test_f1:.4f}\",\n",
       "            'Training Time (s)': f\"{training_time:.2f}\"\n",
       "        })\n",
       "        \n",
       "        print(f\"  Completed in {training_time:.2f} seconds\")\n",
       "        print(f\"  CV Accuracy: {cv_accuracy:.4f} ± {cv_accuracy_std:.4f}\")\n",
       "        print(f\"  Test Accuracy: {test_accuracy:.4f}\")\n",
       "    \n",
       "    # Convert results to DataFrame for easier display\n",
       "    results_df = pd.DataFrame(results)\n",
       "    \n",
       "    return results_df, trained_models, cv_results\n",
       "\n",
       "\n",
       "def visualize_model_performance(results_df, cv_results, preprocessed_data):\n",
       "    \"\"\"\n",
       "    Create visualizations of model performance metrics and confusion matrices.\n",
       "    \n",
       "    Args:\n",
       "        results_df (pd.DataFrame): DataFrame with performance metrics\n",
       "        cv_results (dict): Cross-validation results for each model\n",
       "        preprocessed_data (dict): Preprocessed data including test set\n",
       "        \n",
       "    Returns:\n",
       "        None\n",
       "    \"\"\"\n",
       "    # Set the style for plots\n",
       "    sns.set(style='whitegrid')\n",
       "    \n",
       "    # Extract test data for confusion matrices\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Create a figure for the bar chart of metrics\n",
       "    plt.figure(figsize=(14, 8))\n",
       "    \n",
       "    # Extract numeric values from formatted strings for plotting\n",
       "    metrics_to_plot = ['CV Accuracy', 'CV Precision', 'CV Recall', 'CV F1 Score']\n",
       "    plot_data = []\n",
       "    \n",
       "    for _, row in results_df.iterrows():\n",
       "        model_name = row['Model']\n",
       "        for metric in metrics_to_plot:\n",
       "            # Extract the mean value from the formatted string (e.g., \"0.8500 ± 0.0300\")\n",
       "            value = float(row[metric].split(' ±')[0])\n",
       "            plot_data.append({\n",
       "                'Model': model_name,\n",
       "                'Metric': metric.replace('CV ', ''),\n",
       "                'Value': value\n",
       "            })\n",
       "    \n",
       "    plot_df = pd.DataFrame(plot_data)\n",
       "    \n",
       "    # Create the grouped bar chart\n",
       "    ax = sns.barplot(x='Model', y='Value', hue='Metric', data=plot_df)\n",
       "    ax.set_title('Model Performance Comparison', fontsize=16)\n",
       "    ax.set_xlabel('Model', fontsize=14)\n",
       "    ax.set_ylabel('Score', fontsize=14)\n",
       "    ax.set_ylim([0, 1.0])\n",
       "    ax.legend(title='Metric', fontsize=12)\n",
       "    \n",
       "    # Add value labels on the bars\n",
       "    for container in ax.containers:\n",
       "        ax.bar_label(container, fmt='%.2f', fontsize=10)\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    \n",
       "    # Save the figure\n",
       "    plt.savefig('model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
       "    \n",
       "    # Create confusion matrices for each model\n",
       "    fig, axes = plt.subplots(1, len(cv_results), figsize=(5*len(cv_results), 5))\n",
       "    \n",
       "    # If there's only one model, axes will not be an array\n",
       "    if len(cv_results) == 1:\n",
       "        axes = [axes]\n",
       "    \n",
       "    for i, (name, model) in enumerate(cv_results.items()):\n",
       "        # Get the last estimator from cross-validation\n",
       "        estimator = model['estimator'][-1]\n",
       "        \n",
       "        # Make predictions on the test set\n",
       "        y_pred = estimator.predict(X_test)\n",
       "        \n",
       "        # Calculate confusion matrix\n",
       "        cm = confusion_matrix(y_test, y_pred)\n",
       "        \n",
       "        # Plot confusion matrix\n",
       "        sns.heatmap(\n",
       "            cm, \n",
       "            annot=True, \n",
       "            fmt='d', \n",
       "            cmap='Blues',\n",
       "            ax=axes[i],\n",
       "            cbar=False\n",
       "        )\n",
       "        \n",
       "        axes[i].set_title(f'{name} Confusion Matrix', fontsize=14)\n",
       "        axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
       "        axes[i].set_ylabel('True Label', fontsize=12)\n",
       "        \n",
       "    plt.tight_layout()\n",
       "    \n",
       "    # Save the confusion matrix figure\n",
       "    plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
       "    \n",
       "    # Close plots to free memory\n",
       "    plt.close('all')\n",
       "    \n",
       "    print(\"\\nPerformance visualizations saved as 'model_performance_comparison.png' and 'confusion_matrices.png'\")\n",
       "\n",
       "\n",
       "def analyze_feature_importance(trained_models, preprocessed_data, top_n=10):\n",
       "    \"\"\"\n",
       "    Analyze feature importance for tree-based models.\n",
       "    \n",
       "    Args:\n",
       "        trained_models (dict): Dictionary of trained models\n",
       "        preprocessed_data (dict): Preprocessed data including feature names\n",
       "        top_n (int): Number of top features to display\n",
       "        \n",
       "    Returns:\n",
       "        dict: Dictionary with feature importance data for each model\n",
       "    \"\"\"\n",
       "    feature_names = preprocessed_data['feature_names']\n",
       "    importance_data = {}\n",
       "    \n",
       "    # Create a figure for feature importance\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    \n",
       "    # Counter for subplots\n",
       "    plot_count = 0\n",
       "    \n",
       "    # Models that support feature importance\n",
       "    tree_models = ['Random Forest', 'Gradient Boosting']\n",
       "    \n",
       "    # Filter models\n",
       "    tree_model_dict = {name: model for name, model in trained_models.items() if name in tree_models}\n",
       "    \n",
       "    # If we have tree-based models\n",
       "    if tree_model_dict:\n",
       "        # Set up subplots\n",
       "        fig, axes = plt.subplots(len(tree_model_dict), 1, figsize=(12, 6*len(tree_model_dict)))\n",
       "        \n",
       "        # If there's only one model, axes will not be an array\n",
       "        if len(tree_model_dict) == 1:\n",
       "            axes = [axes]\n",
       "        \n",
       "        for i, (name, model) in enumerate(tree_model_dict.items()):\n",
       "            # Get feature importances\n",
       "            importances = model.feature_importances_\n",
       "            \n",
       "            # Create DataFrame for easier manipulation\n",
       "            importance_df = pd.DataFrame({\n",
       "                'Feature': feature_names,\n",
       "                'Importance': importances\n",
       "            }).sort_values(by='Importance', ascending=False)\n",
       "            \n",
       "            # Store in results dictionary\n",
       "            importance_data[name] = importance_df\n",
       "            \n",
       "            # Plot top N features\n",
       "            top_features = importance_df.head(top_n)\n",
       "            \n",
       "            # Create horizontal bar chart\n",
       "            sns.barplot(\n",
       "                x='Importance', \n",
       "                y='Feature', \n",
       "                data=top_features, \n",
       "                ax=axes[i],\n",
       "                palette='viridis'\n",
       "            )\n",
       "            \n",
       "            axes[i].set_title(f'Top {top_n} Features - {name}', fontsize=14)\n",
       "            axes[i].set_xlabel('Importance', fontsize=12)\n",
       "            axes[i].set_ylabel('Feature', fontsize=12)\n",
       "            \n",
       "        plt.tight_layout()\n",
       "        \n",
       "        # Save the feature importance figure\n",
       "        plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
       "        print(\"\\nFeature importance visualization saved as 'feature_importance.png'\")\n",
       "    \n",
       "    # Close plots to free memory\n",
       "    plt.close('all')\n",
       "    \n",
       "    return importance_data\n",
       "\n",
       "\n",
       "def plot_roc_curves(trained_models, preprocessed_data):\n",
       "    \"\"\"\n",
       "    Plot ROC curves for all models.\n",
       "    \n",
       "    Args:\n",
       "        trained_models (dict): Dictionary of trained models\n",
       "        preprocessed_data (dict): Preprocessed data including test set\n",
       "        \n",
       "    Returns:\n",
       "        None\n",
       "    \"\"\"\n",
       "    # Extract test data\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Create figure for ROC curves\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    \n",
       "    # Colors for different models\n",
       "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
       "    \n",
       "    # Plot ROC curve for each model\n",
       "    for i, (name, model) in enumerate(trained_models.items()):\n",
       "        # Get probability predictions\n",
       "        if hasattr(model, \"predict_proba\"):\n",
       "            y_prob = model.predict_proba(X_test)[:, 1]\n",
       "            \n",
       "            # Calculate ROC curve\n",
       "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
       "            \n",
       "            # Calculate AUC\n",
       "            roc_auc = auc(fpr, tpr)\n",
       "            \n",
       "            # Plot ROC curve\n",
       "            plt.plot(\n",
       "                fpr, \n",
       "                tpr, \n",
       "                color=colors[i % len(colors)],\n",
       "                lw=2, \n",
       "                label=f'{name} (AUC = {roc_auc:.4f})'\n",
       "            )\n",
       "    \n",
       "    # Plot diagonal reference line\n",
       "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
       "    \n",
       "    # Customize plot\n",
       "    plt.xlim([0.0, 1.0])\n",
       "    plt.ylim([0.0, 1.05])\n",
       "    plt.xlabel('False Positive Rate', fontsize=12)\n",
       "    plt.ylabel('True Positive Rate', fontsize=12)\n",
       "    plt.title('Receiver Operating Characteristic (ROC) Curves', fontsize=16)\n",
       "    plt.legend(loc=\"lower right\", fontsize=10)\n",
       "    \n",
       "    # Save the ROC figure\n",
       "    plt.savefig('roc_curves.png', dpi=300, bbox_inches='tight')\n",
       "    print(\"\\nROC curves saved as 'roc_curves.png'\")\n",
       "    \n",
       "    # Close plot to free memory\n",
       "    plt.close()\n",
       "\n",
       "\n",
       "def generate_report(results_df, trained_models, cv_results, preprocessed_data):\n",
       "    \"\"\"\n",
       "    Generate a comprehensive HTML report of model evaluation results.\n",
       "    \n",
       "    Args:\n",
       "        results_df (pd.DataFrame): DataFrame with performance metrics\n",
       "        trained_models (dict): Dictionary of trained models\n",
       "        cv_results (dict): Cross-validation results for each model\n",
       "        preprocessed_data (dict): Preprocessed data\n",
       "        \n",
       "    Returns:\n",
       "        None\n",
       "    \"\"\"\n",
       "    # Create a timestamp for the report\n",
       "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
       "    filename = f\"churn_prediction_report_{timestamp}.html\"\n",
       "    \n",
       "    # Start building the HTML content\n",
       "    html_content = f\"\"\"\n",
       "    <!DOCTYPE html>\n",
       "    <html>\n",
       "    <head>\n",
       "        <title>Customer Churn Prediction Model Evaluation Report</title>\n",
       "        <style>\n",
       "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
       "            h1 {{ color: #2c3e50; }}\n",
       "            h2 {{ color: #3498db; margin-top: 30px; }}\n",
       "            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}\n",
       "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
       "            th {{ background-color: #f2f2f2; }}\n",
       "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
       "            .timestamp {{ color: #7f8c8d; font-style: italic; }}\n",
       "            .section {{ margin: 30px 0; }}\n",
       "            .image-container {{ text-align: center; margin: 20px 0; }}\n",
       "            img {{ max-width: 100%; height: auto; }}\n",
       "        </style>\n",
       "    </head>\n",
       "    <body>\n",
       "        <h1>Customer Churn Prediction Model Evaluation Report</h1>\n",
       "        <p class=\"timestamp\">Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
       "        \n",
       "        <div class=\"section\">\n",
       "            <h2>Model Performance Summary</h2>\n",
       "            {results_df.to_html(index=False)}\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section\">\n",
       "            <h2>Performance Visualizations</h2>\n",
       "            <div class=\"image-container\">\n",
       "                <img src=\"model_performance_comparison.png\" alt=\"Model Performance Comparison\">\n",
       "                <p>Figure 1: Comparison of model performance metrics across different models.</p>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"image-container\">\n",
       "                <img src=\"confusion_matrices.png\" alt=\"Confusion Matrices\">\n",
       "                <p>Figure 2: Confusion matrices for each model, showing true positives, false positives, true negatives, and false negatives.</p>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"image-container\">\n",
       "                <img src=\"feature_importance.png\" alt=\"Feature Importance\">\n",
       "                <p>Figure 3: Importance of different features for tree-based models.</p>\n",
       "            </div>\n",
       "            \n",
       "            <div class=\"image-container\">\n",
       "                <img src=\"roc_curves.png\" alt=\"ROC Curves\">\n",
       "                <p>Figure 4: Receiver Operating Characteristic (ROC) curves showing the trade-off between true positive rate and false positive rate.</p>\n",
       "            </div>\n",
       "        </div>\n",
       "    </body>\n",
       "    </html>\n",
       "    \"\"\"\n",
       "    \n",
       "    # Write HTML content to file\n",
       "    with open(filename, 'w') as f:\n",
       "        f.write(html_content)\n",
       "    \n",
       "    print(f\"\\nComprehensive evaluation report saved as '{filename}'\")\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"Main function to demonstrate model training and evaluation.\"\"\"\n",
       "    print(\"Customer Churn Prediction - Model Training and Evaluation\")\n",
       "    print(\"=\"*60)\n",
       "    \n",
       "    # Step 1: Preprocess the data\n",
       "    preprocessed_data = preprocess_customer_data()\n",
       "    \n",
       "    # Step 2: Train and evaluate models\n",
       "    results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
       "    \n",
       "    # Step 3: Display results\n",
       "    print(\"\\nModel Performance Summary:\")\n",
       "    print(results_df.to_string(index=False))\n",
       "    \n",
       "    # Step 4: Visualize results\n",
       "    visualize_model_performance(results_df, trained_models, preprocessed_data)\n",
       "    \n",
       "    # Step 5: Analyze feature importance for tree-based models\n",
       "    importance_data = analyze_feature_importance(trained_models, preprocessed_data)\n",
       "    \n",
       "    # Step 6: Plot ROC curves\n",
       "    plot_roc_curves(trained_models, preprocessed_data)\n",
       "    \n",
       "    # Step 7: Generate comprehensive report\n",
       "    generate_report(results_df, trained_models, cv_results, preprocessed_data)\n",
       "    \n",
       "    print(\"\\nModel evaluation complete!\")\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "### Overview\n",
       "\n",
       "This script trains and evaluates multiple machine learning models for customer churn prediction. It takes the preprocessed data from the previous module and:\n",
       "\n",
       "1. Trains three different models (Random Forest, Gradient Boosting, Logistic Regression)\n",
       "2. Performs cross-validation to assess model performance\n",
       "3. Evaluates models using various metrics (accuracy, precision, recall, F1 score)\n",
       "4. Visualizes performance with charts and plots\n",
       "5. Generates a comprehensive HTML report\n",
       "\n",
       "### Key Functions\n",
       "\n",
       "#### 1. `train_and_evaluate_models(preprocessed_data, cv_folds, random_state)`\n",
       "\n",
       "This function:\n",
       "- Sets up three model types with hyperparameters\n",
       "- Runs cross-validation for each model\n",
       "- Calculates performance metrics for both cross-validation and test set\n",
       "- Returns results as a DataFrame along with trained models\n",
       "\n",
       "#### 2. `visualize_model_performance(results_df, cv_results, preprocessed_data)`\n",
       "\n",
       "Creates two visualizations:\n",
       "- Bar chart comparing performance metrics across models\n",
       "- Confusion matrices for each model's performance on the test set\n",
       "\n",
       "#### 3. `analyze_feature_importance(trained_models, preprocessed_data, top_n)`\n",
       "\n",
       "For tree-based models (Random Forest and Gradient Boosting):\n",
       "- Extracts feature importance scores\n",
       "- Creates bar charts showing the top N most important features\n",
       "- Returns feature importance data for further analysis\n",
       "\n",
       "#### 4. `plot_roc_curves(trained_models, preprocessed_data)`\n",
       "\n",
       "Creates ROC curves for all models:\n",
       "- Calculates true positive and false positive rates at different thresholds\n",
       "- Computes Area Under the Curve (AUC) scores\n",
       "- Plots the ROC curves for performance comparison\n",
       "\n",
       "#### 5. `generate_report(results_df, trained_models, cv_results, preprocessed_data)`\n",
       "\n",
       "Generates a comprehensive HTML report:\n",
       "- Displays performance metrics for all models\n",
       "- Includes visualizations (performance comparison, confusion matrices, feature importance, ROC curves)\n",
       "- Saves the report with a timestamp for documentation\n",
       "\n",
       "### Model Training Details\n",
       "\n",
       "The script trains three different algorithms:\n",
       "\n",
       "1. **Random Forest Classifier**\n",
       "   - Ensemble method that builds multiple decision trees\n",
       "   - Parameters: 100 trees, maximum depth of 10, minimum of 5 samples to split a node\n",
       "\n",
       "2. **Gradient Boosting Classifier**\n",
       "   - Builds trees sequentially, each correcting errors of the previous one\n",
       "   - Parameters: 100 trees, learning rate of 0.1, maximum depth of 5\n",
       "\n",
       "3. **Logistic Regression**\n",
       "   - Classical linear classification algorithm\n",
       "   - Parameters: C=1.0 (regularization strength), maximum of 1000 iterations\n",
       "\n",
       "Each model is evaluated with 5-fold cross-validation, ensuring robust performance assessment.\n",
       "\n",
       "## Evaluation Metrics\n",
       "\n",
       "The script calculates and reports several key metrics:\n",
       "\n",
       "1. **Accuracy**: Overall correctness of the model (correct predictions / total predictions)\n",
       "2. **Precision**: Ability to avoid false positives (true positives / (true positives + false positives))\n",
       "3. **Recall**: Ability to find all positive cases (true positives / (true positives + false negatives))\n",
       "4. **F1 Score**: Harmonic mean of precision and recall, balancing both concerns\n",
       "\n",
       "For each metric, both the mean and standard deviation across cross-validation folds are reported.\n",
       "\n",
       "## Visualizations\n",
       "\n",
       "The script generates four key visualizations:\n",
       "\n",
       "1. **Model Performance Comparison**: Bar chart comparing accuracy, precision, recall, and F1 score across models\n",
       "2. **Confusion Matrices**: Visual representation of true/false positives and negatives for each model\n",
       "3. **Feature Importance**: Bar charts showing the most influential features for tree-based models\n",
       "4. **ROC Curves**: Plots showing the tradeoff between true positive rate and false positive rate\n",
       "\n",
       "## How to Use This Code\n",
       "\n",
       "### Basic Usage\n",
       "\n",
       "```python\n",
       "# Import both modules\n",
       "from churn_preprocessing import preprocess_customer_data\n",
       "from churn_modeling import train_and_evaluate_models, visualize_model_performance\n",
       "\n",
       "# Step 1: Preprocess the data\n",
       "preprocessed_data = preprocess_customer_data()\n",
       "\n",
       "# Step 2: Train and evaluate models\n",
       "results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
       "\n",
       "# Step 3: Visualize results\n",
       "visualize_model_performance(results_df, trained_models, preprocessed_data)\n",
       "\n",
       "# Print the performance summary\n",
       "print(results_df)\n",
       "```\n",
       "\n",
       "### Custom Dataset\n",
       "\n",
       "```python\n",
       "# Preprocess your own data file\n",
       "preprocessed_data = preprocess_customer_data('your_customer_data.csv')\n",
       "\n",
       "# Train with custom cross-validation settings\n",
       "results_df, trained_models, cv_results = train_and_evaluate_models(\n",
       "    preprocessed_data, \n",
       "    cv_folds=10,  # Increase number of folds\n",
       "    random_state=123  # Different random seed\n",
       ")\n",
       "```\n",
       "\n",
       "## Running the Script\n",
       "\n",
       "1. Ensure you have saved the preprocessing code from the previous step as `churn_preprocessing.py`\n",
       "2. Save this modeling code as `churn_modeling.py`\n",
       "3. Install required libraries:\n",
       "   ```bash\n",
       "   pip install pandas numpy scikit-learn matplotlib seaborn\n",
       "   ```\n",
       "4. Run the modeling script:\n",
       "   ```bash\n",
       "   python churn_modeling.py\n",
       "   ```\n",
       "\n",
       "## Expected Output\n",
       "\n",
       "When run, the script will:\n",
       "1. Load or generate the preprocessed customer data\n",
       "2. Train and evaluate three different models\n",
       "3. Display a performance summary table in the console\n",
       "4. Generate several visualization files:\n",
       "   - `model_performance_comparison.png`\n",
       "   - `confusion_matrices.png`\n",
       "   - `feature_importance.png`\n",
       "   - `roc_curves.png`\n",
       "5. Create an HTML report with all results and visualizations\n",
       "\n",
       "The console will show:\n",
       "- Cross-validation results for each model\n",
       "- Performance metrics on the test set\n",
       "- Information about saved visualizations and reports\n",
       "\n",
       "## Extensions and Next Steps\n",
       "\n",
       "After model training and evaluation, you can:\n",
       "1. Select the best performing model for deployment\n",
       "2. Fine-tune hyperparameters using grid search or randomized search\n",
       "3. Consider ensemble methods combining multiple models\n",
       "4. Explore more complex models (e.g., neural networks, XGBoost)\n",
       "5. Add interpretation tools for better model understanding (e.g., SHAP values)\n",
       "\n",
       "## Notes\n",
       "\n",
       "- The code uses stratified k-fold cross-validation to ensure balanced class distribution in all folds\n",
       "- Standard deviations are reported to assess model stability across different data subsets\n",
       "- Multiple metrics are used to evaluate models from different perspectives (accuracy, precision, recall)\n",
       "- Visualizations help with comparison and communication of model performance\n",
       "- The HTML report provides a comprehensive documentation of the analysis that can be shared with stakeholders"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Customer Churn Model Selection and Deployment\n",
      "\n",
      "This Python module selects the best performing model from the previously trained models and prepares it for deployment.\n",
      "\n",
      "## Required Libraries\n",
      "\n",
      "```bash\n",
      "pip install pandas numpy scikit-learn matplotlib seaborn joblib\n",
      "```\n",
      "\n",
      "## Code Implementation\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Customer Churn Prediction - Model Selection and Deployment Module\n",
      "\n",
      "This module evaluates multiple trained models, selects the best performer based on F1 score,\n",
      "creates visualizations for model performance, and saves the selected model for deployment.\n",
      "It builds on the preprocessing and model training modules.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import joblib\n",
      "from datetime import datetime\n",
      "\n",
      "# For metrics and visualization\n",
      "from sklearn.metrics import (\n",
      "    classification_report, confusion_matrix, \n",
      "    roc_curve, auc, precision_recall_curve,\n",
      "    f1_score, accuracy_score, precision_score, recall_score\n",
      ")\n",
      "\n",
      "# Import from previous modules\n",
      "from churn_preprocessing import preprocess_customer_data\n",
      "from churn_modeling import train_and_evaluate_models\n",
      "\n",
      "\n",
      "def select_best_model(results_df, trained_models, preprocessed_data, metric='f1'):\n",
      "    \"\"\"\n",
      "    Evaluate models and select the best performing one based on the specified metric.\n",
      "    \n",
      "    Args:\n",
      "        results_df (pd.DataFrame): DataFrame with model performance metrics\n",
      "        trained_models (dict): Dictionary of trained model objects\n",
      "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
      "        metric (str): Metric to use for model selection ('f1', 'accuracy', 'precision', 'recall')\n",
      "                     Default is 'f1'\n",
      "    \n",
      "    Returns:\n",
      "        tuple: (best_model_name, best_model, best_model_metrics)\n",
      "    \"\"\"\n",
      "    print(f\"\\nSelecting best model based on {metric.upper()} score...\")\n",
      "    \n",
      "    # Extract test data for final evaluation\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Dictionary to store raw metric values\n",
      "    metric_values = {}\n",
      "    \n",
      "    # Dictionary to store comprehensive metrics for each model\n",
      "    model_metrics = {}\n",
      "    \n",
      "    # Evaluate each model on the test set\n",
      "    for model_name, model in trained_models.items():\n",
      "        # Get predictions\n",
      "        y_pred = model.predict(X_test)\n",
      "        \n",
      "        # Calculate metrics\n",
      "        accuracy = accuracy_score(y_test, y_pred)\n",
      "        precision = precision_score(y_test, y_pred)\n",
      "        recall = recall_score(y_test, y_pred)\n",
      "        f1 = f1_score(y_test, y_pred)\n",
      "        \n",
      "        # Store metrics\n",
      "        model_metrics[model_name] = {\n",
      "            'accuracy': accuracy,\n",
      "            'precision': precision,\n",
      "            'recall': recall,\n",
      "            'f1': f1\n",
      "        }\n",
      "        \n",
      "        # Store the selected metric for comparison\n",
      "        metric_values[model_name] = model_metrics[model_name][metric.lower()]\n",
      "    \n",
      "    # Find the best model based on the selected metric\n",
      "    best_model_name = max(metric_values, key=metric_values.get)\n",
      "    best_model = trained_models[best_model_name]\n",
      "    best_model_score = metric_values[best_model_name]\n",
      "    \n",
      "    print(f\"Best model based on {metric.upper()} score: {best_model_name}\")\n",
      "    print(f\"{metric.upper()} score: {best_model_score:.4f}\")\n",
      "    \n",
      "    # Get all metrics for the best model\n",
      "    best_model_metrics = model_metrics[best_model_name]\n",
      "    print(f\"Full metrics for {best_model_name}:\")\n",
      "    for metric_name, value in best_model_metrics.items():\n",
      "        print(f\"  {metric_name.capitalize()}: {value:.4f}\")\n",
      "    \n",
      "    return best_model_name, best_model, best_model_metrics\n",
      "\n",
      "\n",
      "def plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name=None):\n",
      "    \"\"\"\n",
      "    Create a detailed ROC curve plot for all models with the best model highlighted.\n",
      "    \n",
      "    Args:\n",
      "        trained_models (dict): Dictionary of trained model objects\n",
      "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
      "        best_model_name (str): Name of the best model to highlight\n",
      "        \n",
      "    Returns:\n",
      "        matplotlib.figure.Figure: The ROC curve figure\n",
      "    \"\"\"\n",
      "    # Extract test data\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Create figure\n",
      "    plt.figure(figsize=(10, 8))\n",
      "    \n",
      "    # Define colors and line styles\n",
      "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
      "    \n",
      "    # Plot ROC curve for each model\n",
      "    for i, (name, model) in enumerate(trained_models.items()):\n",
      "        # Get probability predictions (if model supports it)\n",
      "        if hasattr(model, \"predict_proba\"):\n",
      "            y_prob = model.predict_proba(X_test)[:, 1]\n",
      "            \n",
      "            # Calculate ROC curve\n",
      "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
      "            \n",
      "            # Calculate AUC\n",
      "            roc_auc = auc(fpr, tpr)\n",
      "            \n",
      "            # Set line style and width based on whether this is the best model\n",
      "            linestyle = '-'\n",
      "            linewidth = 2\n",
      "            alpha = 0.8\n",
      "            \n",
      "            if name == best_model_name:\n",
      "                linewidth = 3\n",
      "                alpha = 1.0\n",
      "                # Add a marker to the best model's line to make it stand out\n",
      "                plt.plot(\n",
      "                    fpr, tpr, \n",
      "                    color=colors[i % len(colors)], \n",
      "                    lw=linewidth, \n",
      "                    linestyle=linestyle, \n",
      "                    alpha=alpha,\n",
      "                    label=f'{name} (AUC = {roc_auc:.4f}) - BEST',\n",
      "                    marker='o',\n",
      "                    markevery=0.1,\n",
      "                    markersize=8\n",
      "                )\n",
      "            else:\n",
      "                plt.plot(\n",
      "                    fpr, tpr, \n",
      "                    color=colors[i % len(colors)], \n",
      "                    lw=linewidth, \n",
      "                    linestyle=linestyle, \n",
      "                    alpha=alpha,\n",
      "                    label=f'{name} (AUC = {roc_auc:.4f})'\n",
      "                )\n",
      "    \n",
      "    # Plot diagonal reference line\n",
      "    plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', alpha=0.7)\n",
      "    \n",
      "    # Customize plot\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.05])\n",
      "    plt.xlabel('False Positive Rate', fontsize=12)\n",
      "    plt.ylabel('True Positive Rate', fontsize=12)\n",
      "    plt.title('ROC Curves Comparison', fontsize=16)\n",
      "    plt.legend(loc=\"lower right\", fontsize=10)\n",
      "    plt.grid(True, alpha=0.3)\n",
      "    \n",
      "    # Add ROC curve interpretation guide\n",
      "    plt.figtext(0.15, 0.02, \n",
      "                \"ROC Curve Interpretation:\\n\"\n",
      "                \"- Curve closer to top-left corner indicates better model performance\\n\"\n",
      "                \"- AUC (Area Under Curve) closer to 1.0 indicates better discrimination\",\n",
      "                horizontalalignment='left', \n",
      "                fontsize=8, \n",
      "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
      "    \n",
      "    # Save the figure\n",
      "    plt.tight_layout()\n",
      "    plt.savefig('best_model_roc_curve.png', dpi=300, bbox_inches='tight')\n",
      "    print(\"\\nROC curve saved as 'best_model_roc_curve.png'\")\n",
      "    \n",
      "    return plt.gcf()\n",
      "\n",
      "\n",
      "def plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name=None):\n",
      "    \"\"\"\n",
      "    Create detailed confusion matrix visualizations for all models with the best model highlighted.\n",
      "    \n",
      "    Args:\n",
      "        trained_models (dict): Dictionary of trained model objects\n",
      "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
      "        best_model_name (str): Name of the best model to highlight\n",
      "        \n",
      "    Returns:\n",
      "        matplotlib.figure.Figure: The confusion matrices figure\n",
      "    \"\"\"\n",
      "    # Extract test data\n",
      "    X_test = preprocessed_data['X_test']\n",
      "    y_test = preprocessed_data['y_test']\n",
      "    \n",
      "    # Set the number of models to plot\n",
      "    n_models = len(trained_models)\n",
      "    \n",
      "    # Create a figure with subplots\n",
      "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 6))\n",
      "    \n",
      "    # If there's only one model, axes will not be an array\n",
      "    if n_models == 1:\n",
      "        axes = [axes]\n",
      "    \n",
      "    # Get class labels if they exist in the data\n",
      "    class_labels = ['Not Churned (0)', 'Churned (1)']\n",
      "    \n",
      "    # Plot confusion matrix for each model\n",
      "    for i, (name, model) in enumerate(trained_models.items()):\n",
      "        # Make predictions\n",
      "        y_pred = model.predict(X_test)\n",
      "        \n",
      "        # Calculate confusion matrix\n",
      "        cm = confusion_matrix(y_test, y_pred)\n",
      "        \n",
      "        # Calculate percentages for annotation\n",
      "        cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
      "        cm_perc = cm / cm_sum.astype(float) * 100\n",
      "        \n",
      "        # Prepare annotations\n",
      "        annot = np.zeros_like(cm, dtype=object)\n",
      "        for j in range(cm.shape[0]):\n",
      "            for k in range(cm.shape[1]):\n",
      "                annot[j, k] = f\"{cm[j, k]}\\n({cm_perc[j, k]:.1f}%)\"\n",
      "        \n",
      "        # Plot confusion matrix with custom colors based on whether this is the best model\n",
      "        cmap = 'Blues'\n",
      "        title_color = 'black'\n",
      "        \n",
      "        if name == best_model_name:\n",
      "            cmap = 'YlGnBu'  # Different colormap for the best model\n",
      "            title_color = 'darkgreen'  # Different title color for the best model\n",
      "        \n",
      "        # Plot confusion matrix\n",
      "        sns.heatmap(\n",
      "            cm, \n",
      "            annot=annot, \n",
      "            fmt='', \n",
      "            cmap=cmap,\n",
      "            ax=axes[i],\n",
      "            cbar=False,\n",
      "            xticklabels=class_labels,\n",
      "            yticklabels=class_labels\n",
      "        )\n",
      "        \n",
      "        title = f'{name} Confusion Matrix'\n",
      "        if name == best_model_name:\n",
      "            title += ' (BEST MODEL)'\n",
      "            \n",
      "        axes[i].set_title(title, fontsize=14, color=title_color, fontweight='bold' if name == best_model_name else 'normal')\n",
      "        axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
      "        axes[i].set_ylabel('True Label', fontsize=12)\n",
      "        \n",
      "        # Add metrics below the confusion matrix\n",
      "        accuracy = accuracy_score(y_test, y_pred)\n",
      "        precision = precision_score(y_test, y_pred)\n",
      "        recall = recall_score(y_test, y_pred)\n",
      "        f1 = f1_score(y_test, y_pred)\n",
      "        \n",
      "        metrics_text = f\"Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\"\n",
      "        axes[i].text(0.5, -0.2, metrics_text, horizontalalignment='center', \n",
      "                    transform=axes[i].transAxes, fontsize=10)\n",
      "    \n",
      "    plt.tight_layout()\n",
      "    \n",
      "    # Save the confusion matrix figure\n",
      "    plt.savefig('best_model_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
      "    print(\"Confusion matrices saved as 'best_model_confusion_matrices.png'\")\n",
      "    \n",
      "    return fig\n",
      "\n",
      "\n",
      "def save_model(model, model_name, preprocessed_data, output_dir='models'):\n",
      "    \"\"\"\n",
      "    Save the selected model and related artifacts for deployment.\n",
      "    \n",
      "    Args:\n",
      "        model: The trained model object to save\n",
      "        model_name (str): Name of the model\n",
      "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
      "        output_dir (str): Directory to save model artifacts\n",
      "        \n",
      "    Returns:\n",
      "        str: Path to the saved model file\n",
      "    \"\"\"\n",
      "    # Create output directory if it doesn't exist\n",
      "    os.makedirs(output_dir, exist_ok=True)\n",
      "    \n",
      "    # Create a timestamp for versioning\n",
      "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
      "    \n",
      "    # Create model filename\n",
      "    model_filename = f\"{output_dir}/{model_name.replace(' ', '_').lower()}_{timestamp}.joblib\"\n",
      "    \n",
      "    # Save the model using joblib\n",
      "    joblib.dump(model, model_filename)\n",
      "    print(f\"\\nModel saved to: {model_filename}\")\n",
      "    \n",
      "    # Save the preprocessing pipeline for future use\n",
      "    preprocessor = preprocessed_data.get('preprocessing_pipeline')\n",
      "    if preprocessor is not None:\n",
      "        preprocessor_filename = f\"{output_dir}/preprocessing_pipeline_{timestamp}.joblib\"\n",
      "        joblib.dump(preprocessor, preprocessor_filename)\n",
      "        print(f\"Preprocessing pipeline saved to: {preprocessor_filename}\")\n",
      "    \n",
      "    # Save feature names for reference\n",
      "    feature_names = preprocessed_data.get('feature_names')\n",
      "    if feature_names is not None:\n",
      "        feature_filename = f\"{output_dir}/feature_names_{timestamp}.txt\"\n",
      "        with open(feature_filename, 'w') as f:\n",
      "            for feature in feature_names:\n",
      "                f.write(f\"{feature}\\n\")\n",
      "        print(f\"Feature names saved to: {feature_filename}\")\n",
      "    \n",
      "    # Create a model metadata file\n",
      "    metadata_filename = f\"{output_dir}/model_metadata_{timestamp}.txt\"\n",
      "    with open(metadata_filename, 'w') as f:\n",
      "        f.write(f\"Model: {model_name}\\n\")\n",
      "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
      "        f.write(f\"Sklearn version: {pd.__version__}\\n\\n\")\n",
      "        \n",
      "        # Write model parameters if available\n",
      "        if hasattr(model, 'get_params'):\n",
      "            f.write(\"Model Parameters:\\n\")\n",
      "            params = model.get_params()\n",
      "            for param, value in params.items():\n",
      "                f.write(f\"  {param}: {value}\\n\")\n",
      "    \n",
      "    print(f\"Model metadata saved to: {metadata_filename}\")\n",
      "    \n",
      "    return model_filename\n",
      "\n",
      "\n",
      "def generate_code_snippet(model_name, model_file):\n",
      "    \"\"\"\n",
      "    Generate a code snippet for using the saved model in production.\n",
      "    \n",
      "    Args:\n",
      "        model_name (str): Name of the model\n",
      "        model_file (str): Path to the saved model file\n",
      "        \n",
      "    Returns:\n",
      "        str: Code snippet text\n",
      "    \"\"\"\n",
      "    snippet = f\"\"\"# Code snippet for using the {model_name} model in production\n",
      "\n",
      "import joblib\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the model\n",
      "model = joblib.load('{model_file}')\n",
      "\n",
      "# Load the preprocessing pipeline\n",
      "# Replace with your actual preprocessing pipeline file\n",
      "preprocessor = joblib.load('models/preprocessing_pipeline_*.joblib')\n",
      "\n",
      "def predict_churn(customer_data):\n",
      "    \\\"\\\"\\\"\n",
      "    Predict customer churn using the trained {model_name} model.\n",
      "    \n",
      "    Args:\n",
      "        customer_data (dict or pd.DataFrame): Customer data with the required features\n",
      "        \n",
      "    Returns:\n",
      "        tuple: (churn_prediction, churn_probability)\n",
      "            - churn_prediction: Boolean indicating whether the customer is predicted to churn\n",
      "            - churn_probability: Probability of the customer churning\n",
      "    \\\"\\\"\\\"\n",
      "    # Convert to DataFrame if data is a dictionary\n",
      "    if isinstance(customer_data, dict):\n",
      "        customer_data = pd.DataFrame([customer_data])\n",
      "        \n",
      "    # Preprocess the data\n",
      "    processed_data = preprocessor.transform(customer_data)\n",
      "    \n",
      "    # Make prediction\n",
      "    churn_prediction = model.predict(processed_data)[0]\n",
      "    \n",
      "    # Get probability (if the model supports it)\n",
      "    churn_probability = None\n",
      "    if hasattr(model, 'predict_proba'):\n",
      "        churn_probability = model.predict_proba(processed_data)[0, 1]\n",
      "    \n",
      "    return bool(churn_prediction), churn_probability\n",
      "\n",
      "# Example usage\n",
      "if __name__ == \"__main__\":\n",
      "    # Example customer data\n",
      "    customer = {{\n",
      "        'Tenure': 24,\n",
      "        'PhoneService': 'Yes',\n",
      "        'InternetService': 'Fiber optic',\n",
      "        'OnlineSecurity': 'No',\n",
      "        'OnlineBackup': 'Yes',\n",
      "        'TechSupport': 'No',\n",
      "        'Contract': 'Month-to-month',\n",
      "        'PaperlessBilling': 'Yes',\n",
      "        'PaymentMethod': 'Electronic check',\n",
      "        'MonthlyCharges': 74.95,\n",
      "        'TotalCharges': 1794.80\n",
      "    }}\n",
      "    \n",
      "    will_churn, probability = predict_churn(customer)\n",
      "    \n",
      "    print(f\"Churn prediction: {{will_churn}}\")\n",
      "    if probability is not None:\n",
      "        print(f\"Churn probability: {{probability:.2%}}\")\n",
      "\"\"\"\n",
      "    \n",
      "    # Save the snippet to a file\n",
      "    snippet_file = \"churn_prediction_usage_example.py\"\n",
      "    with open(snippet_file, 'w') as f:\n",
      "        f.write(snippet)\n",
      "    \n",
      "    print(f\"\\nExample usage code saved to: {snippet_file}\")\n",
      "    \n",
      "    return snippet\n",
      "\n",
      "\n",
      "def main():\n",
      "    \"\"\"Main function to demonstrate model selection and deployment.\"\"\"\n",
      "    print(\"Customer Churn Prediction - Model Selection and Deployment\")\n",
      "    print(\"=\"*60)\n",
      "    \n",
      "    # Step 1: Preprocess the data\n",
      "    preprocessed_data = preprocess_customer_data()\n",
      "    \n",
      "    # Step 2: Train and evaluate models\n",
      "    results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
      "    \n",
      "    # Step 3: Display evaluation summary\n",
      "    print(\"\\nModel Performance Summary:\")\n",
      "    print(results_df.to_string(index=False))\n",
      "    \n",
      "    # Step 4: Select the best model based on F1 score\n",
      "    best_model_name, best_model, best_metrics = select_best_model(\n",
      "        results_df, \n",
      "        trained_models, \n",
      "        preprocessed_data, \n",
      "        metric='f1'\n",
      "    )\n",
      "    \n",
      "    # Step 5: Create detailed visualizations\n",
      "    plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name)\n",
      "    plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name)\n",
      "    \n",
      "    # Step 6: Save the best model\n",
      "    model_file = save_model(best_model, best_model_name, preprocessed_data)\n",
      "    \n",
      "    # Step 7: Generate example usage code\n",
      "    generate_code_snippet(best_model_name, model_file)\n",
      "    \n",
      "    print(\"\\nModel selection and deployment preparation complete!\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "## How the Code Works\n",
      "\n",
      "### Overview\n",
      "\n",
      "This module builds on the previous preprocessing and model training steps to:\n",
      "\n",
      "1. Compare all trained models using multiple metrics\n",
      "2. Select the best model based on the F1 score\n",
      "3. Create detailed visualizations of model performance\n",
      "4. Save the selected model to disk for deployment\n",
      "5. Generate example code to use the model in production\n",
      "\n",
      "### Key Functions\n",
      "\n",
      "#### 1. `select_best_model(results_df, trained_models, preprocessed_data, metric='f1')`\n",
      "\n",
      "This function:\n",
      "- Evaluates all models using metrics like accuracy, precision, recall, and F1 score\n",
      "- Selects the best model based on the specified metric (default: F1 score)\n",
      "- Returns the best model, its name, and its performance metrics\n",
      "\n",
      "The F1 score is used as the default selection criterion because it balances precision and recall, which is important in a churn prediction context where both false positives and false negatives can be costly.\n",
      "\n",
      "#### 2. `plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name)`\n",
      "\n",
      "Creates an enhanced ROC curve visualization:\n",
      "- Plots ROC curves for all models\n",
      "- Highlights the best-performing model\n",
      "- Includes AUC (Area Under the Curve) values for each model\n",
      "- Adds interpretative guidance to help understand the visualization\n",
      "\n",
      "#### 3. `plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name)`\n",
      "\n",
      "Generates detailed confusion matrices:\n",
      "- Shows both counts and percentages for true/false predictions\n",
      "- Displays important metrics below each confusion matrix\n",
      "- Uses visual highlighting to identify the best model\n",
      "- Includes class labels for better interpretability\n",
      "\n",
      "#### 4. `save_model(model, model_name, preprocessed_data, output_dir='models')`\n",
      "\n",
      "Saves all necessary artifacts for deployment:\n",
      "- Serializes the model using joblib\n",
      "- Saves the preprocessing pipeline to ensure consistent data transformation\n",
      "- Records feature names to maintain correct input order\n",
      "- Creates detailed metadata about the model and its parameters\n",
      "\n",
      "#### 5. `generate_code_snippet(model_name, model_file)`\n",
      "\n",
      "Creates example code to demonstrate how to use the model:\n",
      "- Shows how to load the saved model\n",
      "- Provides a function to preprocess new data and make predictions\n",
      "- Includes an example use case with sample customer data\n",
      "\n",
      "### Model Selection Process\n",
      "\n",
      "The script selects the best model using this process:\n",
      "\n",
      "1. **Evaluation**: Each model is evaluated using multiple metrics on the test set\n",
      "2. **Comparison**: Models are compared based on the F1 score (balancing precision and recall)\n",
      "3. **Selection**: The model with the highest F1 score is selected as the best performer\n",
      "4. **Visualization**: The selected model is highlighted in ROC curves and confusion matrices\n",
      "5. **Deployment**: The selected model is saved along with necessary artifacts\n",
      "\n",
      "## Evaluation Metrics and Their Importance\n",
      "\n",
      "For churn prediction, different metrics provide insights into different aspects of model performance:\n",
      "\n",
      "- **Accuracy**: Overall correctness, but can be misleading if classes are imbalanced\n",
      "- **Precision**: Ability to avoid false positives (important if retention campaigns are costly)\n",
      "- **Recall**: Ability to identify all actual churners (important if missing churners is costly)\n",
      "- **F1 Score**: Balanced measure that combines precision and recall (good default choice)\n",
      "\n",
      "The script uses F1 score as the default selection criterion because:\n",
      "1. It balances false positives and false negatives\n",
      "2. It works well with imbalanced datasets (common in churn prediction)\n",
      "3. It provides a single metric for comparison across models\n",
      "\n",
      "## How to Use This Code\n",
      "\n",
      "### Basic Usage\n",
      "\n",
      "```python\n",
      "# Import modules\n",
      "from churn_preprocessing import preprocess_customer_data\n",
      "from churn_modeling import train_and_evaluate_models\n",
      "from churn_deployment import select_best_model, save_model\n",
      "\n",
      "# Step 1: Preprocess data\n",
      "preprocessed_data = preprocess_customer_data()\n",
      "\n",
      "# Step 2: Train models\n",
      "results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
      "\n",
      "# Step 3: Select best model\n",
      "best_model_name, best_model, best_metrics = select_best_model(\n",
      "    results_df, \n",
      "    trained_models, \n",
      "    preprocessed_data\n",
      ")\n",
      "\n",
      "# Step 4: Save model for deployment\n",
      "model_file = save_model(best_model, best_model_name, preprocessed_data)\n",
      "```\n",
      "\n",
      "### Selecting Best Model Based on Different Metrics\n",
      "\n",
      "```python\n",
      "# Select best model based on recall instead of F1 score\n",
      "best_model_name, best_model, best_metrics = select_best_model(\n",
      "    results_df, \n",
      "    trained_models, \n",
      "    preprocessed_data,\n",
      "    metric='recall'  # Prioritize finding all potential churners\n",
      ")\n",
      "```\n",
      "\n",
      "## Running the Script\n",
      "\n",
      "1. Ensure you have the preprocessing and modeling modules from previous steps\n",
      "2. Save this code as `churn_deployment.py`\n",
      "3. Install required libraries:\n",
      "   ```bash\n",
      "   pip install pandas numpy scikit-learn matplotlib seaborn joblib\n",
      "   ```\n",
      "4. Run the deployment script:\n",
      "   ```bash\n",
      "   python churn_deployment.py\n",
      "   ```\n",
      "\n",
      "## Expected Output\n",
      "\n",
      "When run, the script will:\n",
      "1. Load or generate the preprocessed customer data\n",
      "2. Train and evaluate the models\n",
      "3. Print a summary of model performance\n",
      "4. Select and announce the best model based on F1 score\n",
      "5. Generate enhanced ROC curves and confusion matrices\n",
      "6. Save the selected model and related artifacts\n",
      "7. Create example code for using the model in production\n",
      "\n",
      "You'll find these files in your directory:\n",
      "- `best_model_roc_curve.png`: Enhanced ROC curve visualization\n",
      "- `best_model_confusion_matrices.png`: Detailed confusion matrices\n",
      "- `models/`: Directory containing saved model artifacts\n",
      "- `churn_prediction_usage_example.py`: Example code for using the model\n",
      "\n",
      "## Using the Saved Model in Production\n",
      "\n",
      "The saved model can be used in production environments by:\n",
      "1. Loading the model using `joblib.load()`\n",
      "2. Loading the preprocessing pipeline to transform new data\n",
      "3. Calling the model's `predict()` method to get churn predictions\n",
      "4. Calling the model's `predict_proba()` method to get churn probabilities\n",
      "\n",
      "The generated code snippet (`churn_prediction_usage_example.py`) provides a complete example of this process.\n",
      "\n",
      "## Notes and Best Practices\n",
      "\n",
      "- **Model Versioning**: The code timestamps each saved model for version control\n",
      "- **Metadata**: Important model information is saved alongside the model file\n",
      "- **Documentation**: The code snippet shows how to use the model correctly\n",
      "- **Interpretability**: Visualizations help explain model performance to stakeholders\n",
      "- **Complete Pipeline**: Both the model and preprocessing pipeline are saved to ensure consistent results\n",
      "\n",
      "Through this process, we've created a complete workflow that takes raw customer data, preprocesses it, trains multiple models, selects the best performer, and prepares it for deployment in a production environment."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Customer Churn Model Selection and Deployment\n",
       "\n",
       "This Python module selects the best performing model from the previously trained models and prepares it for deployment.\n",
       "\n",
       "## Required Libraries\n",
       "\n",
       "```bash\n",
       "pip install pandas numpy scikit-learn matplotlib seaborn joblib\n",
       "```\n",
       "\n",
       "## Code Implementation\n",
       "\n",
       "```python\n",
       "#!/usr/bin/env python3\n",
       "\"\"\"\n",
       "Customer Churn Prediction - Model Selection and Deployment Module\n",
       "\n",
       "This module evaluates multiple trained models, selects the best performer based on F1 score,\n",
       "creates visualizations for model performance, and saves the selected model for deployment.\n",
       "It builds on the preprocessing and model training modules.\n",
       "\"\"\"\n",
       "\n",
       "import os\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "import joblib\n",
       "from datetime import datetime\n",
       "\n",
       "# For metrics and visualization\n",
       "from sklearn.metrics import (\n",
       "    classification_report, confusion_matrix, \n",
       "    roc_curve, auc, precision_recall_curve,\n",
       "    f1_score, accuracy_score, precision_score, recall_score\n",
       ")\n",
       "\n",
       "# Import from previous modules\n",
       "from churn_preprocessing import preprocess_customer_data\n",
       "from churn_modeling import train_and_evaluate_models\n",
       "\n",
       "\n",
       "def select_best_model(results_df, trained_models, preprocessed_data, metric='f1'):\n",
       "    \"\"\"\n",
       "    Evaluate models and select the best performing one based on the specified metric.\n",
       "    \n",
       "    Args:\n",
       "        results_df (pd.DataFrame): DataFrame with model performance metrics\n",
       "        trained_models (dict): Dictionary of trained model objects\n",
       "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
       "        metric (str): Metric to use for model selection ('f1', 'accuracy', 'precision', 'recall')\n",
       "                     Default is 'f1'\n",
       "    \n",
       "    Returns:\n",
       "        tuple: (best_model_name, best_model, best_model_metrics)\n",
       "    \"\"\"\n",
       "    print(f\"\\nSelecting best model based on {metric.upper()} score...\")\n",
       "    \n",
       "    # Extract test data for final evaluation\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Dictionary to store raw metric values\n",
       "    metric_values = {}\n",
       "    \n",
       "    # Dictionary to store comprehensive metrics for each model\n",
       "    model_metrics = {}\n",
       "    \n",
       "    # Evaluate each model on the test set\n",
       "    for model_name, model in trained_models.items():\n",
       "        # Get predictions\n",
       "        y_pred = model.predict(X_test)\n",
       "        \n",
       "        # Calculate metrics\n",
       "        accuracy = accuracy_score(y_test, y_pred)\n",
       "        precision = precision_score(y_test, y_pred)\n",
       "        recall = recall_score(y_test, y_pred)\n",
       "        f1 = f1_score(y_test, y_pred)\n",
       "        \n",
       "        # Store metrics\n",
       "        model_metrics[model_name] = {\n",
       "            'accuracy': accuracy,\n",
       "            'precision': precision,\n",
       "            'recall': recall,\n",
       "            'f1': f1\n",
       "        }\n",
       "        \n",
       "        # Store the selected metric for comparison\n",
       "        metric_values[model_name] = model_metrics[model_name][metric.lower()]\n",
       "    \n",
       "    # Find the best model based on the selected metric\n",
       "    best_model_name = max(metric_values, key=metric_values.get)\n",
       "    best_model = trained_models[best_model_name]\n",
       "    best_model_score = metric_values[best_model_name]\n",
       "    \n",
       "    print(f\"Best model based on {metric.upper()} score: {best_model_name}\")\n",
       "    print(f\"{metric.upper()} score: {best_model_score:.4f}\")\n",
       "    \n",
       "    # Get all metrics for the best model\n",
       "    best_model_metrics = model_metrics[best_model_name]\n",
       "    print(f\"Full metrics for {best_model_name}:\")\n",
       "    for metric_name, value in best_model_metrics.items():\n",
       "        print(f\"  {metric_name.capitalize()}: {value:.4f}\")\n",
       "    \n",
       "    return best_model_name, best_model, best_model_metrics\n",
       "\n",
       "\n",
       "def plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name=None):\n",
       "    \"\"\"\n",
       "    Create a detailed ROC curve plot for all models with the best model highlighted.\n",
       "    \n",
       "    Args:\n",
       "        trained_models (dict): Dictionary of trained model objects\n",
       "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
       "        best_model_name (str): Name of the best model to highlight\n",
       "        \n",
       "    Returns:\n",
       "        matplotlib.figure.Figure: The ROC curve figure\n",
       "    \"\"\"\n",
       "    # Extract test data\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Create figure\n",
       "    plt.figure(figsize=(10, 8))\n",
       "    \n",
       "    # Define colors and line styles\n",
       "    colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
       "    \n",
       "    # Plot ROC curve for each model\n",
       "    for i, (name, model) in enumerate(trained_models.items()):\n",
       "        # Get probability predictions (if model supports it)\n",
       "        if hasattr(model, \"predict_proba\"):\n",
       "            y_prob = model.predict_proba(X_test)[:, 1]\n",
       "            \n",
       "            # Calculate ROC curve\n",
       "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
       "            \n",
       "            # Calculate AUC\n",
       "            roc_auc = auc(fpr, tpr)\n",
       "            \n",
       "            # Set line style and width based on whether this is the best model\n",
       "            linestyle = '-'\n",
       "            linewidth = 2\n",
       "            alpha = 0.8\n",
       "            \n",
       "            if name == best_model_name:\n",
       "                linewidth = 3\n",
       "                alpha = 1.0\n",
       "                # Add a marker to the best model's line to make it stand out\n",
       "                plt.plot(\n",
       "                    fpr, tpr, \n",
       "                    color=colors[i % len(colors)], \n",
       "                    lw=linewidth, \n",
       "                    linestyle=linestyle, \n",
       "                    alpha=alpha,\n",
       "                    label=f'{name} (AUC = {roc_auc:.4f}) - BEST',\n",
       "                    marker='o',\n",
       "                    markevery=0.1,\n",
       "                    markersize=8\n",
       "                )\n",
       "            else:\n",
       "                plt.plot(\n",
       "                    fpr, tpr, \n",
       "                    color=colors[i % len(colors)], \n",
       "                    lw=linewidth, \n",
       "                    linestyle=linestyle, \n",
       "                    alpha=alpha,\n",
       "                    label=f'{name} (AUC = {roc_auc:.4f})'\n",
       "                )\n",
       "    \n",
       "    # Plot diagonal reference line\n",
       "    plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', alpha=0.7)\n",
       "    \n",
       "    # Customize plot\n",
       "    plt.xlim([0.0, 1.0])\n",
       "    plt.ylim([0.0, 1.05])\n",
       "    plt.xlabel('False Positive Rate', fontsize=12)\n",
       "    plt.ylabel('True Positive Rate', fontsize=12)\n",
       "    plt.title('ROC Curves Comparison', fontsize=16)\n",
       "    plt.legend(loc=\"lower right\", fontsize=10)\n",
       "    plt.grid(True, alpha=0.3)\n",
       "    \n",
       "    # Add ROC curve interpretation guide\n",
       "    plt.figtext(0.15, 0.02, \n",
       "                \"ROC Curve Interpretation:\\n\"\n",
       "                \"- Curve closer to top-left corner indicates better model performance\\n\"\n",
       "                \"- AUC (Area Under Curve) closer to 1.0 indicates better discrimination\",\n",
       "                horizontalalignment='left', \n",
       "                fontsize=8, \n",
       "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'))\n",
       "    \n",
       "    # Save the figure\n",
       "    plt.tight_layout()\n",
       "    plt.savefig('best_model_roc_curve.png', dpi=300, bbox_inches='tight')\n",
       "    print(\"\\nROC curve saved as 'best_model_roc_curve.png'\")\n",
       "    \n",
       "    return plt.gcf()\n",
       "\n",
       "\n",
       "def plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name=None):\n",
       "    \"\"\"\n",
       "    Create detailed confusion matrix visualizations for all models with the best model highlighted.\n",
       "    \n",
       "    Args:\n",
       "        trained_models (dict): Dictionary of trained model objects\n",
       "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
       "        best_model_name (str): Name of the best model to highlight\n",
       "        \n",
       "    Returns:\n",
       "        matplotlib.figure.Figure: The confusion matrices figure\n",
       "    \"\"\"\n",
       "    # Extract test data\n",
       "    X_test = preprocessed_data['X_test']\n",
       "    y_test = preprocessed_data['y_test']\n",
       "    \n",
       "    # Set the number of models to plot\n",
       "    n_models = len(trained_models)\n",
       "    \n",
       "    # Create a figure with subplots\n",
       "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 6))\n",
       "    \n",
       "    # If there's only one model, axes will not be an array\n",
       "    if n_models == 1:\n",
       "        axes = [axes]\n",
       "    \n",
       "    # Get class labels if they exist in the data\n",
       "    class_labels = ['Not Churned (0)', 'Churned (1)']\n",
       "    \n",
       "    # Plot confusion matrix for each model\n",
       "    for i, (name, model) in enumerate(trained_models.items()):\n",
       "        # Make predictions\n",
       "        y_pred = model.predict(X_test)\n",
       "        \n",
       "        # Calculate confusion matrix\n",
       "        cm = confusion_matrix(y_test, y_pred)\n",
       "        \n",
       "        # Calculate percentages for annotation\n",
       "        cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
       "        cm_perc = cm / cm_sum.astype(float) * 100\n",
       "        \n",
       "        # Prepare annotations\n",
       "        annot = np.zeros_like(cm, dtype=object)\n",
       "        for j in range(cm.shape[0]):\n",
       "            for k in range(cm.shape[1]):\n",
       "                annot[j, k] = f\"{cm[j, k]}\\n({cm_perc[j, k]:.1f}%)\"\n",
       "        \n",
       "        # Plot confusion matrix with custom colors based on whether this is the best model\n",
       "        cmap = 'Blues'\n",
       "        title_color = 'black'\n",
       "        \n",
       "        if name == best_model_name:\n",
       "            cmap = 'YlGnBu'  # Different colormap for the best model\n",
       "            title_color = 'darkgreen'  # Different title color for the best model\n",
       "        \n",
       "        # Plot confusion matrix\n",
       "        sns.heatmap(\n",
       "            cm, \n",
       "            annot=annot, \n",
       "            fmt='', \n",
       "            cmap=cmap,\n",
       "            ax=axes[i],\n",
       "            cbar=False,\n",
       "            xticklabels=class_labels,\n",
       "            yticklabels=class_labels\n",
       "        )\n",
       "        \n",
       "        title = f'{name} Confusion Matrix'\n",
       "        if name == best_model_name:\n",
       "            title += ' (BEST MODEL)'\n",
       "            \n",
       "        axes[i].set_title(title, fontsize=14, color=title_color, fontweight='bold' if name == best_model_name else 'normal')\n",
       "        axes[i].set_xlabel('Predicted Label', fontsize=12)\n",
       "        axes[i].set_ylabel('True Label', fontsize=12)\n",
       "        \n",
       "        # Add metrics below the confusion matrix\n",
       "        accuracy = accuracy_score(y_test, y_pred)\n",
       "        precision = precision_score(y_test, y_pred)\n",
       "        recall = recall_score(y_test, y_pred)\n",
       "        f1 = f1_score(y_test, y_pred)\n",
       "        \n",
       "        metrics_text = f\"Accuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\"\n",
       "        axes[i].text(0.5, -0.2, metrics_text, horizontalalignment='center', \n",
       "                    transform=axes[i].transAxes, fontsize=10)\n",
       "    \n",
       "    plt.tight_layout()\n",
       "    \n",
       "    # Save the confusion matrix figure\n",
       "    plt.savefig('best_model_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
       "    print(\"Confusion matrices saved as 'best_model_confusion_matrices.png'\")\n",
       "    \n",
       "    return fig\n",
       "\n",
       "\n",
       "def save_model(model, model_name, preprocessed_data, output_dir='models'):\n",
       "    \"\"\"\n",
       "    Save the selected model and related artifacts for deployment.\n",
       "    \n",
       "    Args:\n",
       "        model: The trained model object to save\n",
       "        model_name (str): Name of the model\n",
       "        preprocessed_data (dict): Dictionary containing preprocessed data\n",
       "        output_dir (str): Directory to save model artifacts\n",
       "        \n",
       "    Returns:\n",
       "        str: Path to the saved model file\n",
       "    \"\"\"\n",
       "    # Create output directory if it doesn't exist\n",
       "    os.makedirs(output_dir, exist_ok=True)\n",
       "    \n",
       "    # Create a timestamp for versioning\n",
       "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
       "    \n",
       "    # Create model filename\n",
       "    model_filename = f\"{output_dir}/{model_name.replace(' ', '_').lower()}_{timestamp}.joblib\"\n",
       "    \n",
       "    # Save the model using joblib\n",
       "    joblib.dump(model, model_filename)\n",
       "    print(f\"\\nModel saved to: {model_filename}\")\n",
       "    \n",
       "    # Save the preprocessing pipeline for future use\n",
       "    preprocessor = preprocessed_data.get('preprocessing_pipeline')\n",
       "    if preprocessor is not None:\n",
       "        preprocessor_filename = f\"{output_dir}/preprocessing_pipeline_{timestamp}.joblib\"\n",
       "        joblib.dump(preprocessor, preprocessor_filename)\n",
       "        print(f\"Preprocessing pipeline saved to: {preprocessor_filename}\")\n",
       "    \n",
       "    # Save feature names for reference\n",
       "    feature_names = preprocessed_data.get('feature_names')\n",
       "    if feature_names is not None:\n",
       "        feature_filename = f\"{output_dir}/feature_names_{timestamp}.txt\"\n",
       "        with open(feature_filename, 'w') as f:\n",
       "            for feature in feature_names:\n",
       "                f.write(f\"{feature}\\n\")\n",
       "        print(f\"Feature names saved to: {feature_filename}\")\n",
       "    \n",
       "    # Create a model metadata file\n",
       "    metadata_filename = f\"{output_dir}/model_metadata_{timestamp}.txt\"\n",
       "    with open(metadata_filename, 'w') as f:\n",
       "        f.write(f\"Model: {model_name}\\n\")\n",
       "        f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
       "        f.write(f\"Sklearn version: {pd.__version__}\\n\\n\")\n",
       "        \n",
       "        # Write model parameters if available\n",
       "        if hasattr(model, 'get_params'):\n",
       "            f.write(\"Model Parameters:\\n\")\n",
       "            params = model.get_params()\n",
       "            for param, value in params.items():\n",
       "                f.write(f\"  {param}: {value}\\n\")\n",
       "    \n",
       "    print(f\"Model metadata saved to: {metadata_filename}\")\n",
       "    \n",
       "    return model_filename\n",
       "\n",
       "\n",
       "def generate_code_snippet(model_name, model_file):\n",
       "    \"\"\"\n",
       "    Generate a code snippet for using the saved model in production.\n",
       "    \n",
       "    Args:\n",
       "        model_name (str): Name of the model\n",
       "        model_file (str): Path to the saved model file\n",
       "        \n",
       "    Returns:\n",
       "        str: Code snippet text\n",
       "    \"\"\"\n",
       "    snippet = f\"\"\"# Code snippet for using the {model_name} model in production\n",
       "\n",
       "import joblib\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "# Load the model\n",
       "model = joblib.load('{model_file}')\n",
       "\n",
       "# Load the preprocessing pipeline\n",
       "# Replace with your actual preprocessing pipeline file\n",
       "preprocessor = joblib.load('models/preprocessing_pipeline_*.joblib')\n",
       "\n",
       "def predict_churn(customer_data):\n",
       "    \\\"\\\"\\\"\n",
       "    Predict customer churn using the trained {model_name} model.\n",
       "    \n",
       "    Args:\n",
       "        customer_data (dict or pd.DataFrame): Customer data with the required features\n",
       "        \n",
       "    Returns:\n",
       "        tuple: (churn_prediction, churn_probability)\n",
       "            - churn_prediction: Boolean indicating whether the customer is predicted to churn\n",
       "            - churn_probability: Probability of the customer churning\n",
       "    \\\"\\\"\\\"\n",
       "    # Convert to DataFrame if data is a dictionary\n",
       "    if isinstance(customer_data, dict):\n",
       "        customer_data = pd.DataFrame([customer_data])\n",
       "        \n",
       "    # Preprocess the data\n",
       "    processed_data = preprocessor.transform(customer_data)\n",
       "    \n",
       "    # Make prediction\n",
       "    churn_prediction = model.predict(processed_data)[0]\n",
       "    \n",
       "    # Get probability (if the model supports it)\n",
       "    churn_probability = None\n",
       "    if hasattr(model, 'predict_proba'):\n",
       "        churn_probability = model.predict_proba(processed_data)[0, 1]\n",
       "    \n",
       "    return bool(churn_prediction), churn_probability\n",
       "\n",
       "# Example usage\n",
       "if __name__ == \"__main__\":\n",
       "    # Example customer data\n",
       "    customer = {{\n",
       "        'Tenure': 24,\n",
       "        'PhoneService': 'Yes',\n",
       "        'InternetService': 'Fiber optic',\n",
       "        'OnlineSecurity': 'No',\n",
       "        'OnlineBackup': 'Yes',\n",
       "        'TechSupport': 'No',\n",
       "        'Contract': 'Month-to-month',\n",
       "        'PaperlessBilling': 'Yes',\n",
       "        'PaymentMethod': 'Electronic check',\n",
       "        'MonthlyCharges': 74.95,\n",
       "        'TotalCharges': 1794.80\n",
       "    }}\n",
       "    \n",
       "    will_churn, probability = predict_churn(customer)\n",
       "    \n",
       "    print(f\"Churn prediction: {{will_churn}}\")\n",
       "    if probability is not None:\n",
       "        print(f\"Churn probability: {{probability:.2%}}\")\n",
       "\"\"\"\n",
       "    \n",
       "    # Save the snippet to a file\n",
       "    snippet_file = \"churn_prediction_usage_example.py\"\n",
       "    with open(snippet_file, 'w') as f:\n",
       "        f.write(snippet)\n",
       "    \n",
       "    print(f\"\\nExample usage code saved to: {snippet_file}\")\n",
       "    \n",
       "    return snippet\n",
       "\n",
       "\n",
       "def main():\n",
       "    \"\"\"Main function to demonstrate model selection and deployment.\"\"\"\n",
       "    print(\"Customer Churn Prediction - Model Selection and Deployment\")\n",
       "    print(\"=\"*60)\n",
       "    \n",
       "    # Step 1: Preprocess the data\n",
       "    preprocessed_data = preprocess_customer_data()\n",
       "    \n",
       "    # Step 2: Train and evaluate models\n",
       "    results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
       "    \n",
       "    # Step 3: Display evaluation summary\n",
       "    print(\"\\nModel Performance Summary:\")\n",
       "    print(results_df.to_string(index=False))\n",
       "    \n",
       "    # Step 4: Select the best model based on F1 score\n",
       "    best_model_name, best_model, best_metrics = select_best_model(\n",
       "        results_df, \n",
       "        trained_models, \n",
       "        preprocessed_data, \n",
       "        metric='f1'\n",
       "    )\n",
       "    \n",
       "    # Step 5: Create detailed visualizations\n",
       "    plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name)\n",
       "    plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name)\n",
       "    \n",
       "    # Step 6: Save the best model\n",
       "    model_file = save_model(best_model, best_model_name, preprocessed_data)\n",
       "    \n",
       "    # Step 7: Generate example usage code\n",
       "    generate_code_snippet(best_model_name, model_file)\n",
       "    \n",
       "    print(\"\\nModel selection and deployment preparation complete!\")\n",
       "\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()\n",
       "```\n",
       "\n",
       "## How the Code Works\n",
       "\n",
       "### Overview\n",
       "\n",
       "This module builds on the previous preprocessing and model training steps to:\n",
       "\n",
       "1. Compare all trained models using multiple metrics\n",
       "2. Select the best model based on the F1 score\n",
       "3. Create detailed visualizations of model performance\n",
       "4. Save the selected model to disk for deployment\n",
       "5. Generate example code to use the model in production\n",
       "\n",
       "### Key Functions\n",
       "\n",
       "#### 1. `select_best_model(results_df, trained_models, preprocessed_data, metric='f1')`\n",
       "\n",
       "This function:\n",
       "- Evaluates all models using metrics like accuracy, precision, recall, and F1 score\n",
       "- Selects the best model based on the specified metric (default: F1 score)\n",
       "- Returns the best model, its name, and its performance metrics\n",
       "\n",
       "The F1 score is used as the default selection criterion because it balances precision and recall, which is important in a churn prediction context where both false positives and false negatives can be costly.\n",
       "\n",
       "#### 2. `plot_roc_curve_detailed(trained_models, preprocessed_data, best_model_name)`\n",
       "\n",
       "Creates an enhanced ROC curve visualization:\n",
       "- Plots ROC curves for all models\n",
       "- Highlights the best-performing model\n",
       "- Includes AUC (Area Under the Curve) values for each model\n",
       "- Adds interpretative guidance to help understand the visualization\n",
       "\n",
       "#### 3. `plot_confusion_matrices_detailed(trained_models, preprocessed_data, best_model_name)`\n",
       "\n",
       "Generates detailed confusion matrices:\n",
       "- Shows both counts and percentages for true/false predictions\n",
       "- Displays important metrics below each confusion matrix\n",
       "- Uses visual highlighting to identify the best model\n",
       "- Includes class labels for better interpretability\n",
       "\n",
       "#### 4. `save_model(model, model_name, preprocessed_data, output_dir='models')`\n",
       "\n",
       "Saves all necessary artifacts for deployment:\n",
       "- Serializes the model using joblib\n",
       "- Saves the preprocessing pipeline to ensure consistent data transformation\n",
       "- Records feature names to maintain correct input order\n",
       "- Creates detailed metadata about the model and its parameters\n",
       "\n",
       "#### 5. `generate_code_snippet(model_name, model_file)`\n",
       "\n",
       "Creates example code to demonstrate how to use the model:\n",
       "- Shows how to load the saved model\n",
       "- Provides a function to preprocess new data and make predictions\n",
       "- Includes an example use case with sample customer data\n",
       "\n",
       "### Model Selection Process\n",
       "\n",
       "The script selects the best model using this process:\n",
       "\n",
       "1. **Evaluation**: Each model is evaluated using multiple metrics on the test set\n",
       "2. **Comparison**: Models are compared based on the F1 score (balancing precision and recall)\n",
       "3. **Selection**: The model with the highest F1 score is selected as the best performer\n",
       "4. **Visualization**: The selected model is highlighted in ROC curves and confusion matrices\n",
       "5. **Deployment**: The selected model is saved along with necessary artifacts\n",
       "\n",
       "## Evaluation Metrics and Their Importance\n",
       "\n",
       "For churn prediction, different metrics provide insights into different aspects of model performance:\n",
       "\n",
       "- **Accuracy**: Overall correctness, but can be misleading if classes are imbalanced\n",
       "- **Precision**: Ability to avoid false positives (important if retention campaigns are costly)\n",
       "- **Recall**: Ability to identify all actual churners (important if missing churners is costly)\n",
       "- **F1 Score**: Balanced measure that combines precision and recall (good default choice)\n",
       "\n",
       "The script uses F1 score as the default selection criterion because:\n",
       "1. It balances false positives and false negatives\n",
       "2. It works well with imbalanced datasets (common in churn prediction)\n",
       "3. It provides a single metric for comparison across models\n",
       "\n",
       "## How to Use This Code\n",
       "\n",
       "### Basic Usage\n",
       "\n",
       "```python\n",
       "# Import modules\n",
       "from churn_preprocessing import preprocess_customer_data\n",
       "from churn_modeling import train_and_evaluate_models\n",
       "from churn_deployment import select_best_model, save_model\n",
       "\n",
       "# Step 1: Preprocess data\n",
       "preprocessed_data = preprocess_customer_data()\n",
       "\n",
       "# Step 2: Train models\n",
       "results_df, trained_models, cv_results = train_and_evaluate_models(preprocessed_data)\n",
       "\n",
       "# Step 3: Select best model\n",
       "best_model_name, best_model, best_metrics = select_best_model(\n",
       "    results_df, \n",
       "    trained_models, \n",
       "    preprocessed_data\n",
       ")\n",
       "\n",
       "# Step 4: Save model for deployment\n",
       "model_file = save_model(best_model, best_model_name, preprocessed_data)\n",
       "```\n",
       "\n",
       "### Selecting Best Model Based on Different Metrics\n",
       "\n",
       "```python\n",
       "# Select best model based on recall instead of F1 score\n",
       "best_model_name, best_model, best_metrics = select_best_model(\n",
       "    results_df, \n",
       "    trained_models, \n",
       "    preprocessed_data,\n",
       "    metric='recall'  # Prioritize finding all potential churners\n",
       ")\n",
       "```\n",
       "\n",
       "## Running the Script\n",
       "\n",
       "1. Ensure you have the preprocessing and modeling modules from previous steps\n",
       "2. Save this code as `churn_deployment.py`\n",
       "3. Install required libraries:\n",
       "   ```bash\n",
       "   pip install pandas numpy scikit-learn matplotlib seaborn joblib\n",
       "   ```\n",
       "4. Run the deployment script:\n",
       "   ```bash\n",
       "   python churn_deployment.py\n",
       "   ```\n",
       "\n",
       "## Expected Output\n",
       "\n",
       "When run, the script will:\n",
       "1. Load or generate the preprocessed customer data\n",
       "2. Train and evaluate the models\n",
       "3. Print a summary of model performance\n",
       "4. Select and announce the best model based on F1 score\n",
       "5. Generate enhanced ROC curves and confusion matrices\n",
       "6. Save the selected model and related artifacts\n",
       "7. Create example code for using the model in production\n",
       "\n",
       "You'll find these files in your directory:\n",
       "- `best_model_roc_curve.png`: Enhanced ROC curve visualization\n",
       "- `best_model_confusion_matrices.png`: Detailed confusion matrices\n",
       "- `models/`: Directory containing saved model artifacts\n",
       "- `churn_prediction_usage_example.py`: Example code for using the model\n",
       "\n",
       "## Using the Saved Model in Production\n",
       "\n",
       "The saved model can be used in production environments by:\n",
       "1. Loading the model using `joblib.load()`\n",
       "2. Loading the preprocessing pipeline to transform new data\n",
       "3. Calling the model's `predict()` method to get churn predictions\n",
       "4. Calling the model's `predict_proba()` method to get churn probabilities\n",
       "\n",
       "The generated code snippet (`churn_prediction_usage_example.py`) provides a complete example of this process.\n",
       "\n",
       "## Notes and Best Practices\n",
       "\n",
       "- **Model Versioning**: The code timestamps each saved model for version control\n",
       "- **Metadata**: Important model information is saved alongside the model file\n",
       "- **Documentation**: The code snippet shows how to use the model correctly\n",
       "- **Interpretability**: Visualizations help explain model performance to stakeholders\n",
       "- **Complete Pipeline**: Both the model and preprocessing pipeline are saved to ensure consistent results\n",
       "\n",
       "Through this process, we've created a complete workflow that takes raw customer data, preprocesses it, trains multiple models, selects the best performer, and prepares it for deployment in a production environment."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step1_prompt = \"\"\"\n",
    "I'm building a machine learning pipeline for predicting customer churn.\n",
    "\n",
    "First, write Python code for loading and preprocessing the data with the following steps:\n",
    "\n",
    "1. Create a sample CSV file named 'customer_data.csv' containing relevant customer churn data, and then load it.\n",
    "2. Handle missing values appropriately using common techniques (e.g., fill with mean, drop rows).\n",
    "3. Encode categorical variables using suitable methods such as one-hot encoding or label encoding.\n",
    "4. Normalize the numerical features to ensure all values are on a similar scale.\n",
    "5. Split the dataset into training and test sets using an 80/20 ratio.\n",
    "\n",
    "Use the pandas and scikit-learn libraries. Make sure the code includes detailed comments explaining each step.\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "# Test with a simple example\n",
    "step1_response = agent(step1_prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(step1_response)\n",
    "\n",
    "\n",
    "step2_prompt = f\"\"\"\n",
    "Now that we have the preprocessing code, write Python code to train and evaluate multiple machine learning models:\n",
    "\n",
    "1. Use the preprocessed dataset obtained from the previous step.\n",
    "2. Train the following models:\n",
    "   - Random Forest Classifier\n",
    "   - Gradient Boosting Classifier\n",
    "   - Logistic Regression\n",
    "3. Apply 5-fold cross-validation to evaluate each model’s performance.\n",
    "4. For each model, calculate and display the following metrics:\n",
    "   - Accuracy\n",
    "   - Precision\n",
    "   - Recall\n",
    "   - F1 Score\n",
    "\n",
    "Use scikit-learn for modeling and evaluation. The output should include a summary of metrics for each model.\n",
    "\n",
    "Here’s the preprocessing code for reference:\n",
    "{step1_response}\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "step2_response = agent(step2_prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(step2_response)\n",
    "\n",
    "# Step 3: Model Evaluation and Selection\n",
    "step3_prompt = f\"\"\"\n",
    "Finally, write Python code to evaluate and select the best machine learning model:\n",
    "\n",
    "1. Compare the trained models using the evaluation metrics: Accuracy, Precision, Recall, and F1 Score.\n",
    "2. Create visualizations for each model:\n",
    "   - ROC Curve\n",
    "   - Confusion Matrix\n",
    "3. Based on the F1 Score, identify and select the best-performing model.\n",
    "4. Save the selected model to disk using `joblib`.\n",
    "\n",
    "Use libraries like `matplotlib`, `seaborn`, and `scikit-learn` for visualization and evaluation.\n",
    "\n",
    "Here’s the model training code for reference:\n",
    "{step2_response}\n",
    "\n",
    "Requirements:\n",
    "- Include clear inline comments to explain the logic.\n",
    "- Add a docstring for the function, describing its purpose, parameters, and return value.\n",
    "- Provide an example of how to use the function.\n",
    "- List any external libraries that need to be installed with pip (if any).\n",
    "- Include brief documentation describing how the code works and how to run it.\n",
    "\"\"\"\n",
    "\n",
    "step3_response = agent(step3_prompt)\n",
    "\n",
    "# Display with proper markdown formatting\n",
    "display_code_response(step3_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
