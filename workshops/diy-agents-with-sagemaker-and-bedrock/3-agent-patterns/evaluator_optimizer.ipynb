{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator-Optimizer Workflow\n",
    "\n",
    "This notebook demonstrates the evaluator-optimizer workflow, where one LLM evaluates and provides feedback on another LLM's output in an iterative loop. This creates a self-improving system where each iteration builds on feedback from previous attempts.\n",
    "\n",
    "### When to use this workflow\n",
    "\n",
    "This workflow is particularly effective when:\n",
    "\n",
    "- You have clear evaluation criteria\n",
    "- The task can benefit from iterative refinement\n",
    "- LLM responses can be demonstrably improved when feedback is provided\n",
    "- The LLM can provide meaningful feedback itself\n",
    "\n",
    "### Examples where this workflow is useful\n",
    "\n",
    "- Code generation and review: One LLM writes code while another reviews it for bugs, style issues, and performance problems\n",
    "- Content writing: One LLM writes content while another checks for tone, clarity, and accuracy\n",
    "- Data analysis: One LLM performs analysis while another validates the methodology and conclusions\n",
    "- Document summarization: One LLM creates summaries while another ensures key information is retained\n",
    "\n",
    "![](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F14f51e6406ccb29e695da48b17017e899a6119c7-2401x1000.png&w=3840&q=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Configure the LLM to use Amazon Bedrock\n",
    "\n",
    "To simplify things, we're going to use LiteLLM rather than Boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import re\n",
    "import boto3\n",
    "\n",
    "\n",
    "def llm_call(prompt: str, system_prompt: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Calls the model with the given prompt and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user prompt to send to the model.\n",
    "        system_prompt (str, optional): The system prompt to send to the model. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the language model.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"bedrock/us.amazon.nova-pro-v1:0\",\n",
    "        aws_region_name=boto3.Session().region_name,\n",
    "        messages=messages,\n",
    "        max_tokens=4096,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def extract_xml(text: str, tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the content of the specified XML tag from the given text.\n",
    "    Used for parsing structured responses.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text containing the XML.\n",
    "        tag (str): The XML tag to extract content from.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the specified XML tag, or an empty string if the tag is not found.\n",
    "    \"\"\"\n",
    "    match = re.search(f'<{tag}>(.*?)</{tag}>', text, re.DOTALL)\n",
    "    return match.group(1) if match else \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt: str, task: str, context: str = \"\") -> tuple[str, str]:\n",
    "    \"\"\"Generate and improve a solution based on feedback.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\n{context}\\nTask: {task}\" if context else f\"{prompt}\\nTask: {task}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    thoughts = extract_xml(response, \"thoughts\")\n",
    "    result = extract_xml(response, \"response\")\n",
    "    \n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result}\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    \n",
    "    return thoughts, result\n",
    "\n",
    "def evaluate(prompt: str, content: str, task: str) -> tuple[str, str]:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    full_prompt = f\"{prompt}\\nOriginal task: {task}\\nContent to evaluate: {content}\"\n",
    "    response = llm_call(full_prompt)\n",
    "    evaluation = extract_xml(response, \"evaluation\")\n",
    "    feedback = extract_xml(response, \"feedback\")\n",
    "    \n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Status: {evaluation}\")\n",
    "    print(f\"Feedback: {feedback}\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "    \n",
    "    return evaluation, feedback\n",
    "\n",
    "def loop(task: str, evaluator_prompt: str, generator_prompt: str) -> tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "    \n",
    "    thoughts, result = generate(generator_prompt, task)\n",
    "    memory.append(result)\n",
    "    chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})\n",
    "    \n",
    "    while True:\n",
    "        evaluation, feedback = evaluate(evaluator_prompt, result, task)\n",
    "        if evaluation == \"PASS\":\n",
    "            return result, chain_of_thought\n",
    "            \n",
    "        context = \"\\n\".join([\n",
    "            \"Previous attempts:\",\n",
    "            *[f\"- {m}\" for m in memory],\n",
    "            f\"\\nFeedback: {feedback}\"\n",
    "        ])\n",
    "        \n",
    "        thoughts, result = generate(generator_prompt, task, context)\n",
    "        memory.append(result)\n",
    "        chain_of_thought.append({\"thoughts\": thoughts, \"result\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Iterative Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Thoughts:\n",
      "\n",
      "The task is to implement a stack with three operations: push, pop, and getMin, all of which should operate in O(1) time complexity. To achieve this, we need to maintain an additional stack to keep track of the minimum values. When pushing a new element, we compare it with the current minimum and push the minimum value onto the auxiliary stack. When popping, we also pop from the auxiliary stack to maintain consistency. The getMin operation simply returns the top of the auxiliary stack.\n",
      "\n",
      "\n",
      "Generated:\n",
      "\n",
      "```python\n",
      "class MinStack:\n",
      "    def __init__(self):\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> None:\n",
      "        if self.stack[-1] == self.min_stack[-1]:\n",
      "            self.min_stack.pop()\n",
      "        self.stack.pop()\n",
      "\n",
      "    def top(self) -> int:\n",
      "        return self.stack[-1]\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        return self.min_stack[-1]\n",
      "```\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: NEEDS_IMPROVEMENT\n",
      "Feedback: \n",
      "1. **Code Correctness**: The implementation is mostly correct but lacks a crucial method `top()` which is typically expected in a stack implementation to return the top element without removing it. The method `pop()` does not return the popped element, which is standard for stack operations.\n",
      "   \n",
      "2. **Time Complexity**: The time complexity for `push`, `pop`, and `getMin` operations is O(1), which meets the requirement.\n",
      "\n",
      "3. **Style and Best Practices**: \n",
      "   - The class should include docstrings to describe its purpose and the purpose of each method.\n",
      "   - Itâ€™s good practice to handle edge cases, such as popping from an empty stack, by raising appropriate exceptions.\n",
      "   - The method `top()` should be renamed to `peek` for clarity, and it should be added if not already present.\n",
      "   - The `pop` method should return the popped element to conform to typical stack behavior.\n",
      "\n",
      "=== EVALUATION END ===\n",
      "\n",
      "\n",
      "=== GENERATION START ===\n",
      "Thoughts:\n",
      "\n",
      "The previous implementation is mostly correct but needs some improvements based on the feedback. I will add a `peek` method to return the top element without removing it, rename `top` to `peek`, ensure `pop` returns the popped element, and add docstrings and exception handling for edge cases.\n",
      "\n",
      "\n",
      "Generated:\n",
      "\n",
      "```python\n",
      "class MinStack:\n",
      "    \"\"\"\n",
      "    A stack that supports push, pop, peek, and retrieving the minimum element in constant time.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        \"\"\"\n",
      "        Initialize the MinStack.\n",
      "        \"\"\"\n",
      "        self.stack = []\n",
      "        self.min_stack = []\n",
      "\n",
      "    def push(self, x: int) -> None:\n",
      "        \"\"\"\n",
      "        Push element x onto stack.\n",
      "        \"\"\"\n",
      "        self.stack.append(x)\n",
      "        if not self.min_stack or x <= self.min_stack[-1]:\n",
      "            self.min_stack.append(x)\n",
      "\n",
      "    def pop(self) -> int:\n",
      "        \"\"\"\n",
      "        Remove the top element from the stack and return it.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"pop from empty stack\")\n",
      "        if self.stack[-1] == self.min_stack[-1]:\n",
      "            self.min_stack.pop()\n",
      "        return self.stack.pop()\n",
      "\n",
      "    def peek(self) -> int:\n",
      "        \"\"\"\n",
      "        Get the top element.\n",
      "        \"\"\"\n",
      "        if not self.stack:\n",
      "            raise IndexError(\"peek from empty stack\")\n",
      "        return self.stack[-1]\n",
      "\n",
      "    def getMin(self) -> int:\n",
      "        \"\"\"\n",
      "        Retrieve the minimum element in the stack.\n",
      "        \"\"\"\n",
      "        if not self.min_stack:\n",
      "            raise IndexError(\"getMin from empty stack\")\n",
      "        return self.min_stack[-1]\n",
      "```\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: \n",
      "The code correctly implements a stack with the required operations (push, pop, and getMin) all in O(1) time complexity. The use of two stacks (one for the main stack and one to keep track of the minimum values) is an efficient approach to achieve this. The style is clear, and the class and method docstrings provide good documentation. There are no further suggestions for improvements.\n",
      "\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n```python\\nclass MinStack:\\n    \"\"\"\\n    A stack that supports push, pop, peek, and retrieving the minimum element in constant time.\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"\\n        Initialize the MinStack.\\n        \"\"\"\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        \"\"\"\\n        Push element x onto stack.\\n        \"\"\"\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> int:\\n        \"\"\"\\n        Remove the top element from the stack and return it.\\n        \"\"\"\\n        if not self.stack:\\n            raise IndexError(\"pop from empty stack\")\\n        if self.stack[-1] == self.min_stack[-1]:\\n            self.min_stack.pop()\\n        return self.stack.pop()\\n\\n    def peek(self) -> int:\\n        \"\"\"\\n        Get the top element.\\n        \"\"\"\\n        if not self.stack:\\n            raise IndexError(\"peek from empty stack\")\\n        return self.stack[-1]\\n\\n    def getMin(self) -> int:\\n        \"\"\"\\n        Retrieve the minimum element in the stack.\\n        \"\"\"\\n        if not self.min_stack:\\n            raise IndexError(\"getMin from empty stack\")\\n        return self.min_stack[-1]\\n```\\n',\n",
       " [{'thoughts': '\\nThe task is to implement a stack with three operations: push, pop, and getMin, all of which should operate in O(1) time complexity. To achieve this, we need to maintain an additional stack to keep track of the minimum values. When pushing a new element, we compare it with the current minimum and push the minimum value onto the auxiliary stack. When popping, we also pop from the auxiliary stack to maintain consistency. The getMin operation simply returns the top of the auxiliary stack.\\n',\n",
       "   'result': '\\n```python\\nclass MinStack:\\n    def __init__(self):\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> None:\\n        if self.stack[-1] == self.min_stack[-1]:\\n            self.min_stack.pop()\\n        self.stack.pop()\\n\\n    def top(self) -> int:\\n        return self.stack[-1]\\n\\n    def getMin(self) -> int:\\n        return self.min_stack[-1]\\n```\\n'},\n",
       "  {'thoughts': '\\nThe previous implementation is mostly correct but needs some improvements based on the feedback. I will add a `peek` method to return the top element without removing it, rename `top` to `peek`, ensure `pop` returns the popped element, and add docstrings and exception handling for edge cases.\\n',\n",
       "   'result': '\\n```python\\nclass MinStack:\\n    \"\"\"\\n    A stack that supports push, pop, peek, and retrieving the minimum element in constant time.\\n    \"\"\"\\n\\n    def __init__(self):\\n        \"\"\"\\n        Initialize the MinStack.\\n        \"\"\"\\n        self.stack = []\\n        self.min_stack = []\\n\\n    def push(self, x: int) -> None:\\n        \"\"\"\\n        Push element x onto stack.\\n        \"\"\"\\n        self.stack.append(x)\\n        if not self.min_stack or x <= self.min_stack[-1]:\\n            self.min_stack.append(x)\\n\\n    def pop(self) -> int:\\n        \"\"\"\\n        Remove the top element from the stack and return it.\\n        \"\"\"\\n        if not self.stack:\\n            raise IndexError(\"pop from empty stack\")\\n        if self.stack[-1] == self.min_stack[-1]:\\n            self.min_stack.pop()\\n        return self.stack.pop()\\n\\n    def peek(self) -> int:\\n        \"\"\"\\n        Get the top element.\\n        \"\"\"\\n        if not self.stack:\\n            raise IndexError(\"peek from empty stack\")\\n        return self.stack[-1]\\n\\n    def getMin(self) -> int:\\n        \"\"\"\\n        Retrieve the minimum element in the stack.\\n        \"\"\"\\n        if not self.min_stack:\\n            raise IndexError(\"getMin from empty stack\")\\n        return self.min_stack[-1]\\n```\\n'}])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_prompt = \"\"\"\n",
    "Evaluate this following code implementation for:\n",
    "1. code correctness\n",
    "2. time complexity\n",
    "3. style and best practices\n",
    "\n",
    "You should be evaluating only and not attempting to solve the task.\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "Output your evaluation concisely in the following format.\n",
    "\n",
    "<evaluation>PASS, NEEDS_IMPROVEMENT, or FAIL</evaluation>\n",
    "<feedback>\n",
    "What needs improvement and why.\n",
    "</feedback>\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = \"\"\"\n",
    "Your goal is to complete the task based on <user input>. If there are feedback \n",
    "from your previous generations, you should reflect on them to improve your solution\n",
    "\n",
    "Output your answer concisely in the following format: \n",
    "\n",
    "<thoughts>\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "[Your code implementation here]\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "task = \"\"\"\n",
    "<user input>\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "</user input>\n",
    "\"\"\"\n",
    "\n",
    "loop(task, evaluator_prompt, generator_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Content Writing and Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GENERATION START ===\n",
      "Thoughts:\n",
      "\n",
      "In previous attempts, the feedback suggested that the content was too dense and lacked clear examples. To improve, I will break down the explanation into simpler segments, use concrete examples to illustrate key points, and include best practices for implementing the evaluator-optimizer workflow in AI development. I will also ensure the content is concise and under 500 words.\n",
      "\n",
      "\n",
      "Generated:\n",
      "\n",
      "### Understanding the Evaluator-Optimizer Workflow in AI Development\n",
      "\n",
      "The evaluator-optimizer workflow is a critical process in AI development that enhances model performance through iterative improvement. This workflow involves two main components: the evaluator and the optimizer.\n",
      "\n",
      "#### How It Works\n",
      "\n",
      "1. **Evaluation Phase**:\n",
      "   - The evaluator assesses the current performance of the AI model using predefined metrics (e.g., accuracy, precision, recall).\n",
      "   - Example: In a image classification task, the evaluator might use validation data to calculate the modelâ€™s accuracy.\n",
      "\n",
      "2. **Optimization Phase**:\n",
      "   - Based on the evaluation results, the optimizer adjusts the modelâ€™s parameters to improve performance.\n",
      "   - Example: If the accuracy is low, the optimizer might tweak the learning rate or modify the model architecture.\n",
      "\n",
      "3. **Iteration**:\n",
      "   - The process repeats, with the evaluator continuously assessing performance and the optimizer making adjustments until the model meets the desired criteria.\n",
      "\n",
      "#### Benefits\n",
      "\n",
      "- **Enhanced Performance**: By systematically evaluating and optimizing, models achieve higher accuracy and reliability.\n",
      "- **Efficiency**: Automates the tuning process, saving time and resources.\n",
      "- **Adaptability**: Allows models to adapt to new data or changing conditions.\n",
      "\n",
      "#### Concrete Example\n",
      "\n",
      "Consider training a natural language processing (NLP) model for sentiment analysis:\n",
      "\n",
      "1. **Initial Evaluation**:\n",
      "   - The evaluator tests the model on a sentiment dataset and finds an accuracy of 70%.\n",
      "\n",
      "2. **Optimization**:\n",
      "   - The optimizer increases the modelâ€™s complexity by adding more layers.\n",
      "   - It also adjusts the hyperparameters, such as the batch size and epoch count.\n",
      "\n",
      "3. **Re-evaluation**:\n",
      "   - After optimization, the evaluator retests the model and finds the accuracy has improved to 85%.\n",
      "\n",
      "#### Best Practices\n",
      "\n",
      "- **Define Clear Metrics**: Ensure the evaluator uses relevant metrics that align with your goals.\n",
      "- **Regular Iterations**: Perform frequent evaluation and optimization cycles to catch issues early.\n",
      "- **Monitor Overfitting**: Keep an eye on validation performance to avoid overfitting the model to the training data.\n",
      "\n",
      "By integrating the evaluator-optimizer workflow into your AI development process, you can systematically improve model performance, leading to more accurate and reliable AI solutions.\n",
      "\n",
      "=== GENERATION END ===\n",
      "\n",
      "=== EVALUATION START ===\n",
      "Status: PASS\n",
      "Feedback: \n",
      "The content is clear, technically accurate, engaging, and well-structured. It effectively explains the evaluator-optimizer workflow, provides concrete examples, and offers best practices. Grammar and style are appropriate for a technical blog post.\n",
      "\n",
      "=== EVALUATION END ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\n### Understanding the Evaluator-Optimizer Workflow in AI Development\\n\\nThe evaluator-optimizer workflow is a critical process in AI development that enhances model performance through iterative improvement. This workflow involves two main components: the evaluator and the optimizer.\\n\\n#### How It Works\\n\\n1. **Evaluation Phase**:\\n   - The evaluator assesses the current performance of the AI model using predefined metrics (e.g., accuracy, precision, recall).\\n   - Example: In a image classification task, the evaluator might use validation data to calculate the modelâ€™s accuracy.\\n\\n2. **Optimization Phase**:\\n   - Based on the evaluation results, the optimizer adjusts the modelâ€™s parameters to improve performance.\\n   - Example: If the accuracy is low, the optimizer might tweak the learning rate or modify the model architecture.\\n\\n3. **Iteration**:\\n   - The process repeats, with the evaluator continuously assessing performance and the optimizer making adjustments until the model meets the desired criteria.\\n\\n#### Benefits\\n\\n- **Enhanced Performance**: By systematically evaluating and optimizing, models achieve higher accuracy and reliability.\\n- **Efficiency**: Automates the tuning process, saving time and resources.\\n- **Adaptability**: Allows models to adapt to new data or changing conditions.\\n\\n#### Concrete Example\\n\\nConsider training a natural language processing (NLP) model for sentiment analysis:\\n\\n1. **Initial Evaluation**:\\n   - The evaluator tests the model on a sentiment dataset and finds an accuracy of 70%.\\n\\n2. **Optimization**:\\n   - The optimizer increases the modelâ€™s complexity by adding more layers.\\n   - It also adjusts the hyperparameters, such as the batch size and epoch count.\\n\\n3. **Re-evaluation**:\\n   - After optimization, the evaluator retests the model and finds the accuracy has improved to 85%.\\n\\n#### Best Practices\\n\\n- **Define Clear Metrics**: Ensure the evaluator uses relevant metrics that align with your goals.\\n- **Regular Iterations**: Perform frequent evaluation and optimization cycles to catch issues early.\\n- **Monitor Overfitting**: Keep an eye on validation performance to avoid overfitting the model to the training data.\\n\\nBy integrating the evaluator-optimizer workflow into your AI development process, you can systematically improve model performance, leading to more accurate and reliable AI solutions.\\n',\n",
       " [{'thoughts': '\\nIn previous attempts, the feedback suggested that the content was too dense and lacked clear examples. To improve, I will break down the explanation into simpler segments, use concrete examples to illustrate key points, and include best practices for implementing the evaluator-optimizer workflow in AI development. I will also ensure the content is concise and under 500 words.\\n',\n",
       "   'result': '\\n### Understanding the Evaluator-Optimizer Workflow in AI Development\\n\\nThe evaluator-optimizer workflow is a critical process in AI development that enhances model performance through iterative improvement. This workflow involves two main components: the evaluator and the optimizer.\\n\\n#### How It Works\\n\\n1. **Evaluation Phase**:\\n   - The evaluator assesses the current performance of the AI model using predefined metrics (e.g., accuracy, precision, recall).\\n   - Example: In a image classification task, the evaluator might use validation data to calculate the modelâ€™s accuracy.\\n\\n2. **Optimization Phase**:\\n   - Based on the evaluation results, the optimizer adjusts the modelâ€™s parameters to improve performance.\\n   - Example: If the accuracy is low, the optimizer might tweak the learning rate or modify the model architecture.\\n\\n3. **Iteration**:\\n   - The process repeats, with the evaluator continuously assessing performance and the optimizer making adjustments until the model meets the desired criteria.\\n\\n#### Benefits\\n\\n- **Enhanced Performance**: By systematically evaluating and optimizing, models achieve higher accuracy and reliability.\\n- **Efficiency**: Automates the tuning process, saving time and resources.\\n- **Adaptability**: Allows models to adapt to new data or changing conditions.\\n\\n#### Concrete Example\\n\\nConsider training a natural language processing (NLP) model for sentiment analysis:\\n\\n1. **Initial Evaluation**:\\n   - The evaluator tests the model on a sentiment dataset and finds an accuracy of 70%.\\n\\n2. **Optimization**:\\n   - The optimizer increases the modelâ€™s complexity by adding more layers.\\n   - It also adjusts the hyperparameters, such as the batch size and epoch count.\\n\\n3. **Re-evaluation**:\\n   - After optimization, the evaluator retests the model and finds the accuracy has improved to 85%.\\n\\n#### Best Practices\\n\\n- **Define Clear Metrics**: Ensure the evaluator uses relevant metrics that align with your goals.\\n- **Regular Iterations**: Perform frequent evaluation and optimization cycles to catch issues early.\\n- **Monitor Overfitting**: Keep an eye on validation performance to avoid overfitting the model to the training data.\\n\\nBy integrating the evaluator-optimizer workflow into your AI development process, you can systematically improve model performance, leading to more accurate and reliable AI solutions.\\n'}])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_evaluator_prompt = \"\"\"\n",
    "Evaluate this content for:\n",
    "1. Clarity and readability\n",
    "2. Technical accuracy\n",
    "3. Engagement and flow\n",
    "4. Grammar and style\n",
    "\n",
    "Only output \"PASS\" if all criteria are met and you have no further suggestions for improvements.\n",
    "Output your evaluation concisely in the following format.\n",
    "\n",
    "<evaluation>PASS, NEEDS_IMPROVEMENT, or FAIL</evaluation>\n",
    "<feedback>\n",
    "What needs improvement and why.\n",
    "</feedback>\n",
    "\"\"\"\n",
    "\n",
    "content_generator_prompt = \"\"\"\n",
    "Your goal is to write technical content based on the task. If there is feedback\n",
    "from your previous attempts, incorporate it to improve your writing.\n",
    "\n",
    "Output your answer in the following format:\n",
    "\n",
    "<thoughts>\n",
    "[Your understanding of the task and feedback and how you plan to improve]\n",
    "</thoughts>\n",
    "\n",
    "<response>\n",
    "[Your content here]\n",
    "</response>\n",
    "\"\"\"\n",
    "\n",
    "content_task = \"\"\"\n",
    "Write a technical blog post explaining how the evaluator-optimizer workflow works\n",
    "and its benefits for AI development. Include concrete examples and best practices.\n",
    "Keep it under 500 words.\n",
    "\"\"\"\n",
    "\n",
    "loop(content_task, content_evaluator_prompt, content_generator_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
